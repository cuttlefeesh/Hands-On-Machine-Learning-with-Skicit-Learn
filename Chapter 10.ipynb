{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ca2a3e",
   "metadata": {},
   "source": [
    "# Bab 10: Introduction to Artificial Neural Networks (Pengenalan Jaringan Saraf Tiruan)\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab 10 merupakan titik balik penting dalam buku ini, beralih dari model *machine learning* tradisional ke pengantar **Artificial Neural Networks (ANNs)** atau **Jaringan Saraf Tiruan (JST)**, yang merupakan inti dari *Deep Learning*. JST adalah model yang sangat kuat, serbaguna, dan skalabel yang dapat melakukan tugas klasifikasi, regresi, deteksi anomali, dan banyak lagi, terutama pada dataset yang sangat besar dan kompleks.\n",
    "\n",
    "Bab ini akan membahas:\n",
    "* Gambaran singkat sejarah JST.\n",
    "* Bagaimana JST diilhami oleh otak manusia.\n",
    "* Arsitektur JST populer dan cara menggunakannya dengan Keras (API tingkat tinggi untuk TensorFlow).\n",
    "* Pelatihan JST dengan *Gradient Descent*.\n",
    "\n",
    "### 2. Dari Otak Biologis ke Jaringan Saraf Tiruan (From Biological to Artificial Neurons)\n",
    "\n",
    "Bagian ini memulai dengan membandingkan neuron biologis dengan neuron buatan.\n",
    "\n",
    "#### a. Neuron Biologis (Biological Neurons)\n",
    "* Neuron biologis menerima sinyal input melalui **dendrit**.\n",
    "* Sinyal-sinyal ini diakumulasikan di **soma** (badan sel).\n",
    "* Ketika akumulasi sinyal mencapai ambang batas tertentu, neuron menembakkan impuls listrik (disebut **potensi aksi**) sepanjang **akson**.\n",
    "* Akson ini kemudian mentransmisikan sinyal ke neuron lain melalui koneksi yang disebut **sinapsis**. Kekuatan koneksi sinapsis (*synaptic weights*) dapat berubah, yang diyakini sebagai dasar pembelajaran.\n",
    "\n",
    "#### b. Neuron Buatan (Artificial Neurons)\n",
    "Neuron buatan adalah model matematika sederhana yang mencoba meniru fungsi dasar neuron biologis.\n",
    "* Setiap neuron menerima satu atau lebih input, yang dapat berupa nilai fitur atau output dari neuron lain.\n",
    "* Setiap input dikalikan dengan suatu **bobot (weight)**.\n",
    "* Hasil perkalian ini dijumlahkan, bersama dengan **bias term**.\n",
    "* Jumlah ini kemudian dilewatkan melalui **fungsi aktivasi (activation function)**, yang menghasilkan output neuron.\n",
    "\n",
    "#### c. Model Perseptrons (Perceptron Models)\n",
    "Perceptron adalah arsitektur JST yang paling sederhana, dikembangkan oleh Frank Rosenblatt pada tahun 1957.\n",
    "* Sebuah Perceptron terdiri dari satu lapisan neuron buatan (disebut **Threshold Logic Units - TLUs**).\n",
    "* TLU menghitung jumlah bobot input ($\\mathbf{z} = \\mathbf{w}^\\intercal \\mathbf{x} + b$), kemudian menerapkan fungsi *step* (fungsi ambang) untuk menghasilkan output biner (0 atau 1).\n",
    "* JST Perceptron dapat dilatih dengan **Aturan Pembelajaran Perceptron (Perceptron Learning Rule)**, yang merupakan varian dari *Gradient Descent*. Jika prediksi salah, bobot diatur untuk mengurangi *error*.\n",
    "\n",
    "**Batasan Perceptron:** Perceptron tidak dapat menyelesaikan masalah yang tidak dapat dipisahkan secara linier (misalnya, masalah XOR). Ini menyebabkan \"AI winter\" pertama.\n",
    "\n",
    "#### d. Multi-Layer Perceptrons (MLPs)\n",
    "*Multi-Layer Perceptron (MLP)** adalah JST yang terdiri dari satu atau lebih lapisan TLU yang disebut **lapisan tersembunyi (hidden layers)**, dan satu lapisan output. Setiap lapisan dihubungkan ke lapisan berikutnya, tetapi tidak ada koneksi di dalam lapisan yang sama atau koneksi kembali (tidak ada *loops*). JST semacam ini disebut **feedforward neural network (FNN)**.\n",
    "* **Input Layer:** Lapisan tempat data input masuk.\n",
    "* **Hidden Layers:** Lapisan perantara yang melakukan transformasi non-linier pada input. Jumlah lapisan tersembunyi menentukan \"kedalaman\" jaringan.\n",
    "* **Output Layer:** Lapisan yang menghasilkan prediksi.\n",
    "\n",
    "MLP mampu menyelesaikan masalah yang tidak dapat dipisahkan secara linier dan dapat menangani masalah yang sangat kompleks. Kesuksesan MLP sebagian besar karena algoritma **Backpropagation (Propagasi Balik)**.\n",
    "\n",
    "### 3. Backpropagation Training (Pelatihan Backpropagation)\n",
    "\n",
    "Algoritma Backpropagation adalah inti dari pelatihan JST yang kompleks. Ini adalah varian *Gradient Descent* yang cerdas yang memungkinkan pelatihan seluruh JST secara efisien.\n",
    "\n",
    "**Prinsip Kerja:**\n",
    "1.  **Forward Pass:** Sebuah *instance training* dimasukkan ke jaringan. Output setiap neuron dihitung, lapis demi lapis, sampai output akhir dihasilkan.\n",
    "2.  **Error Calculation:** *Error* model (perbedaan antara output yang diprediksi dan target sebenarnya) dihitung.\n",
    "3.  **Backward Pass:** *Error* ini disebarkan mundur melalui jaringan, dari lapisan output ke lapisan input. Algoritma menghitung kontribusi *error* setiap neuron di setiap lapisan.\n",
    "4.  **Gradient Computation:** Algoritma menghitung *gradient* fungsi biaya terhadap setiap parameter (bobot dan bias) dalam jaringan.\n",
    "5.  **Parameter Update:** *Gradient Descent* digunakan untuk memperbarui setiap parameter berdasarkan *gradient* yang dihitung, dalam upaya meminimalkan fungsi biaya.\n",
    "\n",
    "**Fungsi Aktivasi (Activation Functions):**\n",
    "Fungsi aktivasi adalah komponen kunci dalam JST, memungkinkan jaringan mempelajari pola non-linier.\n",
    "* **Step Function:** (Digunakan di Perceptrons) output biner, tidak dapat dibedakan, tidak cocok untuk *Gradient Descent*.\n",
    "* **Sigmoid Function:** Output antara 0 dan 1, cocok untuk klasifikasi probabilitas. Masalah *vanishing gradients* pada nilai input yang ekstrem.\n",
    "* **Hyperbolic Tangent (tanh) Function:** Output antara -1 dan 1, berpusat di 0. Lebih baik dari sigmoid dalam beberapa kasus, tetapi masih rentan terhadap *vanishing gradients*.\n",
    "* **Rectified Linear Unit (ReLU) Function:** Output 0 untuk input negatif, dan input itu sendiri untuk input positif. Cepat dihitung, tidak ada masalah *vanishing gradients* untuk input positif. Masalah *dying ReLUs* (neuron macet di 0).\n",
    "* **Softmax Function:** Digunakan di lapisan output untuk klasifikasi multikelas, menghasilkan distribusi probabilitas.\n",
    "\n",
    "### 4. Regression MLPs (MLP untuk Regresi)\n",
    "\n",
    "MLP dapat digunakan untuk regresi.\n",
    "* **Lapisan Output:** Untuk regresi, lapisan output biasanya hanya memiliki satu neuron (untuk prediksi nilai tunggal).\n",
    "* **Fungsi Aktivasi Output:** Tidak ada fungsi aktivasi di lapisan output, atau fungsi aktivasi linear (identitas).\n",
    "* **Fungsi Biaya:** *Mean Squared Error* (MSE) adalah fungsi biaya umum.\n",
    "\n",
    "### 5. Classification MLPs (MLP untuk Klasifikasi)\n",
    "\n",
    "MLP dapat digunakan untuk klasifikasi biner dan multikelas.\n",
    "* **Klasifikasi Biner:** Lapisan output dengan satu neuron dan fungsi aktivasi *sigmoid*.\n",
    "* **Klasifikasi Multikelas:** Lapisan output dengan satu neuron per kelas dan fungsi aktivasi *softmax*.\n",
    "\n",
    "### 6. Membangun JST dengan Keras (Building an Image Classifier Using the Keras Sequential API)\n",
    "\n",
    "Bagian ini memperkenalkan Keras sebagai API tingkat tinggi dan user-friendly untuk membangun dan melatih model *deep learning*.\n",
    "\n",
    "#### a. Menggunakan Keras untuk Membangun Model\n",
    "\n",
    "Langkah-langkah umum untuk membangun model JST menggunakan Keras Sequential API:\n",
    "1.  **Memuat Dataset:** Contoh menggunakan dataset Fashion MNIST (mirip dengan MNIST, tetapi dengan gambar pakaian).\n",
    "2.  **Pra-pemrosesan Data:** Skala fitur ke rentang 0-1 (normalisasi).\n",
    "3.  **Membangun Model Sequential:**\n",
    "    * `tf.keras.models.Sequential()`: Model linier tumpukan lapisan.\n",
    "    * `tf.keras.layers.Flatten()`: Lapisan pertama untuk mengkonversi data input (misalnya, gambar 2D) menjadi array 1D.\n",
    "    * `tf.keras.layers.Dense()`: Lapisan tersembunyi biasa dengan jumlah neuron dan fungsi aktivasi tertentu (misalnya, `relu`).\n",
    "    * Lapisan `Dense` terakhir (lapisan output) dengan jumlah neuron sesuai jumlah kelas dan fungsi aktivasi `softmax` (untuk klasifikasi multikelas) atau `sigmoid` (untuk klasifikasi biner).\n",
    "4.  **Mengompilasi Model:**\n",
    "    * `model.compile()`: Mengkonfigurasi model untuk pelatihan.\n",
    "    * `optimizer`: Algoritma optimisasi (misalnya, `\"sgd\"` untuk *Stochastic Gradient Descent*, `\"adam\"`).\n",
    "    * `loss`: Fungsi biaya (misalnya, `\"sparse_categorical_crossentropy\"` untuk klasifikasi multikelas dengan label integer, `\"binary_crossentropy\"` untuk biner).\n",
    "    * `metrics`: Metrik kinerja yang akan dievaluasi selama pelatihan (misalnya, `\"accuracy\"`).\n",
    "5.  **Melatih Model:**\n",
    "    * `model.fit()`: Melatih model pada data pelatihan.\n",
    "    * `epochs`: Jumlah *epoch* (berapa kali model akan melewati seluruh *training set*).\n",
    "    * `validation_data`: Data validasi untuk memantau kinerja selama pelatihan.\n",
    "\n",
    "#### b. Mengevaluasi Model\n",
    "\n",
    "Setelah pelatihan, model dievaluasi pada *test set* menggunakan `model.evaluate()`.\n",
    "\n",
    "#### c. Membuat Prediksi\n",
    "\n",
    "`model.predict()` digunakan untuk mendapatkan probabilitas kelas, dan `model.predict_classes()` (atau `np.argmax(model.predict(), axis=1)`) digunakan untuk mendapatkan kelas yang diprediksi.\n",
    "\n",
    "### 7. Membangun JST Kompleks Menggunakan Keras Functional API atau Subclassing API (Building Complex Models Using the Keras Functional API or the Subclassing API)\n",
    "\n",
    "Bagian ini memperkenalkan cara membangun arsitektur JST yang lebih kompleks yang tidak dapat direpresentasikan oleh model Sequential.\n",
    "\n",
    "#### a. Functional API (API Fungsional)\n",
    "Digunakan untuk model yang memiliki input non-sequential, output ganda, atau koneksi lompat (*skip connections*).\n",
    "* Input didefinisikan secara eksplisit (`tf.keras.layers.Input`).\n",
    "* Lapisan dihubungkan seperti fungsi (misalnya, `output_layer = Dense(...)(input_from_previous_layer)`).\n",
    "* `tf.keras.Model()` digunakan untuk membuat model.\n",
    "\n",
    "#### b. Subclassing API (API Subclassing)\n",
    "Memberikan fleksibilitas maksimum dengan memungkinkan Anda mewarisi dari `tf.keras.Model` dan mendefinisikan arsitektur model Anda dalam metode `__init__()` dan `call()`.\n",
    "* Metode `call()` mendefinisikan *forward pass* model.\n",
    "* Ini sangat berguna untuk model yang memiliki *loops*, logika bersyarat, atau perilaku dinamis lainnya.\n",
    "\n",
    "### 8. Menyimpan dan Memuat Model (Saving and Restoring Models)\n",
    "\n",
    "Keras menyediakan cara mudah untuk menyimpan model yang telah dilatih (`model.save()`) dan memuatnya kembali (`tf.keras.models.load_model()`). Ini sangat berguna untuk melanjutkan pelatihan atau menggunakan model yang sudah dilatih.\n",
    "\n",
    "### 9. Menggunakan Callbacks selama Pelatihan (Using Callbacks During Training)\n",
    "\n",
    "*Callbacks* adalah objek khusus yang dapat dipanggil oleh model pada berbagai titik selama pelatihan (misalnya, di awal/akhir *epoch*, sebelum/sesudah *batch*). Ini berguna untuk:\n",
    "* **Early Stopping:** Menghentikan pelatihan ketika *error* validasi tidak lagi membaik.\n",
    "* **Model Checkpointing:** Menyimpan *snapshot* model terbaik selama pelatihan.\n",
    "* **Kustomisasi:** Melakukan tindakan khusus (misalnya, *logging*, penyesuaian *learning rate*).\n",
    "\n",
    "Contoh `ModelCheckpoint` dan `EarlyStopping` ditunjukkan.\n",
    "\n",
    "### 10. Menggunakan TensorBoard untuk Visualisasi (Using TensorBoard for Visualization)\n",
    "\n",
    "TensorBoard adalah alat visualisasi interaktif yang disediakan oleh TensorFlow. Ini memungkinkan Anda melihat *learning curves*, memvisualisasikan grafik komputasi, dan menganalisis metrik lainnya selama dan setelah pelatihan.\n",
    "* `tf.keras.callbacks.TensorBoard()` digunakan untuk mengaktifkan *logging* data pelatihan ke direktori log.\n",
    "* TensorBoard kemudian dijalankan dari *command line* (`tensorboard --logdir=my_logs`).\n",
    "\n",
    "### 11. Penyetelan Hyperparameter (Fine-Tuning Neural Network Hyperparameters)\n",
    "\n",
    "*Hyperparameter* JST meliputi jumlah lapisan, jumlah neuron per lapisan, fungsi aktivasi, *learning rate*, *optimizer*, ukuran *batch*, jumlah *epoch*, dan banyak lagi. Menyetel *hyperparameter* adalah seni dan sains.\n",
    "\n",
    "* **Grid Search / Randomized Search:** Teknik standar untuk mencari *hyperparameter* optimal. Dapat menggunakan `GridSearchCV` atau `RandomizedSearchCV` dari Scikit-Learn dengan Keras.\n",
    "* **Keras Tuner:** Pustaka terpisah yang dirancang khusus untuk menyetel *hyperparameter* model Keras.\n",
    "\n",
    "### 12. Kesimpulan\n",
    "\n",
    "Bab 10 merupakan pengantar yang sangat baik untuk Jaringan Saraf Tiruan, meliputi dasar-dasar biologis dan matematis, arsitektur fundamental (Perceptron, MLP), serta algoritma pelatihan kunci (Backpropagation). Bagian implementasi dengan Keras sangat praktis, menunjukkan bagaimana membangun, melatih, mengevaluasi, menyimpan, dan menyetel JST. Pemahaman yang kokoh di bab ini adalah pondasi untuk eksplorasi lebih lanjut ke dalam *Deep Learning* di bab-bab berikutnya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6ab29",
   "metadata": {},
   "source": [
    "## 1. Building an Image Classifier Using the Keras Sequential API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521e496",
   "metadata": {},
   "source": [
    "### Importing TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a004fed",
   "metadata": {},
   "source": [
    "### Loading the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761a804",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb78e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126471fe",
   "metadata": {},
   "source": [
    "### Splitting data into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad44c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90465b",
   "metadata": {},
   "source": [
    "### Class names for Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76815af0",
   "metadata": {},
   "source": [
    "### Building the Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae57648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15944cef",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8aed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df549166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing layers, weights, and biases\n",
    "hidden1 = model.layers[1]\n",
    "hidden1.name\n",
    "model.get_layer(hidden1.name)\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b43af",
   "metadata": {},
   "source": [
    "### Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594887b4",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b857a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ed3f2",
   "metadata": {},
   "source": [
    "### Plotting Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b19e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f35c9f",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e5437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed3fb3",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538da4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1) # Alternative to model.predict_classes()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d850e14",
   "metadata": {},
   "source": [
    "## 2. Building Complex Models Using the Keras Functional API or the Subclassing API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1cde5",
   "metadata": {},
   "source": [
    "### Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(300, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(100, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(concat)\n",
    "model_functional = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d888661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_functional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model_functional, \"my_complex_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ed6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_functional.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                         optimizer=\"sgd\",\n",
    "                         metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bec521",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_functional = model_functional.fit(X_train, y_train, epochs=20,\n",
    "                                          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127de9a",
   "metadata": {},
   "source": [
    "### Model with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce894818",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model_multi_input = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb19a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_input.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dummy data for multi-input model (as X_train, y_train are for Fashion MNIST)\n",
    "# X_train_A, X_train_B = ... (dummy data)\n",
    "# X_valid_A, X_valid_B = ... (dummy data)\n",
    "# y_train_multi_input = ... (dummy data)\n",
    "# y_valid_multi_input = ... (dummy data)\n",
    "\n",
    "# history_multi_input = model_multi_input.fit((X_train_A, X_train_B), y_train_multi_input, epochs=10,\n",
    "#                                             validation_data=((X_valid_A, X_valid_B), y_valid_multi_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b902410",
   "metadata": {},
   "source": [
    "### Model with multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[6], name=\"input_A\")\n",
    "input_B = keras.layers.Input(shape=[8], name=\"input_B\")\n",
    "\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "\n",
    "output_main = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "output_aux = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "\n",
    "model_multi_output = keras.models.Model(inputs=[input_A, input_B],\n",
    "                                         outputs=[output_main, output_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae82a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_output.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dummy data for multi-output model\n",
    "# X_train_A, X_train_B = ... (dummy data)\n",
    "# X_valid_A, X_valid_B = ... (dummy data)\n",
    "# y_train_main, y_train_aux = ... (dummy data)\n",
    "# y_valid_main, y_valid_aux = ... (dummy data)\n",
    "\n",
    "# history_multi_output = model_multi_output.fit(\n",
    "#     (X_train_A, X_train_B), (y_train_main, y_train_aux), epochs=20,\n",
    "#     validation_data=((X_valid_A, X_valid_B), (y_valid_main, y_valid_aux)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3f166",
   "metadata": {},
   "source": [
    "### Subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequentialModel(keras.models.Sequential):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs)\n",
    "\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.dense2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.dense3 = keras.layers.Dense(10, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "# Example usage (needs data and compilation)\n",
    "# model_subclass = MyModel(name=\"my_model\")\n",
    "# model_subclass.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "# model_subclass.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602dc0bb",
   "metadata": {},
   "source": [
    "## 3. Saving and Restoring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfa688",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6899a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb35c5",
   "metadata": {},
   "source": [
    "## 4. Using Callbacks During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ecbc68",
   "metadata": {},
   "source": [
    "### Model Checkpointing and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history_callbacks = model.fit(X_train, y_train, epochs=100,\n",
    "                              validation_data=(X_valid, y_valid),\n",
    "                              callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4348c17b",
   "metadata": {},
   "source": [
    "## 5. Using TensorBoard for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef618bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history_tensorboard = model.fit(X_train, y_train, epochs=30,\n",
    "                                validation_data=(X_valid, y_valid),\n",
    "                                callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855aa4ed",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c5f5b",
   "metadata": {},
   "source": [
    "### Using GridSearchCV with Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df709025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Keras model inside a function\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[28, 28]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model in a Scikit-Learn compatible estimator\n",
    "keras_clf = keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f218571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [1, 2, 3],\n",
    "    \"n_neurons\": [50, 100, 150],\n",
    "    \"learning_rate\": [3e-3, 3e-2]\n",
    "}\n",
    "\n",
    "grid_search_cv = GridSearchCV(keras_clf, param_distribs, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_cv.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[early_stopping_cb]) # Using early stopping here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model from grid search\n",
    "best_model = grid_search_cv.best_estimator_.model\n",
    "best_model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
