{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bab 19: Training and Deploying TensorFlow Models (Melatih dan Menyebarkan Model TensorFlow)\n",
        "\n",
        "### 1. Pendahuluan\n",
        "\n",
        "Bab 19 ini adalah bab yang sangat praktis, berfokus pada transisi dari melatih model *Machine Learning* di lingkungan penelitian (misalnya, Jupyter Notebook) ke menyebarkannya di lingkungan produksi. Ini mencakup berbagai alat dan praktik terbaik untuk pelatihan skala besar, deployment, dan pemantauan. Meskipun buku ini berfokus pada TensorFlow 2, banyak konsep yang berlaku secara umum untuk alur kerja MLOps (Machine Learning Operations).\n",
        "\n",
        "Bab ini akan membahas:\n",
        "* **Distribusi Pelatihan:** Melatih model di banyak perangkat (GPU/TPU) atau di banyak mesin.\n",
        "* **TensorFlow Serving:** Menyebarkan model terlatih untuk inferensi.\n",
        "* **TensorFlow Lite:** Mengoptimalkan model untuk perangkat seluler dan *edge*.\n",
        "* **TensorFlow.js:** Menjalankan model di browser.\n",
        "\n",
        "### 2. Distribusi Pelatihan (Training at Scale)\n",
        "\n",
        "Melatih model *Deep Learning* yang besar atau pada dataset yang sangat besar membutuhkan strategi distribusi pelatihan.\n",
        "\n",
        "#### a. Paralelisme Data vs Paralelisme Model (Data Parallelism vs. Model Parallelism)\n",
        "* **Paralelisme Data:** Setiap perangkat (GPU/CPU/TPU) menerima *mini-batch* data yang berbeda, menghitung gradien secara independen, dan gradien-gradien ini kemudian digabungkan (misalnya, dirata-ratakan) untuk memperbarui bobot model. Model yang sama disalin ke setiap perangkat. Ini adalah strategi yang paling umum dan mudah diimplementasikan.\n",
        "* **Paralelisme Model:** Model dibagi menjadi beberapa bagian, dan setiap bagian ditempatkan pada perangkat yang berbeda. Data kemudian mengalir melalui perangkat secara berurutan. Ini lebih kompleks dan diperlukan ketika model terlalu besar untuk muat di satu perangkat.\n",
        "\n",
        "#### b. Strategi Distribusi TensorFlow (TensorFlow Distribution Strategies)\n",
        "TensorFlow 2.x menyediakan API yang mudah digunakan untuk distribusi pelatihan melalui `tf.distribute.Strategy`. Ini memungkinkan Anda menulis kode pelatihan standar, dan TensorFlow akan menanganinya secara paralel.\n",
        "\n",
        "* **`MirroredStrategy`:** Strategi paling umum untuk *single-host, multi-GPU training*. Ini membuat salinan model pada setiap GPU dan menggunakan *all-reduce* untuk mengumpulkan gradien.\n",
        "* **`MultiWorkerMirroredStrategy`:** Untuk *multi-host, multi-GPU training* (melatih di banyak mesin). Mirip dengan `MirroredStrategy` tetapi meluas ke beberapa server. Membutuhkan konfigurasi klaster.\n",
        "* **`TPUStrategy`:** Untuk melatih model pada *Tensor Processing Units* (TPUs) Google. TPU adalah akselerator khusus yang dioptimalkan untuk beban kerja *deep learning*.\n",
        "* **`CentralStorageStrategy`:** Bobot dan variabel disimpan di CPU pusat, dan operasi didistribusikan ke perangkat lain. Kurang efisien untuk GPU modern.\n",
        "* **`ParameterServerStrategy`:** Untuk skala yang sangat besar, di mana variabel model disimpan di *parameter servers* dan pekerja mengambil/memperbarui gradien.\n",
        "\n",
        "**Bagaimana Menggunakannya:** Anda membungkus kode pembangunan dan kompilasi model Anda dalam *scope* strategi distribusi.\n",
        "`with strategy.scope(): ...`\n",
        "\n",
        "#### c. Input Pipelines Terdistribusi (Distributed Input Pipelines)\n",
        "Sangat penting untuk memastikan *pipeline* input data juga efisien dalam lingkungan terdistribusi. `tf.data` API dapat terintegrasi dengan strategi distribusi untuk memastikan data di-*shard* dengan benar dan dibaca secara paralel.\n",
        "* `dataset.distribute_splits_from_function()`: Metode untuk membagi dataset di antara pekerja.\n",
        "\n",
        "### 3. TensorFlow Serving\n",
        "\n",
        "Setelah model dilatih, Anda perlu menyebarkannya agar dapat digunakan untuk inferensi dalam produksi. TensorFlow Serving adalah sistem yang fleksibel dan berkinerja tinggi yang dirancang untuk menyebarkan model *Machine Learning* dalam produksi.\n",
        "\n",
        "#### a. Mengekspor Model (Exporting Models)\n",
        "Model Keras dapat diekspor ke format **SavedModel** (`model.save(\"my_model_path\")`), yang merupakan format kanonis TensorFlow untuk menyimpan model yang siap produksi. SavedModel berisi grafik komputasi TensorFlow dan bobot model.\n",
        "\n",
        "#### b. Menjalankan TensorFlow Serving (Running TensorFlow Serving)\n",
        "TensorFlow Serving adalah server yang dapat menerima permintaan inferensi melalui REST API atau gRPC.\n",
        "* Anda menjalankan server TensorFlow Serving (biasanya dalam container Docker).\n",
        "* Server ini \"melayani\" model-model yang disimpan dalam format SavedModel dari direktori yang ditentukan.\n",
        "* Dapat melayani beberapa versi model secara bersamaan (untuk *A/B testing* atau *canary deployments*).\n",
        "\n",
        "#### c. Klien TensorFlow Serving (TensorFlow Serving Client)\n",
        "Aplikasi klien (misalnya, aplikasi Python, Node.js, Java) dapat mengirim permintaan inferensi ke server TensorFlow Serving.\n",
        "* Permintaan gRPC (lebih cepat) atau REST (lebih mudah digunakan) dapat dikirim dengan data input.\n",
        "* Server akan mengembalikan prediksi model.\n",
        "\n",
        "### 4. TensorFlow Lite\n",
        "\n",
        "TensorFlow Lite adalah kerangka kerja ringan yang dirancang untuk menjalankan model TensorFlow pada perangkat seluler, *embedded devices*, dan perangkat IoT (Internet of Things). Ini memungkinkan inferensi *on-device* dengan latensi rendah.\n",
        "\n",
        "#### a. Konverter TensorFlow Lite (TensorFlow Lite Converter)\n",
        "* Anda mengkonversi model TensorFlow yang sudah dilatih (dalam format SavedModel atau Keras H5) ke format TensorFlow Lite (`.tflite`).\n",
        "* Konverter ini melakukan optimisasi:\n",
        "    * **Quantization:** Mengurangi presisi numerik bobot dan/atau aktivasi (misalnya, dari float32 ke int8) untuk mengurangi ukuran model dan mempercepat komputasi.\n",
        "    * **Pruning:** Menghapus bobot yang tidak signifikan.\n",
        "    * **Graph Freezing:** Menggabungkan variabel menjadi konstanta.\n",
        "* Pustaka `tf.lite.TFLiteConverter` digunakan.\n",
        "\n",
        "#### b. Interpreter TensorFlow Lite (TensorFlow Lite Interpreter)\n",
        "* Setelah dikonversi, model `.tflite` dapat dimuat dan dijalankan menggunakan `tf.lite.Interpreter` di perangkat target.\n",
        "* Mendukung berbagai bahasa pemrograman (Java, Swift/Objective-C, C++, Python).\n",
        "\n",
        "### 5. TensorFlow.js\n",
        "\n",
        "TensorFlow.js adalah pustaka JavaScript untuk melatih dan menyebarkan model *Machine Learning* di browser web atau di Node.js.\n",
        "\n",
        "#### a. Konversi Model (Model Conversion)\n",
        "* Model Keras atau TensorFlow yang sudah dilatih dapat dikonversi ke format TensorFlow.js.\n",
        "* Pustaka `tensorflowjs_converter` digunakan (instal via `pip`).\n",
        "\n",
        "#### b. Menjalankan Model di Browser (Running Models in the Browser)\n",
        "* Model yang dikonversi dapat dimuat di JavaScript menggunakan `tf.loadLayersModel()` atau `tf.loadGraphModel()`.\n",
        "* Inferensi kemudian dapat dilakukan di sisi klien, memanfaatkan GPU browser (melalui WebGL).\n",
        "* **Manfaat:** Privasi (data tetap di perangkat pengguna), latensi rendah (tidak perlu bolak-balik ke server), interaktivitas.\n",
        "* **Kekurangan:** Batasan ukuran model dan komputasi oleh kemampuan perangkat pengguna dan browser.\n",
        "\n",
        "### 6. TensorFlow Extended (TFX) (Gambaran Singkat)\n",
        "\n",
        "TFX adalah *platform* end-to-end untuk membangun dan mengelola *pipeline* ML produksi. Ini menyediakan komponen untuk validasi data, *feature engineering*, pelatihan, evaluasi, dan penyebaran model. Ini melampaui cakupan detail buku ini, tetapi penting untuk MLOps yang matang.\n",
        "\n",
        "### 7. Kesimpulan\n",
        "\n",
        "Bab 19 menyediakan panduan penting untuk membawa model *Deep Learning* dari eksperimen ke produksi. Ini membahas strategi distribusi pelatihan TensorFlow untuk skala besar, memperkenalkan TensorFlow Serving sebagai solusi *deployment* yang kuat, dan menjelaskan TensorFlow Lite serta TensorFlow.js untuk inferensi *on-device* dan di browser. Pemahaman tentang alat-alat ini sangat penting bagi setiap insinyur ML yang ingin membangun dan mengelola sistem *deep learning* dunia nyata."
      ],
      "metadata": {
        "id": "O1pvNE09k2pf"
      },
      "id": "O1pvNE09k2pf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "YeNad2XVk33r"
      },
      "id": "YeNad2XVk33r"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil # For removing directories"
      ],
      "metadata": {
        "id": "E3mRAmOpk5NJ"
      },
      "id": "E3mRAmOpk5NJ",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Fashion MNIST (as used in previous chapters)\n"
      ],
      "metadata": {
        "id": "e5Cc5MJik6Bk"
      },
      "id": "e5Cc5MJik6Bk"
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "metadata": {
        "id": "Ea3uB1qtk6pp"
      },
      "id": "Ea3uB1qtk6pp",
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training at Scale"
      ],
      "metadata": {
        "id": "L7NHmlokk7kP"
      },
      "id": "L7NHmlokk7kP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Parallelism with MirroredStrategy (Single-host, multi-GPU)"
      ],
      "metadata": {
        "id": "4lU9Kes5k8hv"
      },
      "id": "4lU9Kes5k8hv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for available GPUs\n",
        "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "# Create a MirroredStrategy\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# Build and compile the model within the strategy's scope\n",
        "with strategy.scope():\n",
        "    model_mirrored = keras.models.Sequential([\n",
        "        keras.layers.Flatten(input_shape=[28, 28]),\n",
        "        keras.layers.Dense(300, activation=\"relu\"),\n",
        "        keras.layers.Dense(100, activation=\"relu\"),\n",
        "        keras.layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "    model_mirrored.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                           optimizer=\"adam\",\n",
        "                           metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model (data will be automatically sharded across devices)\n",
        "model_mirrored.fit(X_train, y_train, epochs=10,\n",
        "                   validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjRNLSkAk9TO",
        "outputId": "3725302a-8d01-4ea5-db7b-0c3551e42867"
      },
      "id": "pjRNLSkAk9TO",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7809 - loss: 0.6090 - val_accuracy: 0.8602 - val_loss: 0.3751\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.8599 - loss: 0.3764 - val_accuracy: 0.8668 - val_loss: 0.3626\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.8736 - loss: 0.3344 - val_accuracy: 0.8834 - val_loss: 0.3192\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.8873 - loss: 0.3006 - val_accuracy: 0.8840 - val_loss: 0.3169\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.8967 - loss: 0.2803 - val_accuracy: 0.8890 - val_loss: 0.3184\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9001 - loss: 0.2655 - val_accuracy: 0.8828 - val_loss: 0.3175\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9059 - loss: 0.2495 - val_accuracy: 0.8906 - val_loss: 0.3044\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.9062 - loss: 0.2436 - val_accuracy: 0.8934 - val_loss: 0.3044\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.9109 - loss: 0.2331 - val_accuracy: 0.8904 - val_loss: 0.3191\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9161 - loss: 0.2240 - val_accuracy: 0.8976 - val_loss: 0.2999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa9af8bbb10>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiWorkerMirroredStrategy (Multi-host, multi-GPU)"
      ],
      "metadata": {
        "id": "hQlvOeKvk-mJ"
      },
      "id": "hQlvOeKvk-mJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# This requires environment variable TF_CONFIG to be set up\n",
        "# Example TF_CONFIG for worker 0 (assuming two workers):\n",
        "# os.environ['TF_CONFIG'] = \"\"\"\n",
        "# {\n",
        "#     \"cluster\": {\n",
        "#         \"worker\": [\"localhost:12345\", \"localhost:12346\"]\n",
        "#     },\n",
        "#     \"task\": {\"type\": \"worker\", \"index\": 0}\n",
        "# }\n",
        "# \"\"\"\n",
        "\n",
        "# This strategy also needs a distributed dataset\n",
        "def create_dataset(x_data, y_data, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).shuffle(len(x_data)).repeat()\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "multi_worker_strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "\n",
        "with multi_worker_strategy.scope():\n",
        "    model_multi_worker = keras.models.Sequential([\n",
        "        keras.layers.Flatten(input_shape=[28, 28]),\n",
        "        keras.layers.Dense(300, activation=\"relu\"),\n",
        "        keras.layers.Dense(100, activation=\"relu\"),\n",
        "        keras.layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "    model_multi_worker.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                               optimizer=\"adam\",\n",
        "                               metrics=[\"accuracy\"])\n",
        "\n",
        "# You would typically use `tf.data.Dataset` for input and `steps_per_epoch` for fit\n",
        "train_dataset_mw = create_dataset(X_train, y_train, batch_size=64)\n",
        "val_dataset_mw = create_dataset(X_valid, y_valid, batch_size=64)\n",
        "\n",
        "model_multi_worker.fit(train_dataset_mw, epochs=10,\n",
        "                      steps_per_epoch=len(X_train) // 64,\n",
        "                      validation_data=val_dataset_mw,\n",
        "                      validation_steps=len(X_valid) // 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRDLN1HIk_Wh",
        "outputId": "d7508b8c-dd89-4be7-f0df-dfe3440c256d"
      },
      "id": "eRDLN1HIk_Wh",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.7748 - loss: 0.6374 - val_accuracy: 0.8632 - val_loss: 0.3835\n",
            "Epoch 2/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.8628 - loss: 0.3762 - val_accuracy: 0.8718 - val_loss: 0.3545\n",
            "Epoch 3/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.8800 - loss: 0.3271 - val_accuracy: 0.8852 - val_loss: 0.3131\n",
            "Epoch 4/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.8863 - loss: 0.3054 - val_accuracy: 0.8866 - val_loss: 0.3149\n",
            "Epoch 5/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.8941 - loss: 0.2854 - val_accuracy: 0.8882 - val_loss: 0.3178\n",
            "Epoch 6/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8986 - loss: 0.2661 - val_accuracy: 0.8870 - val_loss: 0.3095\n",
            "Epoch 7/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9056 - loss: 0.2555 - val_accuracy: 0.8910 - val_loss: 0.3094\n",
            "Epoch 8/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9085 - loss: 0.2425 - val_accuracy: 0.8866 - val_loss: 0.3195\n",
            "Epoch 9/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9132 - loss: 0.2337 - val_accuracy: 0.8902 - val_loss: 0.3030\n",
            "Epoch 10/10\n",
            "\u001b[1m859/859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9151 - loss: 0.2220 - val_accuracy: 0.8970 - val_loss: 0.2955\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa9af11d510>"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. TensorFlow Serving"
      ],
      "metadata": {
        "id": "0IhHklYBlAWh"
      },
      "id": "0IhHklYBlAWh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting the Model (SavedModel format)"
      ],
      "metadata": {
        "id": "bMZYiT2flBKf"
      },
      "id": "bMZYiT2flBKf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple model to export\n",
        "model_export = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_export.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                     optimizer=\"adam\",\n",
        "                     metrics=[\"accuracy\"])\n",
        "# Train briefly\n",
        "model_export.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Define the export path\n",
        "# A good practice is to create a versioned directory\n",
        "model_version = \"001\" # Or a timestamp\n",
        "export_path = os.path.join(\"my_fashion_mnist_model_for_serving\", model_version)\n",
        "\n",
        "# Remove the directory if it already exists (for clean export)\n",
        "if os.path.isdir(export_path):\n",
        "    shutil.rmtree(export_path)\n",
        "\n",
        "tf.saved_model.save(model_export, export_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQSVGJvIlB4m",
        "outputId": "86335500-dd80-4bff-bef0-f98f4189a610"
      },
      "id": "ZQSVGJvIlB4m",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7844 - loss: 0.5996 - val_accuracy: 0.8602 - val_loss: 0.3872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the SavedModel (Optional)"
      ],
      "metadata": {
        "id": "52P-NgP4lC2Y"
      },
      "id": "52P-NgP4lC2Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# To load and test the exported model\n",
        "loaded_model = tf.saved_model.load(export_path)\n",
        "print(list(loaded_model.signatures.keys())) # Should show 'serving_default'\n",
        "infer = loaded_model.signatures[\"serving_default\"]\n",
        "dummy_input = tf.constant(X_test[:1].reshape(1, 28, 28), dtype=tf.float32)\n",
        "# Pass the dummy_input tensor directly to the infer function\n",
        "predictions = infer(dummy_input)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upXgBXeHlDnF",
        "outputId": "10d99c64-f92f-4e2f-c580-2cc3cd6bd1c1"
      },
      "id": "upXgBXeHlDnF",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['serving_default']\n",
            "{'output_0': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
            "array([[2.6912394e-05, 2.8693483e-05, 5.7107441e-06, 6.0593011e-06,\n",
            "        2.3995747e-06, 3.2625917e-02, 1.2455772e-05, 4.8775226e-02,\n",
            "        2.7604532e-04, 9.1824061e-01]], dtype=float32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running TensorFlow Serving (Conceptual, requires Docker)"
      ],
      "metadata": {
        "id": "_POr_gIOlE0S"
      },
      "id": "_POr_gIOlE0S"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Start a Docker container for TensorFlow Serving:\n",
        "#    docker pull tensorflow/serving\n",
        "#    docker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=\"$(pwd)/my_fashion_mnist_model_for_serving\",target=/models/fashion_mnist -e MODEL_NAME=fashion_mnist -t tensorflow/serving &\n",
        "\n",
        "# 2. To test with curl (HTTP REST API):\n",
        "#    curl -d '{\"instances\": [[[0.0, ..., 0.0], ...]]]}' -X POST http://localhost:8501/v1/models/fashion_mnist:predict\n",
        "\n",
        "# 3. Python client example (using requests library)\n",
        "# import requests\n",
        "# import json\n",
        "\n",
        "# def make_prediction_request(model_name, data):\n",
        "#     # Convert input data to a list of lists for JSON\n",
        "#     data_list = data.tolist() # Assuming data is a NumPy array\n",
        "#     request_body = {\"instances\": data_list}\n",
        "#     headers = {\"content-type\": \"application/json\"}\n",
        "#     json_response = requests.post(\n",
        "#         f\"http://localhost:8501/v1/models/{model_name}:predict\",\n",
        "#         data=json.dumps(request_body),\n",
        "#         headers=headers\n",
        "#     )\n",
        "#     response = json.loads(json_response.text)\n",
        "#     return response\n",
        "\n",
        "# # Use a sample from X_test (reshape to 1, 28, 28)\n",
        "# sample_image = X_test[0].reshape(1, 28, 28)\n",
        "# predictions = make_prediction_request(\"fashion_mnist\", sample_image)\n",
        "# print(predictions)"
      ],
      "metadata": {
        "id": "oP03j3htlFxU"
      },
      "id": "oP03j3htlFxU",
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. TensorFlow Lite"
      ],
      "metadata": {
        "id": "e8GhizkAlGoD"
      },
      "id": "e8GhizkAlGoD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting a Keras Model to TensorFlow Lite (.tflite)"
      ],
      "metadata": {
        "id": "8JcAabIRlHZm"
      },
      "id": "8JcAabIRlHZm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple Keras model (already done above as model_export)\n",
        "model_lite = model_export\n",
        "\n",
        "# Create a TFLiteConverter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_export)\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model to a file\n",
        "tflite_model_path = \"my_fashion_mnist_model.tflite\"\n",
        "with open(tflite_model_path, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO-qZil6lICl",
        "outputId": "4e073d6b-77a8-40f5-8442-31d4a27eb02d"
      },
      "id": "uO-qZil6lICl",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmplhuuynhx'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_310')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  140366648548368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648551824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648552784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648553744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648552976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648554512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running a TensorFlow Lite Model (on CPU, for testing)"
      ],
      "metadata": {
        "id": "g6zZMPRFlI4l"
      },
      "id": "g6zZMPRFlI4l"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "\n",
        "# Allocate tensors (necessary step)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Prepare input data (e.g., from X_test)\n",
        "# Input type might need to be cast if quantize was used (e.g., to int8)\n",
        "# Reshape the input data to match the expected 3D input shape (batch_size, height, width)\n",
        "# This matches the input_shape=[28, 28] in the Flatten layer definition\n",
        "input_data = tf.constant(X_test[0].reshape(1, 28, 28), dtype=tf.float32)\n",
        "\n",
        "# Set the tensor\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# Invoke the interpreter (run inference)\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output tensor\n",
        "tflite_predictions = interpreter.get_tensor(output_details[0]['index'])\n",
        "# print(tflite_predictions)"
      ],
      "metadata": {
        "id": "8xRwKitAlJgY"
      },
      "id": "8xRwKitAlJgY",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization (Optimization for TFLite)"
      ],
      "metadata": {
        "id": "MbhFAE6DlKgS"
      },
      "id": "MbhFAE6DlKgS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert with default optimizations (quantization for weights)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model_quantized = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open(\"my_fashion_mnist_model_quantized.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbB_yTVKlLWt",
        "outputId": "35cb9fa9-743b-4160-9864-073e0b50717d"
      },
      "id": "JbB_yTVKlLWt",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpsfiph0dq'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_310')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  140366648548368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648551824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648552784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648553744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648552976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648554512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full integer quantization (requires representative dataset)\n",
        "def representative_data_gen():\n",
        "    # Iterate through the batched dataset\n",
        "    for input_value in tf.data.Dataset.from_tensor_slices(X_train_full).batch(1).take(100):\n",
        "        # Yield the input value directly.\n",
        "        # The batch(1) ensures the shape is (1, 28, 28)\n",
        "        # tf.cast to tf.float32 is already done in the previous cell for model_export.\n",
        "        # However, it's good practice to ensure the data type is float32 for conversion.\n",
        "        yield [tf.cast(input_value, tf.float32)]\n",
        "\n",
        "# The rest of the conversion code remains the same\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_export) # Re-initialize the converter if needed for fresh settings\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Apply default optimizations including quantization\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "\n",
        "tflite_model_full_int = converter.convert()\n",
        "with open(\"my_fashion_mnist_model_full_int.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_full_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQeH_YnalMDE",
        "outputId": "7164f06d-baef-4e43-d9a3-98c3cfcbaaa0"
      },
      "id": "tQeH_YnalMDE",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp_yr0zhr8'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_310')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  140366648548368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648551824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648552784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648553744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648552976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140366648554512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. TensorFlow.js (Conceptual, requires Node.js and browser)"
      ],
      "metadata": {
        "id": "R028dw5QlM7R"
      },
      "id": "R028dw5QlM7R"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install tensorflowjs converter:\n",
        "#    pip install tensorflowjs\n",
        "\n",
        "# 2. Convert a SavedModel or Keras H5 model:\n",
        "#    tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model \\\n",
        "#                           ./my_fashion_mnist_model_for_serving/001 ./tfjs_model/fashion_mnist_graph_model\n",
        "\n",
        "#    Or for Keras H5:\n",
        "#    tensorflowjs_converter --input_format=keras --output_format=tfjs_layers_model \\\n",
        "#                           ./my_keras_model.h5 ./tfjs_model/fashion_mnist_layers_model\n",
        "\n",
        "# 3. Serve in a web browser (requires simple HTML/JS setup):\n",
        "#    <script src=\"[https://cdn.jsdelivr.net/npm/@tensorflow/tfjs](https://cdn.jsdelivr.net/npm/@tensorflow/tfjs)\"></script>\n",
        "#    <script>\n",
        "#        async function loadAndPredict() {\n",
        "#            const model = await tf.loadLayersModel('tfjs_model/fashion_mnist_layers_model/model.json');\n",
        "#            // Prepare input tensor (e.g., from an image)\n",
        "#            const inputTensor = tf.tensor2d([[...your_28x28_pixel_data...]]);\n",
        "#            const predictions = model.predict(inputTensor);\n",
        "#            predictions.array().then(arr => console.log(arr));\n",
        "#        }\n",
        "#        loadAndPredict();\n",
        "#    </script>"
      ],
      "metadata": {
        "id": "3Te25I2PlNtn"
      },
      "id": "3Te25I2PlNtn",
      "execution_count": 125,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}