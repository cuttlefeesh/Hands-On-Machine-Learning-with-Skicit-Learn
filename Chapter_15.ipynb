{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laporan Analisis Bab 15: Processing Sequences Using Recurrent Neural Networks and Attention (Memproses Urutan Menggunakan Jaringan Saraf Berulang dan Atensi)\n",
        "\n",
        "**Judul Bab:** Processing Sequences Using Recurrent Neural Networks and Attention\n",
        "**Penulis:** Aurélien Géron\n",
        "**Sumber:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition (O'Reilly)\n",
        "\n",
        "### 1. Pendahuluan\n",
        "\n",
        "Bab 15 ini membahas Jaringan Saraf Tiruan (JST) yang dirancang khusus untuk memproses data sekuensial atau urutan, seperti teks, suara, video, atau deret waktu. **Recurrent Neural Networks (RNNs)** adalah jenis JST yang memiliki \"memori\" internal yang memungkinkan mereka mempertahankan informasi dari langkah waktu sebelumnya, menjadikannya sangat cocok untuk tugas-tugas yang melibatkan dependensi temporal. Bab ini juga akan memperkenalkan mekanisme **Attention**, sebuah inovasi kunci yang membantu RNN (dan kemudian Transformer) mengatasi keterbatasan tertentu.\n",
        "\n",
        "Aplikasi utama RNN:\n",
        "* Pengenalan Suara\n",
        "* Penerjemahan Mesin\n",
        "* Analisis Sentimen\n",
        "* Prediksi Deret Waktu\n",
        "* Generasi Teks/Musik\n",
        "\n",
        "### 2. Memproses Urutan Menggunakan RNN (Processing Sequences Using RNNs)\n",
        "\n",
        "RNN adalah jaringan yang memiliki koneksi yang berulang, memungkinkan informasi mengalir dari satu langkah waktu ke langkah waktu berikutnya.\n",
        "\n",
        "#### a. Arsitektur Memori (Memory Cells)\n",
        "* Setiap neuron dalam lapisan RNN menerima input dari lapisan sebelumnya *dan* output dari dirinya sendiri dari langkah waktu sebelumnya (atau dari neuron lain di lapisan yang sama).\n",
        "* Output dari neuron RNN pada langkah waktu $t$ tidak hanya bergantung pada input pada $t$, tetapi juga pada state tersembunyi (*hidden state*) dari langkah waktu $t-1$. State tersembunyi ini bertindak sebagai \"memori\" jaringan.\n",
        "* Sifat rekuren ini memungkinkan jaringan mengingat konteks dari input sebelumnya.\n",
        "\n",
        "#### b. Input dan Output Urutan (Sequence Input and Output)\n",
        "RNN dapat dikonfigurasi untuk berbagai tugas:\n",
        "* **Sequence-to-Sequence (Encoder-Decoder):** Input adalah urutan, output juga urutan (misalnya, penerjemahan mesin).\n",
        "* **Sequence-to-Vector:** Input adalah urutan, output adalah vektor tunggal (misalnya, klasifikasi sentimen).\n",
        "* **Vector-to-Sequence:** Input adalah vektor tunggal, output adalah urutan (misalnya, pembuatan teks dari genre).\n",
        "\n",
        "#### c. Recurrent Layer Sederhana (A Simple Recurrent Layer)\n",
        "Struktur dasar dari satu sel RNN sederhana ditunjukkan.\n",
        "Pada langkah waktu $t$, output state tersembunyi $h_t$ dihitung sebagai:\n",
        "$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$\n",
        "di mana $f$ adalah fungsi aktivasi, $W$ adalah matriks bobot, $x_t$ adalah input pada langkah $t$, $h_{t-1}$ adalah state tersembunyi dari langkah $t-1$, dan $b$ adalah bias.\n",
        "\n",
        "Keras menyediakan lapisan `keras.layers.SimpleRNN` untuk implementasi dasar RNN.\n",
        "\n",
        "#### d. RNNs untuk Prediksi Deret Waktu (RNNs for Time Series Forecasting)\n",
        "Contoh penggunaan RNN untuk memprediksi nilai berikutnya dalam deret waktu ditunjukkan. Data deret waktu seringkali berupa data univariat (satu fitur) atau multivariat.\n",
        "\n",
        "#### e. Training RNNs (Melatih RNN)\n",
        "Pelatihan RNN biasanya dilakukan menggunakan **Backpropagation Through Time (BPTT)**. Ini mirip dengan *backpropagation* biasa, tetapi diterapkan pada jaringan yang \"tidak digulung\" (*unrolled*) sepanjang sumbu waktu.\n",
        "* Masalah *vanishing/exploding gradients* lebih parah di RNN karena efeknya terakumulasi sepanjang waktu.\n",
        "* *Gradient clipping* adalah teknik penting untuk mengatasi *exploding gradients* di RNN.\n",
        "\n",
        "### 3. Mengatasi Vanishing Gradients (Tackling the Vanishing Gradients Problem)\n",
        "\n",
        "Masalah *vanishing gradients* sangat signifikan di RNN karena informasi dari langkah waktu yang jauh cenderung \"luntur\" seiring berjalannya waktu. Beberapa jenis *memory cell* yang lebih kompleks telah dikembangkan untuk mengatasi ini:\n",
        "\n",
        "#### a. Long Short-Term Memory (LSTM) Cells (Sel Long Short-Term Memory)\n",
        "* LSTM adalah jenis *memory cell* yang paling populer dan efektif.\n",
        "* Ini menambahkan \"jalur memori\" (*memory path*) yang memungkinkan informasi mengalir tanpa banyak perubahan, sehingga informasi penting dari masa lalu dapat dipertahankan untuk jangka waktu yang lebih lama.\n",
        "* LSTM memiliki beberapa \"gerbang\" (gates) yang dikontrol secara adaptif:\n",
        "    * **Forget Gate:** Mengontrol berapa banyak informasi dari state memori sebelumnya yang harus dilupakan.\n",
        "    * **Input Gate:** Mengontrol berapa banyak input saat ini yang harus disimpan dalam state memori.\n",
        "    * **Output Gate:** Mengontrol berapa banyak state memori yang harus dibuka ke output.\n",
        "* `keras.layers.LSTM` adalah lapisan Keras untuk LSTM.\n",
        "\n",
        "#### b. Gated Recurrent Unit (GRU) Cells (Sel Gated Recurrent Unit)\n",
        "* GRU adalah versi yang sedikit disederhanakan dari LSTM, menggabungkan *forget gate* dan *input gate* menjadi satu \"update gate\", dan menggabungkan *hidden state* dengan *memory state*.\n",
        "* GRU biasanya berkinerja serupa dengan LSTM tetapi lebih cepat dihitung dan membutuhkan lebih sedikit parameter.\n",
        "* `keras.layers.GRU` adalah lapisan Keras untuk GRU.\n",
        "\n",
        "#### c. Sel dan Lapisan Kustom (Custom Cells and Layers)\n",
        "Keras memungkinkan Anda membuat *memory cell* kustom dengan mewarisi dari `keras.layers.Layer` dan mengimplementasikan logika *recurrent* di metode `call()`.\n",
        "\n",
        "#### d. RNN Lapisan Dalam (Deep RNNs)\n",
        "RNN dapat dibuat lebih dalam dengan menumpuk beberapa lapisan *recurrent* (misalnya, LSTM atau GRU). Ini memungkinkan jaringan untuk mempelajari pola hierarkis.\n",
        "\n",
        "#### e. RNN Dua Arah (Bidirectional RNNs)\n",
        "Untuk beberapa tugas (misalnya, pemrosesan bahasa alami), berguna bagi RNN untuk memproses urutan tidak hanya ke depan tetapi juga ke belakang.\n",
        "* **Bi-RNN** terdiri dari dua lapisan RNN terpisah (satu ke depan, satu ke belakang) yang outputnya digabungkan.\n",
        "* `keras.layers.Bidirectional` digunakan untuk membungkus lapisan RNN lainnya.\n",
        "\n",
        "#### f. RNN Encoder-Decoder (Encoder-Decoder RNNs)\n",
        "Arsitektur ini digunakan untuk tugas *sequence-to-sequence* (misalnya, penerjemahan mesin).\n",
        "* **Encoder:** Sebuah RNN yang memproses urutan input dan mengkompresnya menjadi satu vektor *context* tetap (state tersembunyi terakhir).\n",
        "* **Decoder:** Sebuah RNN yang mengambil vektor *context* ini sebagai input awal dan menghasilkan urutan output.\n",
        "* Tantangan: Vektor *context* tunggal mungkin menjadi *bottleneck* informasi untuk urutan yang sangat panjang.\n",
        "\n",
        "#### g. Attention Mechanisms (Mekanisme Atensi)\n",
        "Mekanisme *attention* diperkenalkan untuk mengatasi *bottleneck* pada arsitektur *encoder-decoder* untuk urutan panjang.\n",
        "* Daripada hanya meneruskan satu vektor *context* tunggal dari *encoder* ke *decoder*, *attention* memungkinkan *decoder* untuk \"melihat\" dan memberikan bobot berbeda pada bagian-bagian yang relevan dari urutan input pada setiap langkah waktu output.\n",
        "* Ini membantu model memfokuskan perhatiannya pada bagian-bagian yang paling penting dari input saat menghasilkan output, seperti yang dilakukan manusia saat menerjemahkan atau membaca.\n",
        "* Ini adalah fondasi dari model Transformer.\n",
        "\n",
        "### 4. Transformer Networks (Jaringan Transformer)\n",
        "\n",
        "*Transformer Networks* adalah arsitektur *deep learning* yang sangat revolusioner, diperkenalkan pada tahun 2017, yang telah menggantikan RNN sebagai *state-of-the-art* dalam banyak tugas pemrosesan urutan, terutama *Natural Language Processing* (NLP).\n",
        "\n",
        "* **Self-Attention:** Komponen kunci Transformer adalah mekanisme *self-attention* (juga dikenal sebagai *multi-head attention*). Ini memungkinkan setiap elemen dalam urutan untuk secara langsung berinteraksi dan mempertimbangkan semua elemen lain dalam urutan untuk memahami konteksnya, tanpa dependensi rekuren.\n",
        "* **Encoder-Decoder Architecture:** Transformer juga menggunakan arsitektur *encoder-decoder*, tetapi sepenuhnya didasarkan pada *attention* dan *feedforward layers*, tanpa rekurensi.\n",
        "* **Positional Encoding:** Karena Transformer tidak memiliki rekurensi, informasi posisi elemen dalam urutan ditambahkan melalui *positional encoding*.\n",
        "* **Keuntungan:** Komputasi paralel yang tinggi (tidak ada rekurensi), mampu menangani dependensi jarak jauh lebih baik.\n",
        "\n",
        "### 5. Kesimpulan\n",
        "\n",
        "Bab 15 adalah pengantar yang sangat penting untuk memproses data sekuensial menggunakan JST. Ini mencakup RNN dasar, masalah *vanishing gradients*, dan solusi canggih seperti LSTM dan GRU. Pemahaman tentang arsitektur *encoder-decoder* dan terutama mekanisme *Attention* adalah kunci. Terakhir, pengenalan singkat tentang Transformer Networks mengisyaratkan era baru dalam pemrosesan urutan yang didominasi oleh *attention*."
      ],
      "metadata": {
        "id": "aHER73JTbUAy"
      },
      "id": "aHER73JTbUAy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n"
      ],
      "metadata": {
        "id": "HSL7ClRXbYf1"
      },
      "id": "HSL7ClRXbYf1"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ThHenGIEbcWU"
      },
      "id": "ThHenGIEbcWU",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. RNNs for Time Series Forecasting"
      ],
      "metadata": {
        "id": "c-ojefsMbd5l"
      },
      "id": "c-ojefsMbd5l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating a time series dataset"
      ],
      "metadata": {
        "id": "EfZ7hDT8bey1"
      },
      "id": "EfZ7hDT8bey1"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))  # wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)  # noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "metadata": {
        "id": "SpGd0V5Hbg1d"
      },
      "id": "SpGd0V5Hbg1d",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
      ],
      "metadata": {
        "id": "5mGT-dWcbhqT"
      },
      "id": "5mGT-dWcbhqT",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline models (Naive Forecasting, Linear Regression)"
      ],
      "metadata": {
        "id": "Q9cb9f6IbjFi"
      },
      "id": "Q9cb9f6IbjFi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive forecasting (predicting last value)\n",
        "y_pred_naive = X_valid[:, -1]\n",
        "\n",
        "# Instantiate the MeanSquaredError metric\n",
        "mse_metric = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "# Update the state and get the result\n",
        "mse_metric.update_state(y_valid, y_pred_naive)\n",
        "print(mse_metric.result().numpy()) # Use .numpy() to get a scalar value\n",
        "\n",
        "# Simple Linear Regression (not shown in book for time series directly, but possible)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "# Reshape X_train and X_valid to be 2D arrays (samples, features) for scikit-learn\n",
        "lin_reg.fit(X_train.reshape(-1, n_steps), y_train)\n",
        "y_pred_linear = lin_reg.predict(X_valid.reshape(-1, n_steps))\n",
        "\n",
        "# Instantiate another MeanSquaredError metric for linear regression prediction\n",
        "mse_metric_linear = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "# Update the state and get the result\n",
        "mse_metric_linear.update_state(y_valid, y_pred_linear)\n",
        "print(mse_metric_linear.result().numpy()) # Use .numpy() to get a scalar value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg-XnEU7bjz_",
        "outputId": "0be39636-fec3-4259-f25b-d5fca0c03b5e"
      },
      "id": "wg-XnEU7bjz_",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.021102622\n",
            "0.003097598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple RNN"
      ],
      "metadata": {
        "id": "VXq0FBJUbk9X"
      },
      "id": "VXq0FBJUbk9X"
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1]) # Output one value (the next step in series)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W_p0qCsbl0u",
        "outputId": "53372de3-6615-4d8c-9680-86ec82a520ae"
      },
      "id": "9W_p0qCsbl0u",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCOMJmRqbnG2",
        "outputId": "3c9f52a9-ea09-4d0e-d158-cb24ff28f9e9"
      },
      "id": "PCOMJmRqbnG2",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.4722 - val_loss: 0.1698\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.1539 - val_loss: 0.1492\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.1456 - val_loss: 0.1110\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0902 - val_loss: 0.0530\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0483 - val_loss: 0.0373\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0345 - val_loss: 0.0296\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0280 - val_loss: 0.0249\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0237 - val_loss: 0.0216\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0201 - val_loss: 0.0192\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0184 - val_loss: 0.0174\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0162 - val_loss: 0.0160\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0150 - val_loss: 0.0149\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0140 - val_loss: 0.0140\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0129 - val_loss: 0.0133\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0124 - val_loss: 0.0128\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0120 - val_loss: 0.0124\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0114 - val_loss: 0.0121\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0113 - val_loss: 0.0119\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0109 - val_loss: 0.0118\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0111 - val_loss: 0.0118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep RNNs (Stacked RNNs)"
      ],
      "metadata": {
        "id": "Wtu2qFD8boGm"
      },
      "id": "Wtu2qFD8boGm"
    },
    {
      "cell_type": "code",
      "source": [
        "model_deep = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True), # Another RNN layer\n",
        "    keras.layers.SimpleRNN(1) # Final RNN layer outputting single value\n",
        "])"
      ],
      "metadata": {
        "id": "d2UuWa-Hbo8S"
      },
      "id": "d2UuWa-Hbo8S",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train (example)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
        "model_deep.compile(loss=\"mse\", optimizer=optimizer)\n",
        "history_deep = model_deep.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW6j07Dxbp8Z",
        "outputId": "0d9b36c9-1732-422d-cdeb-64eceaa10398"
      },
      "id": "hW6j07Dxbp8Z",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.0187 - val_loss: 0.0037\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - loss: 0.0038 - val_loss: 0.0035\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - loss: 0.0036 - val_loss: 0.0040\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - loss: 0.0032 - val_loss: 0.0037\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - loss: 0.0035 - val_loss: 0.0030\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - loss: 0.0030 - val_loss: 0.0037\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - loss: 0.0031 - val_loss: 0.0042\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0034 - val_loss: 0.0031\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - loss: 0.0031 - val_loss: 0.0033\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - loss: 0.0031 - val_loss: 0.0039\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - loss: 0.0032 - val_loss: 0.0028\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - loss: 0.0026 - val_loss: 0.0028\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - loss: 0.0026 - val_loss: 0.0030\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 0.0028 - val_loss: 0.0030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN for Sequence-to-Sequence Forecasting"
      ],
      "metadata": {
        "id": "JsgU3TDYbrB6"
      },
      "id": "JsgU3TDYbrB6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the next 10 steps\n",
        "n_steps = 50\n",
        "series_seq2seq = generate_time_series(10000, n_steps + 10)\n",
        "X_train_seq, Y_train_seq = series_seq2seq[:7000, :n_steps], series_seq2seq[:7000, -10:, 0]\n",
        "X_valid_seq, Y_valid_seq = series_seq2seq[7000:9000, :n_steps], series_seq2seq[7000:9000, -10:, 0]\n",
        "X_test_seq, Y_test_seq = series_seq2seq[9000:, :n_steps], series_seq2seq[9000:, -10:, 0]"
      ],
      "metadata": {
        "id": "FS0OAp52br_e"
      },
      "id": "FS0OAp52br_e",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq2seq = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20), # Last RNN layer returns only the last output\n",
        "    keras.layers.Dense(10) # Dense layer to output 10 values\n",
        "])"
      ],
      "metadata": {
        "id": "d5FXedAFbs3i"
      },
      "id": "d5FXedAFbs3i",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train (example)\n",
        "model_seq2seq.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_seq2seq = model_seq2seq.fit(X_train_seq, Y_train_seq, epochs=20,\n",
        "                                    validation_data=(X_valid_seq, Y_valid_seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT30FrfYbtt6",
        "outputId": "438a84e5-fe55-4e95-a08b-d40ca9ad41d0"
      },
      "id": "uT30FrfYbtt6",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 0.1290 - val_loss: 0.0285\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 0.0243 - val_loss: 0.0206\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - loss: 0.0179 - val_loss: 0.0164\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 0.0153 - val_loss: 0.0128\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - loss: 0.0133 - val_loss: 0.0119\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.0130 - val_loss: 0.0109\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - loss: 0.0116 - val_loss: 0.0110\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 0.0112 - val_loss: 0.0106\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - loss: 0.0110 - val_loss: 0.0115\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 0.0111 - val_loss: 0.0097\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.0104 - val_loss: 0.0096\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - loss: 0.0100 - val_loss: 0.0104\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - loss: 0.0104 - val_loss: 0.0089\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - loss: 0.0101 - val_loss: 0.0093\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - loss: 0.0095 - val_loss: 0.0102\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 0.0095 - val_loss: 0.0092\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - loss: 0.0093 - val_loss: 0.0109\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - loss: 0.0094 - val_loss: 0.0102\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 0.0093 - val_loss: 0.0092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom loss for sequence-to-sequence (not directly in book's code, but conceptually)"
      ],
      "metadata": {
        "id": "OQ44e-QfbunG"
      },
      "id": "OQ44e-QfbunG"
    },
    {
      "cell_type": "code",
      "source": [
        "# If predicting the whole sequence as target\n",
        "# Example: y_train is a sequence of length 10\n",
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
        "model_seq2seq.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])"
      ],
      "metadata": {
        "id": "bW0BfcUBbvgY"
      },
      "id": "bW0BfcUBbvgY",
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Tackling the Vanishing Gradients Problem"
      ],
      "metadata": {
        "id": "uliKPGRNbwXH"
      },
      "id": "uliKPGRNbwXH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Layer"
      ],
      "metadata": {
        "id": "lvUmu8J4bxT5"
      },
      "id": "lvUmu8J4bxT5"
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.LSTM(20),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "tMB5wboRbyFc"
      },
      "id": "tMB5wboRbyFc",
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train (example)\n",
        "model_lstm.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_lstm = model_lstm.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR99wUvXbzBF",
        "outputId": "c6d32f11-7aaa-4beb-902d-694b16d66a01"
      },
      "id": "qR99wUvXbzBF",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 48ms/step - loss: 0.0752 - val_loss: 0.0306\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 0.0247 - val_loss: 0.0187\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - loss: 0.0158 - val_loss: 0.0107\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - loss: 0.0097 - val_loss: 0.0075\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 0.0067 - val_loss: 0.0052\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 0.0051 - val_loss: 0.0040\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - loss: 0.0038 - val_loss: 0.0040\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0034 - val_loss: 0.0031\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 0.0029 - val_loss: 0.0033\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - loss: 0.0026 - val_loss: 0.0028\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 0.0025 - val_loss: 0.0028\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 0.0025 - val_loss: 0.0027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU Layer"
      ],
      "metadata": {
        "id": "adPqKH6vb1HS"
      },
      "id": "adPqKH6vb1HS"
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru = keras.models.Sequential([\n",
        "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "WbiCGlV7b0LP"
      },
      "id": "WbiCGlV7b0LP",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train (example)\n",
        "model_gru.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_gru = model_gru.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJuuZUdDb2_j",
        "outputId": "460d02b9-1aaf-4bba-bed4-4d3ba54f2828"
      },
      "id": "bJuuZUdDb2_j",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 61ms/step - loss: 0.1049 - val_loss: 0.0167\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - loss: 0.0114 - val_loss: 0.0048\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - loss: 0.0047 - val_loss: 0.0046\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - loss: 0.0043 - val_loss: 0.0044\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - loss: 0.0042 - val_loss: 0.0044\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - loss: 0.0043 - val_loss: 0.0053\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - loss: 0.0042 - val_loss: 0.0043\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - loss: 0.0040 - val_loss: 0.0040\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 56ms/step - loss: 0.0039 - val_loss: 0.0041\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - loss: 0.0038 - val_loss: 0.0044\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 53ms/step - loss: 0.0038 - val_loss: 0.0038\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 53ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - loss: 0.0034 - val_loss: 0.0038\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - loss: 0.0035 - val_loss: 0.0037\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - loss: 0.0033 - val_loss: 0.0039\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - loss: 0.0031 - val_loss: 0.0032\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - loss: 0.0031 - val_loss: 0.0037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bidirectional RNN (using GRU as base)"
      ],
      "metadata": {
        "id": "jfygYKcEb3z0"
      },
      "id": "jfygYKcEb3z0"
    },
    {
      "cell_type": "code",
      "source": [
        "model_bidirectional = keras.models.Sequential([\n",
        "    keras.layers.Bidirectional(keras.layers.GRU(20, return_sequences=True), input_shape=[None, 1]),\n",
        "    keras.layers.Bidirectional(keras.layers.GRU(20)),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbqSP-n-b4rH",
        "outputId": "78524485-3807-4476-dac7-8336cb080f5c"
      },
      "id": "sbqSP-n-b4rH",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train (example)\n",
        "model_bidirectional.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history_bidirectional = model_bidirectional.fit(X_train, y_train, epochs=10,\n",
        "                                                validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKrcJY8Bb5Ut",
        "outputId": "ff8f8456-9d2d-482c-8075-485a873c15f8"
      },
      "id": "nKrcJY8Bb5Ut",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 94ms/step - loss: 0.0534 - val_loss: 0.0098\n",
            "Epoch 2/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - loss: 0.0074 - val_loss: 0.0061\n",
            "Epoch 3/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 88ms/step - loss: 0.0050 - val_loss: 0.0054\n",
            "Epoch 4/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 94ms/step - loss: 0.0043 - val_loss: 0.0045\n",
            "Epoch 5/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 87ms/step - loss: 0.0041 - val_loss: 0.0048\n",
            "Epoch 6/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 96ms/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 7/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 94ms/step - loss: 0.0038 - val_loss: 0.0046\n",
            "Epoch 8/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 95ms/step - loss: 0.0039 - val_loss: 0.0041\n",
            "Epoch 9/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 87ms/step - loss: 0.0038 - val_loss: 0.0039\n",
            "Epoch 10/10\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - loss: 0.0037 - val_loss: 0.0037\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}