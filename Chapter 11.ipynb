{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e331f274",
   "metadata": {},
   "source": [
    "# Bab 11: Training Deep Neural Networks (Melatih Jaringan Saraf Tiruan Dalam)\n",
    "\n",
    "### 1. Pendahuluan\n",
    "\n",
    "Bab 11 memperkenalkan dasar-dasar Jaringan Saraf Tiruan (JST) dan Keras. Bab 11 ini akan menyelami tantangan utama dalam melatih *Deep Neural Networks* (DNN) – JST dengan banyak lapisan tersembunyi – dan bagaimana mengatasinya. DNN memiliki potensi yang sangat besar, tetapi dapat menghadapi berbagai masalah.\n",
    "\n",
    "Masalah utama yang dibahas di bab ini meliputi:\n",
    "* **Vanishing/Exploding Gradients:** Gradien menjadi sangat kecil atau sangat besar, menghambat atau mengacaukan pelatihan.\n",
    "* **Non-convergence of Gradient Descent:** *Gradient Descent* dapat terjebak dalam *local optima* atau membutuhkan waktu sangat lama untuk konvergen.\n",
    "* **Overfitting:** DNN dengan banyak parameter sangat rentan terhadap *overfitting*.\n",
    "* **Lambatnya Pelatihan:** DNN membutuhkan waktu lama untuk dilatih.\n",
    "\n",
    "Bab ini akan menyajikan berbagai teknik untuk mengatasi masalah-masalah ini, memungkinkan pelatihan model yang lebih dalam dan lebih efisien.\n",
    "\n",
    "### 2. Masalah Vanishing/Exploding Gradients (The Vanishing/Exploding Gradients Problem)\n",
    "\n",
    "Fenomena ini terjadi selama *backpropagation*:\n",
    "* **Vanishing Gradients:** Gradien seringkali menjadi semakin kecil saat algoritma bergerak mundur melalui lapisan-lapisan, menuju lapisan input. Ini berarti *update* bobot pada lapisan-lapisan awal menjadi sangat kecil, dan pelatihan berhenti belajar secara efektif. Masalah ini sangat umum dengan fungsi aktivasi Sigmoid dan Tanh.\n",
    "* **Exploding Gradients:** Ini adalah masalah sebaliknya, di mana gradien dapat menjadi sangat besar, menyebabkan *update* bobot menjadi terlalu besar dan algoritma divergen. Ini lebih sering terjadi pada *recurrent neural networks* (RNNs).\n",
    "\n",
    "Masalah ini adalah salah satu alasan mengapa *Deep Learning* sempat mengalami stagnasi hingga pertengahan 2000-an.\n",
    "\n",
    "### 3. Mengatasi Masalah Vanishing/Exploding Gradients (Tackling the Vanishing/Exploding Gradients Problem)\n",
    "\n",
    "Beberapa teknik telah dikembangkan untuk mengatasi masalah ini:\n",
    "\n",
    "#### a. Glorot Initialization (Xavier Initialization)\n",
    "Ini adalah strategi untuk inisialisasi bobot (weights) model. Ini memastikan bahwa varians output dari setiap lapisan (dan varians gradien pada *backward pass*) tetap kurang lebih sama di seluruh lapisan. Ini membantu mencegah sinyal (baik data maupun gradien) mati atau meledak terlalu dini.\n",
    "* Untuk fungsi aktivasi ReLU (dan variasinya) atau *linear*, Glorot initialization menginisialisasi bobot dari distribusi Gaussian dengan mean 0 dan varians $\\sigma^2 = \\frac{2}{n_{in} + n_{out}}$ atau distribusi seragam.\n",
    "* `keras.layers.Dense` secara *default* menggunakan Glorot initialization dengan distribusi seragam.\n",
    "\n",
    "#### b. He Initialization\n",
    "Ini adalah inisialisasi yang disarankan untuk fungsi aktivasi ReLU dan variasinya. Mirip dengan Glorot, tetapi variansnya $\\sigma^2 = \\frac{2}{n_{in}}$.\n",
    "* Anda dapat menggunakan `kernel_initializer=\"he_normal\"` atau `kernel_initializer=\"he_uniform\"` di lapisan Keras.\n",
    "\n",
    "#### c. Non-saturating Activation Functions (Fungsi Aktivasi Non-saturasi)\n",
    "Menggunakan fungsi aktivasi yang tidak \"jenuh\" pada input besar atau kecil (di mana gradien menjadi sangat datar):\n",
    "* **ReLU (Rectified Linear Unit):** `f(z) = max(0, z)`. Cepat dihitung, tidak ada masalah *vanishing gradient* untuk input positif.\n",
    "    * **Masalah *Dying ReLUs*:** Neuron bisa \"mati\" jika outputnya selalu negatif, menyebabkan gradiennya selalu nol dan neuron berhenti belajar.\n",
    "* **Leaky ReLU:** `f(z) = max(az, z)` di mana $a$ adalah konstanta kecil (misalnya, 0.01). Ini memastikan neuron tidak pernah mati total.\n",
    "* **PReLU (Parametric Leaky ReLU):** Mirip dengan Leaky ReLU, tetapi $a$ adalah parameter yang dipelajari selama pelatihan.\n",
    "* **ELU (Exponential Linear Unit):** `f(z) = z` jika $z \\ge 0$, dan `a(exp(z) - 1)` jika $z < 0$. Ini mengungguli semua fungsi aktivasi ReLU lainnya.\n",
    "* **SELU (Scaled Exponential Linear Unit):** Jika dibangun dengan benar (lapisan `Dense` saja, inisialisasi He, penskalaan input standar), jaringan yang menggunakan SELU dapat melakukan normalisasi diri, membuat jaringan sangat dalam tanpa masalah *vanishing/exploding gradients*.\n",
    "\n",
    "#### d. Batch Normalization (Normalisasi Batch)\n",
    "Batch Normalization (BN) adalah teknik yang sangat efektif dan terpopuler untuk mengatasi masalah *vanishing/exploding gradients*. Ini juga bertindak sebagai regularisasi.\n",
    "* Ini menambahkan operasi Normalisasi Batch di setiap lapisan (atau setelah fungsi aktivasi).\n",
    "* Ia menormalisasi input lapisan dengan mengurangi *mean* dan membagi dengan *standard deviation* *batch* saat ini, kemudian menskalakan dan menggesernya menggunakan dua set parameter yang dipelajari per lapisan (`gamma` dan `beta`).\n",
    "* Ini mengurangi masalah *Internal Covariate Shift* (perubahan distribusi input setiap lapisan selama pelatihan).\n",
    "* `keras.layers.BatchNormalization()`: Dapat ditambahkan sebagai lapisan.\n",
    "\n",
    "#### e. Gradient Clipping (Pemotongan Gradien)\n",
    "Untuk mengatasi *exploding gradients*, terutama di RNN, Anda dapat memotong gradien agar tidak melebihi ambang batas tertentu. Ini dapat dilakukan dengan mengatur `clipvalue` atau `clipnorm` di *optimizer* Keras.\n",
    "\n",
    "### 4. Mentransfer Pembelajaran (Transfer Learning)\n",
    "\n",
    "*Transfer Learning* adalah teknik yang sangat kuat di mana Anda menggunakan sebagian dari model terlatih sebelumnya (biasanya model yang sangat dalam dan dilatih pada dataset besar) dan menggunakannya kembali sebagai dasar untuk tugas baru. Ini mengurangi waktu dan data yang dibutuhkan untuk melatih model baru.\n",
    "\n",
    "* **Pembekuan Lapisan:** Lapisan-lapisan awal dari model yang sudah dilatih biasanya \"dibekukan\" (tidak diperbarui selama pelatihan) karena mereka telah mempelajari fitur tingkat rendah yang generik.\n",
    "* **Lapisan Output Baru:** Lapisan output model lama diganti dengan lapisan output baru yang cocok untuk tugas baru.\n",
    "* **Penyesuaian (Fine-tuning):** Setelah beberapa *epoch* dengan lapisan yang dibekukan, Anda dapat mencoba \"mencairkan\" beberapa lapisan teratas (lapisan yang lebih dekat ke output) dan melatih kembali seluruh model dengan *learning rate* yang sangat rendah.\n",
    "\n",
    "### 5. Optimizer Lebih Cepat (Faster Optimizers)\n",
    "\n",
    "*Gradient Descent* standar seringkali terlalu lambat. Banyak *optimizer* yang lebih canggih telah dikembangkan:\n",
    "\n",
    "#### a. Momentum Optimizer\n",
    "Mempercepat *Gradient Descent* dengan menambahkan \"momentum\" ke *gradient*. Ia tidak hanya menggunakan *gradient* saat ini tetapi juga *gradient* dari langkah-langkah sebelumnya, mirip bola yang menggelinding menuruni bukit.\n",
    "* `keras.optimizers.SGD(momentum=...)`\n",
    "\n",
    "#### b. Nesterov Accelerated Gradient (NAG)\n",
    "Versi *momentum* yang lebih baik yang mengukur *gradient* tidak pada posisi saat ini, tetapi sedikit di depan ke arah momentum. Ini biasanya konvergen lebih cepat daripada Momentum biasa.\n",
    "* `keras.optimizers.SGD(nesterov=True)`\n",
    "\n",
    "#### c. AdaGrad\n",
    "Menyesuaikan *learning rate* untuk setiap parameter secara individual. Ini mengurangi *learning rate* untuk parameter yang sering diperbarui dan meningkatkan untuk parameter yang jarang diperbarui. Cocok untuk masalah *sparse*.\n",
    "* `keras.optimizers.Adagrad()`\n",
    "* **Kekurangan:** *Learning rate* dapat berkurang terlalu cepat, menghentikan pembelajaran.\n",
    "\n",
    "#### d. RMSProp\n",
    "Mengatasi masalah AdaGrad dengan hanya mengumpulkan gradien dari iterasi terbaru, bukan semua gradien masa lalu.\n",
    "* `keras.optimizers.RMSprop()`\n",
    "\n",
    "#### e. Adam (Adaptive Moment Estimation)\n",
    "Menggabungkan ide *Momentum* dan RMSProp. Ia melacak rata-rata eksponensial dari *gradient* kuadrat dan rata-rata eksponensial dari gradien itu sendiri. Ini seringkali merupakan *optimizer* default yang baik.\n",
    "* `keras.optimizers.Adam()`\n",
    "\n",
    "#### f. Adamax\n",
    "Varian Adam yang didasarkan pada norma tak terbatas (L-infinity norm). Mungkin lebih baik dalam beberapa kasus, terutama dengan fitur *sparse*.\n",
    "\n",
    "#### g. Nadam\n",
    "Adam + Nesterov momentum.\n",
    "\n",
    "**Memilih Optimizer:** Adam, RMSProp, dan Nesterov Accelerated Gradient (NAG) adalah pilihan yang baik. Adam seringkali merupakan titik awal yang baik.\n",
    "\n",
    "### 6. Menghindari Overfitting Melalui Regularisasi (Avoiding Overfitting Through Regularization)\n",
    "\n",
    "DNN memiliki jutaan parameter dan sangat rentan terhadap *overfitting*.\n",
    "\n",
    "#### a. L1 dan L2 Regularization (Regularisasi L1 dan L2)\n",
    "Menambahkan penalti pada fungsi biaya berdasarkan ukuran bobot model.\n",
    "* **L1 (Lasso):** Menambahkan $\\ell_1$ norm dari bobot. Cenderung mendorong bobot ke nol (seleksi fitur).\n",
    "* **L2 (Ridge/Weight Decay):** Menambahkan $\\ell_2$ norm kuadrat dari bobot. Mendorong bobot menjadi kecil tetapi tidak nol.\n",
    "* Dapat ditambahkan ke lapisan Keras menggunakan argumen `kernel_regularizer` dan `bias_regularizer`.\n",
    "\n",
    "#### b. Dropout\n",
    "Dropout adalah teknik regularisasi yang sangat populer dan efektif.\n",
    "* Pada setiap langkah pelatihan, setiap neuron di lapisan tertentu (kecuali lapisan output) memiliki probabilitas $p$ untuk \"dihilangkan\" (outputnya disetel ke nol sementara).\n",
    "* Ini memaksa neuron untuk belajar untuk tidak terlalu bergantung pada neuron tetangga tertentu, membuat model lebih kuat dan kurang rentan *overfitting*.\n",
    "* `keras.layers.Dropout()`: Dapat ditambahkan sebagai lapisan.\n",
    "* **Catatan:** Dropout hanya aktif selama pelatihan; selama inferensi, neuron tidak dihilangkan.\n",
    "\n",
    "#### c. Alpha Dropout\n",
    "Varian Dropout yang bekerja dengan SELU. Ini menjaga properti normalisasi diri SELU.\n",
    "\n",
    "#### d. Monte Carlo (MC) Dropout\n",
    "Melakukan prediksi beberapa kali dengan Dropout aktif dan mengambil rata-rata, yang dapat memberikan estimasi kepercayaan dan meningkatkan kinerja.\n",
    "\n",
    "#### e. Max-Norm Regularization\n",
    "Membatasi $\\ell_2$ norm dari vektor bobot input neuron. Setelah setiap langkah pelatihan, jika $\\ell_2$ norm bobot melebihi batas, bobot diskalakan kembali. Ini juga membantu mengatasi *exploding gradients*.\n",
    "\n",
    "### 7. Ringkasan Praktis (Practical Guidelines)\n",
    "\n",
    "Bab ini diakhiri dengan ringkasan praktis tentang *hyperparameter* JST:\n",
    "\n",
    "| Hyperparameter        | Default Baik (Good Default) | Perubahan untuk Underfitting | Perubahan untuk Overfitting |\n",
    "| :-------------------- | :-------------------------- | :--------------------------- | :-------------------------- |\n",
    "| Inisialisasi Bobot    | He Initialization (ReLU/ELU/SELU) | -                            | -                           |\n",
    "| Fungsi Aktivasi       | ReLU (terutama di hidden layers), Softmax (output klasifikasi), Sigmoid (output biner) | -                            | -                           |\n",
    "| Optimizer             | Adam, Nadam, RMSProp        | Naikkan *learning rate* | Turunkan *learning rate* |\n",
    "| Learning Rate         | ~1e-3 hingga 3e-3           | Naikkan                       | Turunkan                    |\n",
    "| Ukuran Batch          | 32                          | -                            | Turunkan                    |\n",
    "| Jumlah Layer Tersembunyi | 2-5                         | Tambah                       | Kurangi                     |\n",
    "| Neuron per Layer      | 10-100                      | Tambah                       | Kurangi                     |\n",
    "| Regularisasi L1/L2    | Tidak ada secara default, bisa ditambahkan | Kurangi                       | Tambah                      |\n",
    "| Dropout Rate          | Tidak ada secara default, 0.2-0.5 jika ditambahkan | Kurangi                       | Tambah                      |\n",
    "| Batch Normalization   | Sangat disarankan           | -                            | -                           |\n",
    "| Early Stopping        | Sangat disarankan           | -                            | -                           |\n",
    "\n",
    "### 8. Kesimpulan \n",
    "\n",
    "Bab 11 sangat penting untuk pelatihan DNN yang efektif. Ini mengidentifikasi dan memberikan solusi untuk masalah *vanishing/exploding gradients* (inisialisasi yang lebih baik, fungsi aktivasi non-saturasi, Batch Normalization, Gradient Clipping), masalah *overfitting* (regularisasi L1/L2, Dropout), dan meningkatkan kecepatan pelatihan (*faster optimizers*). Pengenalan teknik *transfer learning* juga membuka jalan untuk memanfaatkan model yang sudah dilatih pada tugas-tugas baru. Bab ini melengkapi pembaca dengan \"kotak peralatan\" yang diperlukan untuk membangun dan melatih JST yang dalam dan tangguh.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2cf32d",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcccdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd # For plotting history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a48567",
   "metadata": {},
   "source": [
    "## 2. Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe0aca",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glorot initialization is the default for Keras Dense layers (kernel_initializer='glorot_uniform')\n",
    "# He initialization can be specified using 'he_normal' or 'he_uniform'\n",
    "# Example:\n",
    "keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d165940",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Leaky ReLU activation layer\n",
    "leaky_relu_activation = keras.layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "# Example model with Leaky ReLU\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    leaky_relu_activation,\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    leaky_relu_activation,\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd8270",
   "metadata": {},
   "source": [
    "### PReLU (Parametric ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15893195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PReLU is implemented as a Keras layer\n",
    "keras.layers.PReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd032b",
   "metadata": {},
   "source": [
    "### ELU (Exponential Linear Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELU is a Keras activation function\n",
    "# Example:\n",
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54638b",
   "metadata": {},
   "source": [
    "### SELU (Scaled Exponential Linear Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELU is a Keras activation function\n",
    "# Example:\n",
    "keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\")\n",
    "# For SELU to work properly, inputs must be standardized (mean 0, std 1)\n",
    "# and network must be purely sequential (no skip connections, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f6c09",
   "metadata": {},
   "source": [
    "## 3. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cfbce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(), # BN layer after Flatten (input layer)\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(), # BN layer after hidden layer\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(), # BN layer after hidden layer\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also add BN before activation function\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model (assuming X_train, y_train, X_valid, y_valid are loaded)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e945be",
   "metadata": {},
   "source": [
    "## 4. Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an optimizer with gradient clipping\n",
    "# Example: SGD with clipvalue\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0) # Clip gradients to max value 1.0\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0) # Clip gradients by norm\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0fdbf",
   "metadata": {},
   "source": [
    "## 5. Reusing Pretrained Layers (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa187fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model (e.g., from Keras Applications)\n",
    "# Note: This is an example, you might load a model like VGG16, ResNet, etc.\n",
    "# For simplicity, let's pretend we have a pre-trained model.\n",
    "# In a real scenario, you would load a model like:\n",
    "# base_model = keras.applications.ResNet50(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# For demonstration, let's create a dummy base model for structure\n",
    "base_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(50, activation=\"relu\")\n",
    "])\n",
    "\n",
    "# Create a new model on top of the base model\n",
    "model = keras.models.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc4fff",
   "metadata": {},
   "source": [
    "### Freezing the base model's layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68339f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False # Freeze the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the new model with frozen layers\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a9262d",
   "metadata": {},
   "source": [
    "### Unfreezing layers (fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c087f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True # Unfreeze the base model\n",
    "# It's important to recompile the model after unfreezing layers\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-4), # Use a very low learning rate\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff5a92",
   "metadata": {},
   "source": [
    "## 6. Faster Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6339e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum Optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesterov Accelerated Gradient (NAG)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad Optimizer\n",
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba6ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSProp Optimizer\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Optimizer (often a good default)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adamax Optimizer\n",
    "optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nadam Optimizer\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25858de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of compiling with an optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b4c0c",
   "metadata": {},
   "source": [
    "## 7. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c54eea",
   "metadata": {},
   "source": [
    "### L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a676f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization on Dense layer\n",
    "kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "bias_regularizer=keras.regularizers.l2(0.01)\n",
    "activity_regularizer=keras.regularizers.l2(0.01)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"relu\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16216fc9",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdf70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2), # Dropout layer after Flatten\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dropout(rate=0.2), # Dropout layer after hidden layer\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dropout(rate=0.2), # Dropout layer after hidden layer\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0726d95",
   "metadata": {},
   "source": [
    "### Alpha Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use with SELU activation and lecun_normal initializer\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689af1a",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented using a Keras constraint\n",
    "from keras.constraints import max_norm\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\", kernel_constraint=max_norm(3)),\n",
    "    keras.layers.Dense(100, activation=\"relu\", kernel_constraint=max_norm(3)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
