{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bab 18: Reinforcement Learning (Pembelajaran Penguatan)\n",
        "\n",
        "### 1. Pendahuluan\n",
        "\n",
        "Bab 18 memperkenalkan **Reinforcement Learning (RL)**, paradigma *Machine Learning* yang fundamental berbeda dari *supervised* dan *unsupervised learning*. Dalam RL, sebuah **agen (agent)** belajar bagaimana berperilaku dalam lingkungan dengan melakukan tindakan dan menerima **hadiah (rewards)** atau **hukuman (penalties)**. Tujuannya adalah untuk memaksimalkan total hadiah kumulatif dari waktu ke waktu. RL adalah bidang yang sangat aktif dan telah mencapai keberhasilan luar biasa, terutama dalam permainan (misalnya, AlphaGo, Dota 2) dan robotika.\n",
        "\n",
        "**Komponen Utama RL:**\n",
        "* **Agen (Agent):** Pembelajar atau pembuat keputusan.\n",
        "* **Lingkungan (Environment):** Dunia tempat agen berinteraksi.\n",
        "* **Tindakan (Actions):** Pilihan yang dapat diambil agen.\n",
        "* **Keadaan (State):** Deskripsi saat ini dari lingkungan.\n",
        "* **Hadiah (Reward):** Umpan balik numerik dari lingkungan setelah tindakan.\n",
        "* **Kebijakan (Policy):** Strategi agen, memetakan keadaan ke tindakan.\n",
        "\n",
        "### 2. Pembelajaran Penguatan vs Pembelajaran Lainnya (Reinforcement Learning vs. Other Types of Learning)\n",
        "\n",
        "* **Supervised Learning:** Belajar dari data berlabel. RL tidak memiliki label eksplisit; umpan baliknya adalah hadiah, yang bisa tertunda.\n",
        "* **Unsupervised Learning:** Mencari pola dalam data tanpa label. RL memiliki tujuan (memaksimalkan hadiah) dan berinteraksi secara dinamis.\n",
        "\n",
        "### 3. Kebijakan Optimal (Optimal Policy)\n",
        "\n",
        "Tujuan utama RL adalah menemukan **kebijakan optimal** yang akan memaksimalkan total hadiah jangka panjang (seringkali dengan diskon untuk hadiah di masa depan).\n",
        "\n",
        "#### a. Fungsi Nilai Tindakan (Action-Value Function - Q-Value)\n",
        "Fungsi nilai tindakan (sering disebut **Q-value**) adalah fungsi yang mengestimasi total hadiah diskon yang diharapkan yang akan diterima agen jika ia mengambil tindakan $a$ dalam keadaan $s$, kemudian mengikuti kebijakan optimal setelah itu.\n",
        "$Q^{\\ast}(s, a) = \\text{max total discounted future rewards if taking action } a \\text{ in state } s \\text{ and then following the optimal policy.}$\n",
        "\n",
        "#### b. Persamaan Bellman (Bellman Equation)\n",
        "Persamaan Bellman adalah hubungan fundamental dalam RL yang secara rekursif mendefinisikan Q-value optimal:\n",
        "$Q^{\\ast}(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s' | s, a) \\max_{a'} Q^{\\ast}(s', a')$\n",
        "di mana:\n",
        "* $R(s, a)$ adalah hadiah instan setelah mengambil tindakan $a$ dalam keadaan $s$.\n",
        "* $\\gamma$ (gamma) adalah **faktor diskon** (discount factor), nilai antara 0 dan 1. Hadiah di masa depan didiskon agar nilainya berkurang seiring waktu.\n",
        "* $P(s' | s, a)$ adalah probabilitas transisi dari keadaan $s$ ke $s'$ setelah mengambil tindakan $a$.\n",
        "* $\\max_{a'} Q^{\\ast}(s', a')$ adalah Q-value optimal dari keadaan berikutnya $s'$.\n",
        "\n",
        "### 4. Algoritma RL (RL Algorithms)\n",
        "\n",
        "Bab ini kemudian membahas beberapa algoritma RL penting:\n",
        "\n",
        "#### a. Q-Learning\n",
        "\n",
        "Q-Learning adalah algoritma RL *off-policy* (yaitu, dapat mempelajari kebijakan optimal bahkan saat mengikuti kebijakan eksplorasi yang berbeda) dan *model-free* (tidak memerlukan model lingkungan).\n",
        "\n",
        "* **Prinsip Kerja:** Agen membangun **Q-Table** (tabel Q) yang menyimpan Q-value untuk setiap pasangan (keadaan, tindakan).\n",
        "* **Update Q-value:** Pada setiap langkah, agen mengambil tindakan $a$ dalam keadaan $s$, mengamati hadiah $r$ dan keadaan baru $s'$. Kemudian ia memperbarui Q-value-nya menggunakan persamaan:\n",
        "    $Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma \\max_{a'} Q(s', a'))$\n",
        "    di mana $\\alpha$ (alpha) adalah *learning rate*.\n",
        "* **Epsilon-Greedy Policy:** Untuk eksplorasi, agen sering mengikuti kebijakan *epsilon-greedy*: dengan probabilitas $\\epsilon$, agen memilih tindakan acak; jika tidak, agen memilih tindakan dengan Q-value tertinggi (eksploitasi). $\\epsilon$ biasanya dimulai tinggi dan berkurang seiring waktu.\n",
        "\n",
        "#### b. Approximate Q-Learning and Deep Q-Networks (DQN)\n",
        "\n",
        "Untuk masalah dengan ruang keadaan yang besar atau kontinu, Q-Table tidak praktis. Solusinya adalah menggunakan **Jaringan Saraf Tiruan (ANN)** sebagai fungsi perkiraan (function approximator) untuk Q-value. Ini adalah inti dari **Deep Q-Networks (DQNs)**.\n",
        "\n",
        "* **DQN:** ANN mengambil keadaan sebagai input dan mengembalikan Q-value untuk semua tindakan yang mungkin sebagai output.\n",
        "* **Experience Replay:** DQNs menggunakan buffer *experience replay* untuk menyimpan transisi (keadaan, tindakan, hadiah, keadaan baru). Selama pelatihan, *mini-batch* diambil secara acak dari buffer ini. Ini membantu:\n",
        "    * Mengurangi korelasi antar sampel (melanggar asumsi i.i.d. untuk *Gradient Descent*).\n",
        "    * Memungkinkan penggunaan kembali pengalaman masa lalu.\n",
        "* **Target Network:** DQNs menggunakan dua jaringan: *online network* (yang terus diperbarui) dan *target network* (salinan *online network* yang diperbarui lebih jarang). Ini membantu menstabilkan pelatihan dengan memberikan target Q-value yang lebih stabil.\n",
        "\n",
        "#### c. Kebijakan Gradien (Policy Gradients - PG)\n",
        "\n",
        "Berbeda dengan Q-Learning yang mempelajari fungsi nilai, algoritma *Policy Gradients* langsung mempelajari **kebijakan** yang memetakan keadaan ke probabilitas tindakan.\n",
        "\n",
        "* **Prinsip Kerja:** Jaringan saraf (Policy Network) mengambil keadaan sebagai input dan mengembalikan probabilitas untuk setiap tindakan sebagai output.\n",
        "* **Update Kebijakan:** Jaringan dilatih untuk meningkatkan probabilitas tindakan yang mengarah pada hadiah yang lebih tinggi dan mengurangi probabilitas tindakan yang mengarah pada hadiah yang lebih rendah. Ini dilakukan dengan menghitung *gradient* dari *score* kinerja (seringkali total hadiah) terhadap parameter kebijakan.\n",
        "* **Keuntungan:** Dapat menangani ruang tindakan diskrit dan kontinu, dan dapat mempelajari kebijakan stokastik.\n",
        "* **REINFORCE Algorithm:** Algoritma PG dasar yang menggunakan Monte Carlo untuk mengestimasi total hadiah.\n",
        "\n",
        "#### d. Actor-Critic Methods (Metode Aktor-Kritik)\n",
        "\n",
        "Metode Actor-Critic menggabungkan ide-ide dari Policy Gradients (Actor) dan Q-Learning (Critic).\n",
        "\n",
        "* **Actor:** Jaringan saraf yang mempelajari kebijakan (mirip Policy Network).\n",
        "* **Critic:** Jaringan saraf yang mempelajari fungsi nilai (mirip DQN), mengestimasi Q-value atau V-value (nilai keadaan).\n",
        "* **Interaksi:** Critic memberikan umpan balik (sinyal *error* Temporal-Difference) kepada Actor untuk membimbing pembelajaran kebijakan Actor. Actor menyesuaikan tindakannya berdasarkan umpan balik Critic.\n",
        "* **Keuntungan:** Seringkali lebih stabil dan efisien daripada Policy Gradients murni karena Critic mengurangi *variance* estimasi hadiah.\n",
        "* **A2C (Advantage Actor-Critic) / A3C (Asynchronous Advantage Actor-Critic):** Algoritma Actor-Critic populer. A3C menggunakan beberapa agen paralel untuk eksplorasi yang lebih efisien.\n",
        "\n",
        "#### e. Proximal Policy Optimization (PPO)\n",
        "\n",
        "PPO adalah salah satu algoritma RL *state-of-the-art* yang paling populer dan seimbang. Ini adalah algoritma *on-policy* (belajar dari pengalaman yang dikumpulkan dengan kebijakan saat ini) dan merupakan ekstensi dari Actor-Critic.\n",
        "\n",
        "* **Keuntungan:** Relatif sederhana untuk diimplementasikan, berkinerja baik dalam berbagai tugas, dan lebih stabil daripada banyak algoritma PG lainnya. Ini membatasi seberapa banyak kebijakan dapat berubah pada setiap langkah, yang membantu menstabilkan pelatihan.\n",
        "\n",
        "### 5. Open AI Gym\n",
        "\n",
        "OpenAI Gym adalah toolkit yang banyak digunakan untuk mengembangkan dan membandingkan algoritma RL. Ini menyediakan berbagai lingkungan simulasi (misalnya, permainan papan, masalah robotika) dengan antarmuka standar. Ini adalah lingkungan yang ideal untuk menguji algoritma RL.\n",
        "\n",
        "### 6. Kesimpulan\n",
        "\n",
        "Bab 18 memberikan pengantar yang komprehensif dan mendalam untuk Reinforcement Learning, dari konsep dasar agen, lingkungan, hadiah, dan kebijakan, hingga algoritma kunci seperti Q-Learning (termasuk DQN dengan *experience replay* dan *target networks*), Policy Gradients (REINFORCE), Actor-Critic (A2C/A3C), dan PPO. Pemahaman tentang OpenAI Gym sebagai platform standar untuk eksperimen RL juga ditekankan. Bab ini menjelaskan mengapa RL adalah bidang yang menarik dan menjanjikan, terutama untuk masalah yang melibatkan pengambilan keputusan berurutan dalam lingkungan yang dinamis."
      ],
      "metadata": {
        "id": "64jSM3WxjRJZ"
      },
      "id": "64jSM3WxjRJZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "v9Gi74VjjURQ"
      },
      "id": "v9Gi74VjjURQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk instalasi OpenAI Gym (dasar):\n",
        "%pip install gym\n",
        "\n",
        "# Untuk lingkungan spesifik (misalnya, Box2D untuk LunarLander, atau rendering):\n",
        "%pip install 'gym[classic_control]'\n",
        "%pip install 'gym[box2d]'\n",
        "%pip install pyglet # Or pygame, for rendering if you need it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qob8G5WAjVWD",
        "outputId": "365d843e-5273-41fa-c87d-4b5d70bccb34"
      },
      "id": "qob8G5WAjVWD",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym[classic_control]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym[classic_control]) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[classic_control])\n",
            "  Downloading pygame-2.1.0.tar.gz (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym[box2d]) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Using cached pygame-2.1.0.tar.gz (5.8 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting pyglet\n",
            "  Downloading pyglet-2.1.6-py3-none-any.whl.metadata (7.7 kB)\n",
            "Downloading pyglet-2.1.6-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m984.0/984.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyglet\n",
            "Successfully installed pyglet-2.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gym # OpenAI Gym"
      ],
      "metadata": {
        "id": "mMrCXcujjXIe"
      },
      "id": "mMrCXcujjXIe",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. The CartPole Environment (Example)"
      ],
      "metadata": {
        "id": "tGXsj82xjYOj"
      },
      "id": "tGXsj82xjYOj"
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.seed(42) # Set seed for reproducibility\n",
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvJOWSy0jY7q",
        "outputId": "f8c23dff-cb8e-40de-826a-2609a752f053"
      },
      "id": "SvJOWSy0jY7q",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observation space (e.g., cart position, velocity, pole angle, angular velocity)\n",
        "env.observation_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnDCb6_5jZrR",
        "outputId": "4f99f8a7-b046-4400-f366-f8691d2da0e0"
      },
      "id": "GnDCb6_5jZrR",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Action space (e.g., push left, push right)\n",
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq9qWPOgjac_",
        "outputId": "17960e6a-5cb0-460c-eb5c-71017e0da52d"
      },
      "id": "Zq9qWPOgjac_",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Playing a few episodes (example of interaction)"
      ],
      "metadata": {
        "id": "mA8-VkfhjbRe"
      },
      "id": "mA8-VkfhjbRe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Running one random episode\n",
        "frames = []\n",
        "for step in range(200): # Max steps for CartPole-v1 is 500\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "    action = env.action_space.sample() # Take a random action\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close() # Close the rendering window"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ul5oEvOjcGh",
        "outputId": "d7cf011c-2dd3-489a-9b73-7c5987d799bf"
      },
      "id": "6ul5oEvOjcGh",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Policy Gradients (REINFORCE Algorithm for CartPole)"
      ],
      "metadata": {
        "id": "2TuQtnDCjdFL"
      },
      "id": "2TuQtnDCjdFL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple Policy Network\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "n_inputs = 4 # Number of observations in CartPole-v1\n",
        "n_outputs = 2 # Number of possible actions (left or right)\n",
        "\n",
        "model_pg = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(n_outputs, activation=\"softmax\") # Output probabilities for each action\n",
        "])"
      ],
      "metadata": {
        "id": "Ua6LviByjd0d"
      },
      "id": "Ua6LviByjd0d",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a custom training step for REINFORCE"
      ],
      "metadata": {
        "id": "vpAxZooWje2Q"
      },
      "id": "vpAxZooWje2Q"
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Predict action probabilities\n",
        "        logits = model(obs[np.newaxis]) # Add batch dimension\n",
        "        action_probs = tf.nn.softmax(logits)\n",
        "\n",
        "        # Sample an action based on probabilities\n",
        "        action = tf.random.categorical(tf.math.log(action_probs), num_samples=1)[0, 0].numpy()\n",
        "\n",
        "        # Execute action in environment\n",
        "        new_obs, reward, done, info = env.step(action)\n",
        "\n",
        "        # Calculate loss (policy gradient loss)\n",
        "        # We want to increase the probability of the chosen action if it leads to high reward.\n",
        "        # This is a bit tricky: it's a \"pseudo-loss\" that we will multiply by advantage later.\n",
        "        # The loss should be the negative log probability of the chosen action\n",
        "        # We'll use this 'pseudo-loss' and multiply by the advantage later in the training loop\n",
        "        chosen_action_prob = action_probs[0, action]\n",
        "        # Add a small epsilon to avoid log(0)\n",
        "        pseudo_loss = -tf.math.log(chosen_action_prob + 1e-8)\n",
        "\n",
        "\n",
        "    return new_obs, reward, done, action, pseudo_loss # Return pseudo_loss\n",
        "\n",
        "# Function to play multiple episodes and gather experiences\n",
        "def play_multiple_episodes(env, n_episodes, model, loss_fn):\n",
        "    all_rewards = [] # List of lists, each inner list is rewards for one episode\n",
        "    all_losses = [] # List of lists, each inner list is pseudo_losses for one episode\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = [] # Rewards for the current episode\n",
        "        current_losses = []  # Pseudo_losses for the current episode\n",
        "        obs = env.reset()\n",
        "        # Ensure obs is a numpy array if it's not already\n",
        "        if not isinstance(obs, np.ndarray):\n",
        "            obs = np.array(obs)\n",
        "\n",
        "        for step in range(200): # Max steps per episode\n",
        "            obs, reward, done, action, pseudo_loss = play_one_step(env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_losses.append(pseudo_loss)\n",
        "            if done:\n",
        "                break\n",
        "        all_rewards.append(current_rewards) # Append list of rewards for this episode\n",
        "        all_losses.append(current_losses)   # Append list of pseudo_losses for this episode\n",
        "    return all_rewards, all_losses\n",
        "\n",
        "# Function to discount and normalize rewards (advantage calculation)\n",
        "def discount_and_normalize_rewards(rewards, discount_factor):\n",
        "    discounted_rewards = np.array(rewards)\n",
        "    # Apply discount factor from the end to the beginning\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted_rewards[step] += discounted_rewards[step + 1] * discount_factor\n",
        "\n",
        "    # Normalize rewards (optional but common for stability)\n",
        "    # Avoid division by zero if std is zero\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
        "    return discounted_rewards"
      ],
      "metadata": {
        "id": "yIMGkmcHjftF"
      },
      "id": "yIMGkmcHjftF",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop for REINFORCE"
      ],
      "metadata": {
        "id": "8zkA05FOjgvi"
      },
      "id": "8zkA05FOjgvi"
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "# Loss function for policy gradient (custom, as it's weighted by advantage)\n",
        "# The actual \"loss\" in REINFORCE is the negative of the policy gradient objective,\n",
        "# and it needs to be multiplied by the advantage (discounted reward).\n",
        "# Keras requires a loss function, so we define one that will be scaled later.\n",
        "# We are not using this pg_loss_fn directly in model.compile, but the concept\n",
        "# of the loss being -log_prob * advantage is implemented in the training loop.\n",
        "# We define a placeholder or conceptual loss function here if needed by some Keras utilities,\n",
        "# but for our custom tape-based training, we'll calculate the objective directly.\n",
        "def pg_loss_fn(y_true, y_pred):\n",
        "    # This function is conceptually used in play_one_step to get the log probability of the chosen action.\n",
        "    # The actual loss with advantage is calculated in the training loop.\n",
        "    # y_true is not used in the typical sense here; y_pred are the action probabilities.\n",
        "    # play_one_step calculates the pseudo_loss which is -log_prob(chosen_action)\n",
        "    return tf.constant(0.0) # Placeholder, as the actual loss is calculated in the training loop\n",
        "\n",
        "\n",
        "# Compile the model (though we'll use a custom training step)\n",
        "# model_pg.compile(loss=pg_loss_fn, optimizer=optimizer) # We will use a custom training loop\n",
        "\n",
        "# Training loop\n",
        "n_iterations = 100\n",
        "n_episodes_per_iteration = 10 # Number of episodes to play to collect experience\n",
        "discount_factor = 0.95\n",
        "\n",
        "# Initialize the environment here if not already initialized\n",
        "# import gym\n",
        "# env = gym.make(\"CartPole-v1\") # Make sure the environment is created\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # Collect experiences\n",
        "    # We need to collect observations, actions, and rewards\n",
        "    all_rewards = [] # List of lists, each inner list is rewards for one episode\n",
        "    all_actions = [] # List of lists, each inner list is actions for one episode\n",
        "    all_observations = [] # List of lists, each inner list is observations for one episode\n",
        "\n",
        "    for episode in range(n_episodes_per_iteration):\n",
        "        current_rewards = []\n",
        "        current_actions = []\n",
        "        current_observations = []\n",
        "        obs = env.reset()\n",
        "        if not isinstance(obs, np.ndarray):\n",
        "            obs = np.array(obs)\n",
        "\n",
        "        for step in range(200): # Max steps per episode\n",
        "            # Collect observation before taking action\n",
        "            current_observations.append(obs)\n",
        "\n",
        "            # Predict action probabilities - this will now happen inside the tape later\n",
        "            logits = model_pg(obs[np.newaxis]) # Still need logits here to sample action\n",
        "            action_probs = tf.nn.softmax(logits)\n",
        "\n",
        "            # Sample an action based on probabilities\n",
        "            action = tf.random.categorical(tf.math.log(action_probs), num_samples=1)[0, 0].numpy()\n",
        "            current_actions.append(action)\n",
        "\n",
        "            # Execute action in environment\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            current_rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_actions.append(current_actions)\n",
        "        all_observations.append(current_observations)\n",
        "\n",
        "\n",
        "    # Flatten data across episodes\n",
        "    all_actions_flat = np.concatenate(all_actions)\n",
        "    all_observations_flat = np.concatenate(all_observations, axis=0)\n",
        "\n",
        "\n",
        "    # Calculate advantages by processing each episode's step rewards\n",
        "    discounted_rewards_per_episode = [discount_and_normalize_rewards(rewards, discount_factor) for rewards in all_rewards]\n",
        "    discounted_rewards_flat = np.concatenate(discounted_rewards_per_episode)\n",
        "\n",
        "\n",
        "    # Perform one optimization step\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Need to re-run the forward pass for all collected observations within the tape\n",
        "        # This allows the tape to track the gradients from the model's output to the loss\n",
        "        all_logits = model_pg(tf.constant(all_observations_flat, dtype=tf.float32))\n",
        "        all_action_probs = tf.nn.softmax(all_logits)\n",
        "\n",
        "        # Get the probabilities of the actions that were actually taken\n",
        "        # We need to select the probability of the chosen action for each step\n",
        "        # Use tf.gather_nd or equivalent to select probabilities based on chosen actions\n",
        "        action_indices = tf.stack([tf.range(tf.shape(all_actions_flat)[0]), tf.constant(all_actions_flat, dtype=tf.int32)], axis=1)\n",
        "        chosen_action_probs = tf.gather_nd(all_action_probs, action_indices)\n",
        "\n",
        "        # Calculate the pseudo_loss for each step (-log_prob(chosen_action))\n",
        "        pseudo_losses = -tf.math.log(chosen_action_probs + 1e-8)\n",
        "\n",
        "\n",
        "        # The objective is sum((log_prob * advantage))\n",
        "        # which is equivalent to minimizing sum((-log_prob) * advantage) = sum(pseudo_loss * advantage)\n",
        "        # We want to maximize the objective, so we minimize the negative objective:\n",
        "        # Minimize -sum(pseudo_loss * advantage)\n",
        "        objective = tf.reduce_sum(pseudo_losses * tf.constant(discounted_rewards_flat, dtype=tf.float32)) # Note: the sign is positive here to minimize -objective\n",
        "\n",
        "\n",
        "        gradients = tape.gradient(objective, model_pg.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, model_pg.trainable_variables))\n",
        "\n",
        "    if iteration % 10 == 0:\n",
        "        # Calculate the mean total reward per episode for printing\n",
        "        mean_reward = np.mean([np.sum(rewards) for rewards in all_rewards])\n",
        "        print(f\"Iteration {iteration}, Mean Reward: {mean_reward}\")\n",
        "        if mean_reward >= 195: # CartPole-v1 solved threshold\n",
        "            print(\"CartPole-v1 solved!\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCROi-jQjhgv",
        "outputId": "d252f0cd-4568-4cff-ce3a-7e4e3fdaf579"
      },
      "id": "OCROi-jQjhgv",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Mean Reward: 17.4\n",
            "Iteration 10, Mean Reward: 22.5\n",
            "Iteration 20, Mean Reward: 39.2\n",
            "Iteration 30, Mean Reward: 43.1\n",
            "Iteration 40, Mean Reward: 88.2\n",
            "Iteration 50, Mean Reward: 75.9\n",
            "Iteration 60, Mean Reward: 61.8\n",
            "Iteration 70, Mean Reward: 81.3\n",
            "Iteration 80, Mean Reward: 99.5\n",
            "Iteration 90, Mean Reward: 66.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Deep Q-Networks (DQN) (Conceptual/Structure)"
      ],
      "metadata": {
        "id": "JM508e44jihb"
      },
      "id": "JM508e44jihb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a DQN (Q-Network)\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "input_shape_dqn = [4] # CartPole observation space\n",
        "n_outputs_dqn = 2 # CartPole action space\n",
        "\n",
        "q_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"relu\", input_shape=input_shape_dqn),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(n_outputs_dqn) # Output Q-values for each action\n",
        "])\n",
        "\n",
        "# Target network (a copy of the online network)\n",
        "# target_q_model = keras.models.clone_model(q_model)\n",
        "# target_q_model.set_weights(q_model.get_weights())\n",
        "\n",
        "# Experience Replay Buffer (Conceptual)\n",
        "# from collections import deque\n",
        "# replay_buffer = deque(maxlen=20000) # Max 20,000 experiences\n",
        "\n",
        "# Epsilon-greedy policy function\n",
        "# def epsilon_greedy_policy(obs, epsilon):\n",
        "#     if np.random.rand() < epsilon:\n",
        "#         return env.action_space.sample()\n",
        "#     else:\n",
        "#         q_values = q_model.predict(obs[np.newaxis])\n",
        "#         return np.argmax(q_values[0])\n",
        "\n",
        "# Training loop for DQN is more complex and involves:\n",
        "# 1. Experience collection\n",
        "# 2. Sampling from replay buffer\n",
        "# 3. Training the online Q-network using the target network\n",
        "# 4. Periodically updating the target network weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddANvAvUjjXK",
        "outputId": "0be23001-b2f8-4713-d70f-8b6297b6388c"
      },
      "id": "ddANvAvUjjXK",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}