{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30b897b3",
      "metadata": {
        "id": "30b897b3"
      },
      "source": [
        "# Bab 16: Processing Natural Language with RNNs and Attention (Memproses Bahasa Alami dengan RNN dan Atensi)\n",
        "\n",
        "### 1. Pendahuluan\n",
        "\n",
        "Bab 16 ini adalah kelanjutan dari Bab 15, secara spesifik menerapkan konsep Jaringan Saraf Berulang (RNNs) dan mekanisme Attention ke bidang **Natural Language Processing (NLP)**. NLP adalah salah satu aplikasi *Deep Learning* yang paling menantang dan menarik, meliputi tugas-tugas seperti penerjemahan mesin, analisis sentimen, pembuatan teks, dan tanya jawab. Memproses bahasa manusia memiliki kompleksitas unik yang membuat RNN dan Transformer menjadi sangat cocok.\n",
        "\n",
        "### 2. Word Embeddings (Embedding Kata)\n",
        "\n",
        "Kata-kata harus dikonversi menjadi representasi numerik yang dapat diproses oleh JST. Teknik yang paling umum adalah *Word Embeddings*.\n",
        "\n",
        "* **One-Hot Encoding:** Representasi yang paling sederhana, di mana setiap kata unik diwakili oleh vektor biner dengan `1` pada indeks kata tersebut dan `0` di tempat lain. Kekurangan: sangat *sparse*, tidak menangkap hubungan semantik antar kata, dan menghasilkan vektor berdimensi tinggi.\n",
        "* **Word Embeddings:** Representasi vektor padat dan berdimensi rendah yang menangkap makna semantik dan hubungan antar kata. Kata-kata dengan makna yang serupa akan memiliki vektor *embedding* yang dekat dalam ruang vektor.\n",
        "    * **Pembelajaran Embedding:** *Word embeddings* dapat dipelajari selama pelatihan jaringan saraf (misalnya, `keras.layers.Embedding` di Keras).\n",
        "    * **Pre-trained Embeddings:** Seringkali lebih baik menggunakan *word embeddings* yang sudah dilatih pada korpus teks yang sangat besar (misalnya, Word2Vec, GloVe, FastText). Ini dapat mempercepat pelatihan dan meningkatkan kinerja, terutama pada dataset kecil.\n",
        "\n",
        "### 3. Encoder-Decoder Network untuk Penerjemahan Mesin (An Encoder-Decoder Network for Machine Translation)\n",
        "\n",
        "Arsitektur *encoder-decoder* sangat umum untuk tugas *sequence-to-sequence* seperti penerjemahan mesin.\n",
        "\n",
        "* **Encoder:** Sebuah RNN (misalnya, LSTM atau GRU) yang memproses urutan teks input (misalnya, kalimat dalam bahasa sumber) dan mengkompresnya menjadi satu vektor *context* atau *state* tersembunyi.\n",
        "* **Decoder:** RNN lain yang mengambil vektor *context* ini sebagai input awal dan menghasilkan urutan teks output (misalnya, kalimat terjemahan dalam bahasa target) kata demi kata.\n",
        "* **Tantangan:** Vektor *context* tunggal dapat menjadi *bottleneck* informasi, terutama untuk kalimat yang sangat panjang, menyebabkan hilangnya detail.\n",
        "\n",
        "### 4. Attention Mechanisms (Mekanisme Atensi)\n",
        "\n",
        "Mekanisme *Attention* adalah inovasi penting yang mengatasi *bottleneck* *encoder-decoder* pada urutan panjang.\n",
        "\n",
        "* **Bagaimana Atensi Bekerja:** Daripada memaksa *encoder* mengkompres seluruh kalimat input ke dalam satu vektor *context* tunggal, *attention* memungkinkan *decoder* untuk \"melihat kembali\" dan memberikan bobot berbeda pada setiap langkah waktu input *encoder* saat menghasilkan setiap kata output.\n",
        "* Ini memungkinkan *decoder* untuk fokus pada bagian-bagian yang paling relevan dari kalimat input pada setiap langkah waktu output, mirip dengan bagaimana manusia menerjemahkan.\n",
        "* **Manfaat:** Meningkatkan kinerja secara signifikan pada tugas *sequence-to-sequence* dengan urutan panjang, dan juga memberikan interpretasi tentang bagian input mana yang paling penting untuk setiap bagian output.\n",
        "\n",
        "### 5. Transformer Architecture (Arsitektur Transformer)\n",
        "\n",
        "Arsitektur Transformer, diperkenalkan pada paper \"Attention Is All You Need\" (2017), telah merevolusi NLP. Ia sepenuhnya menghilangkan rekurensi dan mengandalkan mekanisme *self-attention*.\n",
        "\n",
        "* **Self-Attention:** Komponen kunci Transformer. Ini memungkinkan setiap kata dalam urutan untuk menghitung *score* relevansinya terhadap semua kata lain dalam urutan, dan kemudian menghasilkan representasi baru untuk setiap kata sebagai jumlah bobot dari representasi semua kata lain.\n",
        "    * **Query, Key, Value:** Untuk setiap kata, Transformer menghitung tiga vektor: *Query* ($Q$), *Key* ($K$), dan *Value* ($V$). *Query* dan *Key* digunakan untuk menghitung skor kesamaan, yang kemudian digunakan untuk membobot *Value*.\n",
        "    * **Multi-Head Attention:** Menggabungkan beberapa mekanisme *self-attention* yang berjalan secara paralel (*attention heads*), masing-masing belajar fokus pada bagian yang berbeda dari urutan, kemudian outputnya digabungkan.\n",
        "* **Positional Encoding:** Karena Transformer tidak memiliki rekurensi, informasi posisi kata dalam urutan ditambahkan secara eksplisit ke *embedding* kata.\n",
        "* **Encoder-Decoder Stack:** Transformer juga memiliki arsitektur *encoder-decoder*, dengan setiap bagian terdiri dari beberapa lapisan yang identik. Setiap lapisan encoder dan decoder mengandung blok *multi-head attention* dan *feed-forward neural network*.\n",
        "* **Keuntungan:** Komputasi yang sangat paralel (tidak ada dependensi rekuren), mampu menangani dependensi jarak jauh secara efektif, dan menjadi dasar untuk banyak model NLP *pre-trained* seperti BERT, GPT, dll.\n",
        "\n",
        "### 6. Large Language Models (LLMs) (Model Bahasa Besar)\n",
        "\n",
        "Model Transformer yang sangat besar, dilatih pada korpus teks yang sangat luas, disebut *Large Language Models* (LLMs). Ini adalah perkembangan paling signifikan di NLP dalam beberapa tahun terakhir.\n",
        "\n",
        "* **Pre-training & Fine-tuning:** LLMs biasanya dilatih dengan proses dua tahap:\n",
        "    1.  **Pre-training (Pelatihan Awal):** Model dilatih pada dataset teks yang masif (misalnya, seluruh internet) untuk tugas *unsupervised* seperti memprediksi kata berikutnya atau mengisi bagian yang hilang dalam kalimat. Ini memungkinkan model mempelajari representasi bahasa yang kaya.\n",
        "    2.  **Fine-tuning (Penyetelan Halus):** Model yang sudah dilatih kemudian disetel pada dataset yang lebih kecil dan spesifik untuk tugas hilir (misalnya, klasifikasi sentimen, tanya jawab).\n",
        "* **Transfer Learning:** LLMs memanfaatkan *transfer learning* secara ekstrem, memungkinkan kinerja *state-of-the-art* pada berbagai tugas dengan data spesifik tugas yang relatif sedikit.\n",
        "* **Prompt Engineering:** Dengan LLMs, seringkali cukup dengan \"meminta\" model untuk melakukan tugas tertentu melalui *prompt* yang dirancang dengan cermat, tanpa perlu *fine-tuning* sama sekali (disebut *zero-shot* atau *few-shot learning*).\n",
        "\n",
        "### 7. Kesimpulan\n",
        "\n",
        "Bab 16 adalah pengantar penting untuk NLP dengan *deep learning*. Ini menjelaskan bagaimana kata-kata direpresentasikan secara numerik melalui *embeddings*, bagaimana RNN (dengan LSTM/GRU) dan arsitektur *encoder-decoder* digunakan untuk tugas-tugas *sequence-to-sequence*. Diskusi mendalam tentang mekanisme *Attention* dan arsitektur Transformer adalah kunci, karena mereka membentuk dasar dari perkembangan terkini di NLP, termasuk *Large Language Models*. Bab ini membuka pintu ke dunia yang kompleks dan menarik dari pemahaman dan pembuatan bahasa alami oleh mesin."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c5f67f",
      "metadata": {
        "id": "04c5f67f"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "96856aca",
      "metadata": {
        "id": "96856aca"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "961981a6",
      "metadata": {
        "id": "961981a6"
      },
      "source": [
        "### Loading and Preprocessing Data (Example: IMDB Reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "83262b60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83262b60",
        "outputId": "79a1cd8b-8bf8-4854-eca5-ddc20532fff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# The book often uses the IMDB dataset for sentiment analysis.\n",
        "imdb = keras.datasets.imdb\n",
        "(X_train_full, y_train_full), (X_test, y_test) = imdb.load_data()\n",
        "word_index = imdb.get_word_index() # A dictionary mapping words to integer indices\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_ in range(3): # For special tokens\n",
        "    id_to_word[id_] = [\"<pad>\", \"<sos>\", \"<unk>\"][id_]\n",
        "X_train_full = [\n",
        "    np.array([id_to_word[word_id] for word_id in review])\n",
        "    for review in X_train_full\n",
        "]\n",
        "X_test = [\n",
        "    np.array([id_to_word[word_id] for word_id in review])\n",
        "    for review in X_test\n",
        "]\n",
        "\n",
        "# For code reproducibility, let's create simple dummy data if actual dataset loading is not feasible.\n",
        "# Maximum sequence length\n",
        "max_len = 20\n",
        "# Vocabulary size (e.g., top 10000 words + 3 special tokens)\n",
        "vocab_size = 10000 + 3 # <pad>, <sos>, <unk>\n",
        "\n",
        "# Dummy data for demonstration purposes\n",
        "X_train_full = keras.preprocessing.sequence.pad_sequences(\n",
        "    np.random.randint(0, vocab_size, size=(25000, max_len)),\n",
        "    maxlen=max_len)\n",
        "y_train_full = np.random.randint(0, 2, size=(25000,)) # Binary classification\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(\n",
        "    np.random.randint(0, vocab_size, size=(5000, max_len)),\n",
        "    maxlen=max_len)\n",
        "y_test = np.random.randint(0, 2, size=(5000,))\n",
        "\n",
        "X_valid, X_train = X_train_full[:2000], X_train_full[2000:]\n",
        "y_valid, y_train = y_train_full[:2000], y_train_full[2000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db20b4e",
      "metadata": {
        "id": "2db20b4e"
      },
      "source": [
        "## 2. Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f2a91e1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2a91e1f",
        "outputId": "1e5fc262-cc6f-4b1c-ca95-a67ee5e1c02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Embedding layer\n",
        "embedding_dim = 100 # Example embedding dimension\n",
        "model_embedding = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    keras.layers.GlobalAveragePooling1D(), # Or GlobalMaxPooling1D, Flatten\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\") # For binary classification\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "46b87a07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "46b87a07",
        "outputId": "6ec78bfc-2296-464c-9d6f-cd41d68f3f6a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_embedding.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fde93aab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fde93aab",
        "outputId": "da6e03ad-dd5a-4708-cb0f-d2fbf5d60cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.4963 - loss: 0.6935 - val_accuracy: 0.5105 - val_loss: 0.6934\n",
            "Epoch 2/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.6681 - loss: 0.6592 - val_accuracy: 0.5050 - val_loss: 0.7286\n",
            "Epoch 3/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.7296 - loss: 0.5615 - val_accuracy: 0.5010 - val_loss: 0.8066\n",
            "Epoch 4/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.7507 - loss: 0.5181 - val_accuracy: 0.5065 - val_loss: 0.8739\n",
            "Epoch 5/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.7675 - loss: 0.4927 - val_accuracy: 0.5105 - val_loss: 0.9257\n",
            "Epoch 6/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.7784 - loss: 0.4764 - val_accuracy: 0.5075 - val_loss: 0.9619\n",
            "Epoch 7/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7792 - loss: 0.4735 - val_accuracy: 0.5045 - val_loss: 1.0024\n",
            "Epoch 8/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.7860 - loss: 0.4658 - val_accuracy: 0.5045 - val_loss: 1.0277\n",
            "Epoch 9/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.7866 - loss: 0.4627 - val_accuracy: 0.5015 - val_loss: 1.0557\n",
            "Epoch 10/10\n",
            "\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.7894 - loss: 0.4577 - val_accuracy: 0.5045 - val_loss: 1.0770\n"
          ]
        }
      ],
      "source": [
        "# Compile and train (example)\n",
        "model_embedding.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history_embedding = model_embedding.fit(X_train, y_train, epochs=10,\n",
        "                                        validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22537339",
      "metadata": {
        "id": "22537339"
      },
      "source": [
        "### Pre-trained Embeddings (Conceptual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "37fa3737",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37fa3737",
        "outputId": "a1add198-f031-4ee4-e292-59dc06ce72d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File GloVe tidak ditemukan di glove.6B.100d.txt. Pastikan Anda sudah mengunduh dan mengekstraknya.\n",
            "Melewatkan bagian pre-trained embeddings.\n",
            "Tidak melatih model dengan pre-trained embeddings karena file GloVe tidak ditemukan.\n"
          ]
        }
      ],
      "source": [
        "# !wget [http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip)\n",
        "# !unzip -q glove.6B.zip\n",
        "\n",
        "def load_glove_embeddings(filepath):\n",
        "    embeddings_index = {}\n",
        "    with open(filepath, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "glove_embedding_dim = 100\n",
        "glove_filepath = 'glove.6B.100d.txt'\n",
        "\n",
        "try:\n",
        "    glove_embeddings = load_glove_embeddings(glove_filepath)\n",
        "except FileNotFoundError:\n",
        "    print(f\"File GloVe tidak ditemukan di {glove_filepath}. Pastikan Anda sudah mengunduh dan mengekstraknya.\")\n",
        "    print(\"Melewatkan bagian pre-trained embeddings.\")\n",
        "    embedding_matrix_glove = None # Set to None to skip if file not found\n",
        "else:\n",
        "    # Build embedding matrix\n",
        "    embedding_matrix_glove = np.zeros((vocab_size, glove_embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < vocab_size: # Only include words in our vocabulary\n",
        "            embedding_vector = glove_embeddings.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix_glove[i] = embedding_vector\n",
        "\n",
        "    print(\"Ukuran matriks embedding GloVe:\", embedding_matrix_glove.shape)\n",
        "\n",
        "\n",
        "if embedding_matrix_glove is not None:\n",
        "    model_pretrained_embedding = keras.models.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, glove_embedding_dim,\n",
        "                               weights=[embedding_matrix_glove],\n",
        "                               trainable=False, # Freeze pre-trained weights\n",
        "                               input_length=max_len),\n",
        "        keras.layers.GlobalAveragePooling1D(),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    model_pretrained_embedding.summary()\n",
        "\n",
        "    # Compile and train\n",
        "    model_pretrained_embedding.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    history_pretrained = model_pretrained_embedding.fit(X_train, y_train, epochs=10,\n",
        "                                                        validation_data=(X_valid, y_valid))\n",
        "else:\n",
        "    print(\"Tidak melatih model dengan pre-trained embeddings karena file GloVe tidak ditemukan.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff2308e",
      "metadata": {
        "id": "bff2308e"
      },
      "source": [
        "## 3. Encoder-Decoder Network for Machine Translation (Conceptual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9fe7d48b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "9fe7d48b",
        "outputId": "e74bc163-5eb0-44fe-d4d3-a81b16235b34"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m64,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m64,000\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m98,816\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m98,816\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │                   │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │                   │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │    \u001b[38;5;34m129,000\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m1000\u001b[0m)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">64,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">64,000</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">129,000</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m454,632\u001b[0m (1.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">454,632</span> (1.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m454,632\u001b[0m (1.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">454,632</span> (1.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 6.9077\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 6.9048\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 6.9028\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 6.9009\n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 6.8992\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78ae243e1d10>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "source_vocab_size = 1000\n",
        "target_vocab_size = 1000\n",
        "encoder_max_len = 15\n",
        "decoder_max_len = 20\n",
        "embedding_dim_mt = 64\n",
        "rnn_units = 128\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32, name=\"encoder_inputs\")\n",
        "encoder_embedding = keras.layers.Embedding(source_vocab_size, embedding_dim_mt)(encoder_inputs)\n",
        "encoder_outputs, state_h_enc, state_c_enc = keras.layers.LSTM(\n",
        "    rnn_units, return_state=True, name=\"encoder_lstm\"\n",
        ")(encoder_embedding)\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32, name=\"decoder_inputs\")\n",
        "decoder_embedding = keras.layers.Embedding(target_vocab_size, embedding_dim_mt)(decoder_inputs)\n",
        "decoder_lstm = keras.layers.LSTM(\n",
        "    rnn_units, return_sequences=True, name=\"decoder_lstm\"\n",
        ")\n",
        "decoder_outputs = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(target_vocab_size, activation=\"softmax\", name=\"decoder_output\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model\n",
        "model_encoder_decoder = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                                           outputs=decoder_outputs)\n",
        "\n",
        "model_encoder_decoder.summary()\n",
        "\n",
        "# --- Kompilasi dan Pelatihan (Menggunakan data dummy) ---\n",
        "# Untuk pelatihan nyata, y_true_decoder akan menjadi shifted target sequence\n",
        "# (misalnya, [SOS, w1, w2, ..., wN] sebagai input, dan [w1, w2, ..., wN, EOS] sebagai target)\n",
        "dummy_encoder_input_data = np.random.randint(0, source_vocab_size, size=(100, encoder_max_len))\n",
        "dummy_decoder_input_data = np.random.randint(0, target_vocab_size, size=(100, decoder_max_len))\n",
        "dummy_decoder_target_data = np.random.randint(0, target_vocab_size, size=(100, decoder_max_len)) # Example: one-hot encoded or integer labels\n",
        "\n",
        "model_encoder_decoder.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\") # For integer targets\n",
        "model_encoder_decoder.fit([dummy_encoder_input_data, dummy_decoder_input_data], dummy_decoder_target_data, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c501e51",
      "metadata": {
        "id": "8c501e51"
      },
      "source": [
        "## 4. Attention Mechanisms (Conceptual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7c57ede1",
      "metadata": {
        "id": "7c57ede1"
      },
      "outputs": [],
      "source": [
        "# Attention mechanisms are often integrated within Encoder-Decoder models.\n",
        "# TensorFlow 2.x provides `keras.layers.Attention` and `keras.layers.MultiHeadAttention`.\n",
        "\n",
        "# Example of a simple Attention layer (conceptual, requires specific context/inputs):\n",
        "# from keras.layers import Attention\n",
        "\n",
        "# Query = decoder_output # e.g., output of a decoder GRU\n",
        "# Value = encoder_outputs # e.g., sequence of outputs from encoder GRU (return_sequences=True)\n",
        "\n",
        "# attention_output = Attention()([Query, Value])\n",
        "# Then concatenate attention_output with Query for next step."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}