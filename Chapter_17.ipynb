{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bab 17: Autoencoders and GANs (Autoencoder dan GAN)\n",
        "\n",
        "### 1. Pendahuluan\n",
        "\n",
        "Bab 17 membahas dua keluarga arsitektur *Neural Network* yang sangat menarik dan kuat dalam kategori *unsupervised learning* atau *generative models*: **Autoencoders** dan **Generative Adversarial Networks (GANs)**. Kedua jenis model ini berfokus pada pembelajaran representasi data dan/atau generasi data baru yang realistis.\n",
        "\n",
        "### 2. Autoencoders (Autoencoder)\n",
        "\n",
        "Autoencoder adalah Jaringan Saraf Tiruan yang dilatih untuk menghasilkan output yang hampir identik dengan inputnya. Ini mungkin terdengar tidak berguna, tetapi Autoencoder tidak hanya sekadar menyalin input ke output; mereka dilatih untuk menyalin input ke output di bawah beberapa kendala, yang memaksa mereka untuk mempelajari representasi data yang efisien.\n",
        "\n",
        "#### a. Arsitektur Autoencoder (Autoencoder Architecture)\n",
        "Sebuah Autoencoder umumnya terdiri dari dua bagian:\n",
        "* **Encoder:** Bagian jaringan yang mengkompres input menjadi representasi berdimensi rendah, sering disebut **pengkodean (coding)** atau **ruang laten (latent space)**. Ini adalah representasi *bottleneck* dari input.\n",
        "* **Decoder:** Bagian jaringan yang menerima pengkodean (output dari encoder) dan mengembangkannya kembali ke dimensi input asli, mencoba merekonstruksi input.\n",
        "\n",
        "Fungsi *loss* yang digunakan adalah metrik perbedaan antara input asli dan output yang direkonstruksi (misalnya, MSE untuk data numerik, *binary cross-entropy* untuk gambar piksel).\n",
        "\n",
        "#### b. Undercomplete Autoencoders (Autoencoder Kurang Lengkap)\n",
        "Jenis Autoencoder yang paling sederhana di mana dimensi *coding* (lapisan *bottleneck*) lebih kecil dari dimensi input. Ini memaksa Autoencoder untuk mempelajari representasi data yang paling penting, karena ia harus mengkompres informasi.\n",
        "\n",
        "* **Tujuan:** Reduksi dimensi non-linier.\n",
        "* **Masalah:** Jika *coding* terlalu kecil atau model tidak cukup kuat, rekonstruksi akan buruk. Jika terlalu besar, model mungkin belajar fungsi identitas dan tidak mempelajari fitur yang berguna.\n",
        "\n",
        "#### c. Tumpukan Autoencoder (Stacked Autoencoders)\n",
        "Autoencoder dapat ditumpuk dengan melatih beberapa Autoencoder secara berurutan. Setiap Autoencoder dilatih pada output *coding* dari Autoencoder sebelumnya. Ini adalah cara untuk melatih jaringan yang dalam (DNN) secara bertahap.\n",
        "\n",
        "#### d. Autoencoder Konvolusional (Convolutional Autoencoders)\n",
        "Untuk gambar, Autoencoder konvolusional lebih disukai. Encoder biasanya terdiri dari lapisan `Conv2D` dan `MaxPooling2D` (mirip dengan CNN klasik), dan Decoder menggunakan `Conv2DTranspose` (juga dikenal sebagai *deconvolution* atau *transposed convolution*) dan `UpSampling2D` untuk memperbesar gambar kembali ke ukuran aslinya.\n",
        "\n",
        "#### e. Autoencoder Denoising (Denoising Autoencoders)\n",
        "Ini adalah Autoencoder yang dilatih untuk merekonstruksi input \"bersih\" dari input \"bising\" (noisy). Selama pelatihan, *noise* ditambahkan ke input, tetapi fungsi *loss* dihitung berdasarkan perbandingan output dengan input *tanpa noise*. Ini memaksa Autoencoder untuk mempelajari fitur-fitur penting yang membedakan sinyal dari *noise*.\n",
        "\n",
        "#### f. Autoencoder Sparse (Sparse Autoencoders)\n",
        "Autoencoder yang memaksakan sejumlah kecil neuron di lapisan *coding* (atau lapisan tersembunyi lainnya) untuk aktif pada saat tertentu. Ini dapat dicapai dengan menambahkan istilah regularisasi ke fungsi *loss* yang menghukum aktivasi rata-rata yang tinggi (misalnya, KL divergence terhadap distribusi Bernoulli yang jarang). Ini mendorong Autoencoder untuk mempelajari representasi yang lebih informatif dan terpisah.\n",
        "\n",
        "#### g. Autoencoder Variasional (Variational Autoencoders - VAEs)\n",
        "VAEs adalah jenis Autoencoder *generatif* yang sangat populer.\n",
        "* **Perbedaan dari AE Klasik:** VAEs tidak hanya mengkodekan input menjadi satu vektor tunggal dalam ruang laten, tetapi menjadi *distribusi* probabilitas (mean dan *standard deviation*) dalam ruang laten.\n",
        "* **Sampling:** Selama rekonstruksi, vektor dari distribusi ini diambil sampelnya (misalnya, dengan `reparameterization trick` untuk memungkinkan *backpropagation*).\n",
        "* **Fungsi Loss:** Fungsi *loss* VAE memiliki dua bagian:\n",
        "    1.  **Reconstruction Loss:** Mengukur seberapa baik Autoencoder merekonstruksi input (misalnya, MSE).\n",
        "    2.  **Regularization Loss (KL Divergence):** Mengukur seberapa dekat distribusi yang dipelajari di ruang laten dengan distribusi prior yang sederhana (misalnya, distribusi Gaussian standar). Ini memaksa ruang laten menjadi terstruktur dan memungkinkan generasi sampel baru yang berarti.\n",
        "* **Generasi:** Setelah dilatih, VAEs dapat menghasilkan *instance* baru yang realistis dengan mengambil sampel dari distribusi prior di ruang laten dan memberikannya ke *decoder*.\n",
        "\n",
        "### 3. Generative Adversarial Networks (GANs)\n",
        "\n",
        "GANs adalah arsitektur *generatif* yang sangat inovatif yang pertama kali diperkenalkan oleh Ian Goodfellow pada tahun 2014. Mereka terdiri dari dua Jaringan Saraf Tiruan yang bersaing dalam permainan *zero-sum*:\n",
        "\n",
        "* **Generator:** Jaringan yang mencoba menghasilkan data baru yang realistis (misalnya, gambar) yang mirip dengan *training data* asli. Inputnya adalah vektor *noise* acak (seringkali dari distribusi Gaussian).\n",
        "* **Discriminator:** Jaringan yang mencoba membedakan antara data \"nyata\" (dari *training set* asli) dan data \"palsu\" (dihasilkan oleh Generator). Ini adalah pengklasifikasi biner.\n",
        "\n",
        "#### a. Pelatihan GAN (Training a GAN)\n",
        "Pelatihan GAN adalah proses iteratif dua langkah:\n",
        "1.  **Langkah Diskriminator:** Discriminator dilatih untuk mengklasifikasikan *instance* nyata sebagai nyata, dan *instance* palsu (yang dihasilkan oleh Generator) sebagai palsu.\n",
        "2.  **Langkah Generator:** Generator dilatih untuk menghasilkan data yang lebih baik, tujuannya adalah \"menipu\" Discriminator agar berpikir bahwa data yang dihasilkan itu nyata. Selama langkah ini, bobot Discriminator dibekukan.\n",
        "\n",
        "Game ini berlanjut hingga Generator menghasilkan data yang sangat realistis sehingga Discriminator tidak dapat lagi membedakan antara data nyata dan palsu (probabilitas 0.5).\n",
        "\n",
        "#### b. Tantangan Pelatihan GAN (Challenges in Training GANs)\n",
        "GANs terkenal sulit dilatih:\n",
        "* **Mode Collapse:** Generator mungkin hanya menghasilkan beberapa jenis *instance* yang sangat meyakinkan, mengabaikan keragaman dataset.\n",
        "* **Vanishing Gradients (Discriminator terlalu kuat):** Jika Discriminator menjadi terlalu kuat, gradien yang diterima Generator bisa menjadi sangat kecil, menghentikan pembelajarannya.\n",
        "* **Ketidakstabilan Pelatihan:** Pelatihan bisa tidak stabil dan tidak konvergen.\n",
        "\n",
        "#### c. Deep Convolutional GANs (DCGANs)\n",
        "DCGANs adalah GANs yang menggunakan lapisan konvolusional (tanpa *pooling* dan lapisan *fully connected* di Generator dan Discriminator) untuk menghasilkan gambar. Mereka juga menggunakan *Batch Normalization* secara ekstensif. DCGANs jauh lebih stabil dan mampu menghasilkan gambar yang lebih realistis.\n",
        "\n",
        "#### d. Conditional GANs (CGANs)\n",
        "CGANs memungkinkan kontrol atas data yang dihasilkan. Anda memberikan input tambahan (misalnya, label kelas, atau gambar lain) ke Generator dan Discriminator, sehingga Generator dapat menghasilkan data yang spesifik berdasarkan kondisi yang diberikan.\n",
        "\n",
        "#### e. Wasserstein GANs (WGANs)\n",
        "WGANs memperkenalkan *loss function* baru (Wasserstein distance atau Earth Mover's distance) yang lebih stabil daripada *binary cross-entropy* tradisional. Ini membantu mengatasi masalah *vanishing gradients* dan *mode collapse* pada GANs.\n",
        "\n",
        "### 4. Kesimpulan\n",
        "\n",
        "Bab 17 memperkenalkan dua kelas model *Deep Learning* yang penting dan menarik dalam ranah *unsupervised learning* dan *generative models*. Autoencoders dibahas sebagai alat untuk reduksi dimensi non-linier dan pembelajaran representasi (termasuk *denoising*, *sparse*, dan *variational* autoencoders). GANs diperkenalkan sebagai model *generatif* revolusioner yang terdiri dari dua jaringan yang bersaing, mampu menghasilkan data baru yang sangat realistis, meskipun pelatihannya menantang. Pemahaman di bab ini adalah fondasi untuk bidang *generative AI* yang berkembang pesat."
      ],
      "metadata": {
        "id": "Hs_09oEtg3GN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "QGbRJecag5Rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BVRKWuAog6LK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Fashion MNIST (as used in previous chapters, for image data)\n"
      ],
      "metadata": {
        "id": "fZELS9dyg7QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "metadata": {
        "id": "dShkQ5JHg8O9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Autoencoders"
      ],
      "metadata": {
        "id": "3W5nk4gDg_oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Undercomplete Autoencoder"
      ],
      "metadata": {
        "id": "89wYKxHehAWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder layers\n",
        "encoder = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(30, activation=\"relu\") # Coding layer (bottleneck)\n",
        "])\n",
        "\n",
        "# Decoder layers\n",
        "decoder = keras.models.Sequential([\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"), # Output layer reconstructs image\n",
        "    keras.layers.Reshape([28, 28]) # Reshape output to match input image shape\n",
        "])\n",
        "\n",
        "# Full Autoencoder model\n",
        "autoencoder = keras.models.Sequential([encoder, decoder])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkjOgxkZhBH5",
        "outputId": "b247949c-ffb6-4ed0-aa45-8d2cd4b27fe1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0))\n",
        "# For images, binary_crossentropy is common for pixel values 0-1 (treated as probabilities)\n",
        "# For data already normalized to 0-1, you might use MSE as well if you assume Gaussian noise.\n",
        "# The book uses binary_crossentropy for pixel intensities."
      ],
      "metadata": {
        "id": "fMMW5pJFhCSe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Autoencoder\n",
        "autoencoder.fit(X_train, X_train, epochs=10,\n",
        "                validation_data=(X_valid, X_valid))\n",
        "# Note: X_train is passed as both input and target for autoencoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68TeCBxPhC_l",
        "outputId": "f3098ccc-5d89-4d65-a5e0-62dbbc6b1ed3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.4356 - val_loss: 0.3206\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.3192 - val_loss: 0.3071\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.3093 - val_loss: 0.3027\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.3034 - val_loss: 0.2973\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.3000 - val_loss: 0.2947\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.2965 - val_loss: 0.2919\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 0.2950 - val_loss: 0.2891\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.2927 - val_loss: 0.2877\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.2908 - val_loss: 0.2872\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.2898 - val_loss: 0.2856\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4d59feaed0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacked Autoencoders"
      ],
      "metadata": {
        "id": "uveLvWXchETu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# First Autoencoder (similar to undercomplete AE above)\n",
        "stacked_encoder = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(30, activation=\"relu\") # Coding layer\n",
        "])\n",
        "stacked_decoder = keras.models.Sequential([\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
        "    keras.layers.Reshape([28, 28]) # Add Reshape layer here\n",
        "])\n",
        "stacked_autoencoder = keras.models.Sequential([stacked_encoder, stacked_decoder])\n",
        "stacked_autoencoder.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "# Train the first autoencoder\n",
        "history1 = stacked_autoencoder.fit(X_train, X_train, epochs=10,\n",
        "                                  validation_data=(X_valid, X_valid))\n",
        "\n",
        "# Extract the encoder part to train the next autoencoder\n",
        "X_train_code = stacked_encoder.predict(X_train)\n",
        "X_valid_code = stacked_encoder.predict(X_valid)\n",
        "\n",
        "# Second Autoencoder (trained on the codes from the first)\n",
        "hidden_neurons_2 = 10 # Smaller bottleneck\n",
        "autoencoder_2 = keras.models.Sequential([\n",
        "    keras.layers.Dense(hidden_neurons_2, activation=\"relu\", input_shape=[stacked_encoder.layers[-1].units]),\n",
        "    keras.layers.Dense(stacked_encoder.layers[-1].units, activation=\"relu\")\n",
        "])\n",
        "autoencoder_2_decoder = keras.models.Sequential([\n",
        "    keras.layers.Dense(stacked_encoder.layers[-1].units, activation=\"relu\"),\n",
        "    keras.layers.Dense(28 * 28, activation=\"sigmoid\") # This part might connect back differently\n",
        "])\n",
        "\n",
        "# For practical stacked autoencoders, you often train each layer greedily.\n",
        "# The common approach in Keras is to build the full stacked autoencoder, then\n",
        "# use transfer learning or freeze layers.\n",
        "\n",
        "# Full Stacked Autoencoder (end-to-end)\n",
        "stacked_autoencoder_full = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(100, activation=\"relu\"), # First encoder layer\n",
        "    keras.layers.Dense(30, activation=\"relu\"), # Second encoder layer (coding)\n",
        "    keras.layers.Dense(100, activation=\"relu\"), # First decoder layer\n",
        "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"), # Output layer\n",
        "    keras.layers.Reshape([28, 28]) # Add Reshape layer here for the full autoencoder\n",
        "])\n",
        "stacked_autoencoder_full.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "# history_full_stacked = stacked_autoencoder_full.fit(X_train, X_train, epochs=10,\n",
        "#                                                     validation_data=(X_valid, X_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lukLqpC7hFMu",
        "outputId": "1165ab39-c3fd-46e8-d783-df90f3d84ac2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.4328 - val_loss: 0.3162\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.3172 - val_loss: 0.3087\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 0.3071 - val_loss: 0.2993\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.3023 - val_loss: 0.2973\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 0.2987 - val_loss: 0.2932\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.2961 - val_loss: 0.2938\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.2940 - val_loss: 0.2939\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.2912 - val_loss: 0.2887\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.2897 - val_loss: 0.2857\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.2881 - val_loss: 0.2916\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Autoencoders"
      ],
      "metadata": {
        "id": "RHoq-sqzhGkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "conv_encoder = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(16, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[28, 28, 1]),\n",
        "    keras.layers.MaxPooling2D(pool_size=2), # Output 14x14x16\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPooling2D(pool_size=2), # Output 7x7x32\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"), # Coding layer output: 7x7x64\n",
        "])\n",
        "\n",
        "# Decoder\n",
        "conv_decoder = keras.models.Sequential([\n",
        "    keras.layers.Conv2DTranspose(32, kernel_size=3, padding=\"same\", activation=\"relu\",\n",
        "                                 input_shape=[7, 7, 64]), # Matches encoder output shape\n",
        "    keras.layers.UpSampling2D(size=2), # Output 14x14x32\n",
        "    keras.layers.Conv2DTranspose(16, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.UpSampling2D(size=2), # Output 28x28x16\n",
        "    keras.layers.Conv2D(1, kernel_size=3, padding=\"same\", activation=\"sigmoid\") # Reconstructs grayscale image\n",
        "])\n",
        "\n",
        "# Full Convolutional Autoencoder\n",
        "conv_autoencoder = keras.models.Sequential([conv_encoder, conv_decoder])\n",
        "conv_autoencoder.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "# Need to reshape X_train for Conv2D input (add channel dimension)\n",
        "X_train_reshaped = X_train[..., np.newaxis]\n",
        "X_valid_reshaped = X_valid[..., np.newaxis]\n",
        "\n",
        "# Training the Conv Autoencoder\n",
        "conv_autoencoder.fit(X_train_reshaped, X_train_reshaped, epochs=10,\n",
        "                     validation_data=(X_valid_reshaped, X_valid_reshaped))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U08xdZAehHXh",
        "outputId": "a22a88af-744c-4759-e7ae-adb806dc054b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv_transpose.py:94: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 60ms/step - loss: 0.3070 - val_loss: 0.2620\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 59ms/step - loss: 0.2625 - val_loss: 0.2553\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 58ms/step - loss: 0.2577 - val_loss: 0.2529\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 60ms/step - loss: 0.2556 - val_loss: 0.2515\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 62ms/step - loss: 0.2541 - val_loss: 0.2518\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 59ms/step - loss: 0.2534 - val_loss: 0.2500\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 62ms/step - loss: 0.2522 - val_loss: 0.2494\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 59ms/step - loss: 0.2524 - val_loss: 0.2493\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 59ms/step - loss: 0.2526 - val_loss: 0.2488\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 60ms/step - loss: 0.2513 - val_loss: 0.2486\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4d59fd8150>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Denoising Autoencoders"
      ],
      "metadata": {
        "id": "5Rias8KShIlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add noise to the input data for training\n",
        "noise_factor = 0.2\n",
        "X_train_noisy = X_train + noise_factor * np.random.randn(*X_train.shape)\n",
        "X_valid_noisy = X_valid + noise_factor * np.random.randn(*X_valid.shape)\n",
        "X_test_noisy = X_test + noise_factor * np.random.randn(*X_test.shape)\n",
        "\n",
        "# Clip values to stay within [0, 1] range\n",
        "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
        "X_valid_noisy = np.clip(X_valid_noisy, 0., 1.)\n",
        "X_test_noisy = np.clip(X_test_noisy, 0., 1.)\n",
        "\n",
        "# The autoencoder architecture can be the same as standard AEs (e.g., conv_autoencoder)\n",
        "# The key is training with noisy input, but clean target.\n",
        "denoising_autoencoder = conv_autoencoder # Using the previously defined conv_autoencoder structure\n",
        "\n",
        "# Compile and train\n",
        "denoising_autoencoder.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "denoising_autoencoder.fit(X_train_noisy[..., np.newaxis], X_train[..., np.newaxis], epochs=10,\n",
        "                          validation_data=(X_valid_noisy[..., np.newaxis], X_valid[..., np.newaxis]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3ewNoLbhJV3",
        "outputId": "820850b6-e3eb-4891-9680-32e77185ca06"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 63ms/step - loss: 0.2707 - val_loss: 0.2622\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 63ms/step - loss: 0.2647 - val_loss: 0.2610\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 64ms/step - loss: 0.2633 - val_loss: 0.2605\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 63ms/step - loss: 0.2633 - val_loss: 0.2602\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 62ms/step - loss: 0.2629 - val_loss: 0.2603\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 62ms/step - loss: 0.2631 - val_loss: 0.2597\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 61ms/step - loss: 0.2625 - val_loss: 0.2595\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 61ms/step - loss: 0.2621 - val_loss: 0.2595\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 63ms/step - loss: 0.2625 - val_loss: 0.2598\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 62ms/step - loss: 0.2617 - val_loss: 0.2594\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4d5b638390>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sparse Autoencoders"
      ],
      "metadata": {
        "id": "TpViAgEKhLvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Add activity regularizer to Dense layers to encourage sparsity\n",
        "# alpha is the sparsity parameter\n",
        "# Example:\n",
        "# Keras does not have a direct \"KL Divergence\" regularizer in `keras.regularizers` for sparsity.\n",
        "# You'd typically implement it as a custom loss or a custom layer.\n",
        "# A simpler approximation is L1 regularization on activations, though it's not truly sparse AE.\n",
        "\n",
        "model_sparse = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(100, activation=\"relu\", activity_regularizer=keras.regularizers.l1(1e-3)),\n",
        "    keras.layers.Dense(30, activation=\"relu\", activity_regularizer=keras.regularizers.l1(1e-3)),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
        "    keras.layers.Reshape([28, 28]) # Add Reshape layer here\n",
        "])\n",
        "model_sparse.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "model_sparse.fit(X_train, X_train, epochs=10, validation_data=(X_valid, X_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc6sP8NwhMdj",
        "outputId": "b1c24720-38f4-40af-d4b8-dcb0e1c80f33"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 0.5388 - val_loss: 0.4898\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.4911 - val_loss: 0.4898\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.4899 - val_loss: 0.4897\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - loss: 0.4905 - val_loss: 0.4897\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.4907 - val_loss: 0.4899\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - loss: 0.4906 - val_loss: 0.4897\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - loss: 0.4905 - val_loss: 0.4898\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - loss: 0.4910 - val_loss: 0.4897\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.4907 - val_loss: 0.4897\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - loss: 0.4908 - val_loss: 0.4897\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4d583ce550>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational Autoencoders (VAEs)"
      ],
      "metadata": {
        "id": "onUkoycThNr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Encoder (generates mean and log_variance of the latent distribution)\n",
        "codings_size = 10 # Dimension of latent space\n",
        "\n",
        "class Sampling(keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        mean, log_var = inputs\n",
        "        epsilon = tf.random.normal(tf.shape(log_var)) # Sample from N(0, 1)\n",
        "        return mean + tf.exp(0.5 * log_var) * epsilon # Reparameterization trick\n",
        "\n",
        "# Define the encoder using the Functional API to get mean, log_var, and sampled z\n",
        "inputs_vae_func = keras.layers.Input(shape=[28, 28])\n",
        "flatten_input_func = keras.layers.Flatten()(inputs_vae_func)\n",
        "dense_150_func = keras.layers.Dense(150, activation=\"relu\")(flatten_input_func)\n",
        "dense_100_func = keras.layers.Dense(100, activation=\"relu\")(dense_150_func)\n",
        "z_mean_log_var_func = keras.layers.Dense(2 * codings_size)(dense_100_func)\n",
        "z_mean_func = keras.layers.Dense(codings_size, name='z_mean')(z_mean_log_var_func)\n",
        "z_log_var_func = keras.layers.Dense(codings_size, name='z_log_var')(z_mean_log_var_func)\n",
        "z_func = Sampling()([z_mean_func, z_log_var_func])\n",
        "\n",
        "# Create the functional encoder model that outputs mean, log_var, and z\n",
        "variational_encoder_func = keras.models.Model(\n",
        "    inputs=inputs_vae_func,\n",
        "    outputs=[z_mean_func, z_log_var_func, z_func],\n",
        "    name=\"vae_encoder\"\n",
        ")\n",
        "\n",
        "# Define the decoder\n",
        "decoder_vae = keras.models.Sequential([\n",
        "    keras.layers.Dense(100, activation=\"relu\", input_shape=[codings_size]),\n",
        "    keras.layers.Dense(150, activation=\"relu\"),\n",
        "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
        "    keras.layers.Reshape([28, 28]) # Reshape to image dimensions\n",
        "], name=\"vae_decoder\")\n",
        "\n",
        "# Define the VAE model by subclassing keras.Model\n",
        "class VariationalAutoencoder(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Get mean, log_var, and z from the encoder\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "\n",
        "        # Reconstruct the image using the decoder\n",
        "        reconstruction = self.decoder(z)\n",
        "\n",
        "        # Calculate reconstruction loss (binary crossentropy)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            keras.losses.binary_crossentropy(tf.reshape(inputs, [-1, 28 * 28]), tf.reshape(reconstruction, [-1, 28 * 28]))\n",
        "        )\n",
        "\n",
        "        # Calculate KL divergence loss\n",
        "        kl_loss = -0.5 * tf.reduce_sum(\n",
        "            1 + z_log_var - tf.exp(z_log_var) - tf.square(z_mean),\n",
        "            axis=-1\n",
        "        )\n",
        "        kl_loss = tf.reduce_mean(kl_loss)\n",
        "\n",
        "        # Add the total VAE loss to the model's losses\n",
        "        total_vae_loss = reconstruction_loss + kl_loss\n",
        "        self.add_loss(total_vae_loss)\n",
        "\n",
        "        return reconstruction # Return the reconstruction\n",
        "\n",
        "# Instantiate the custom VAE model\n",
        "vae = VariationalAutoencoder(variational_encoder_func, decoder_vae)\n",
        "\n",
        "# Compile the VAE model - loss is added internally, so set loss to None\n",
        "optimizer_vae = keras.optimizers.Adam(learning_rate=0.001)\n",
        "vae.compile(optimizer=optimizer_vae, loss=None)\n",
        "\n",
        "# Train the VAE\n",
        "history_vae = vae.fit(X_train, epochs=10, batch_size=32,\n",
        "                      validation_data=(X_valid,)) # Pass validation data as a tuple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMw1-IMYhOcl",
        "outputId": "8b10318a-f8a2-4789-e335-d1cc5adee836"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.5282 - val_loss: 0.4910\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - loss: 0.4911 - val_loss: 0.4902\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - loss: 0.4909 - val_loss: 0.4901\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - loss: 0.4907 - val_loss: 0.4902\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - loss: 0.4907 - val_loss: 0.4898\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 0.4913 - val_loss: 0.4899\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - loss: 0.4904 - val_loss: 0.4901\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - loss: 0.4902 - val_loss: 0.4899\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - loss: 0.4904 - val_loss: 0.4897\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - loss: 0.4903 - val_loss: 0.4898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generative Adversarial Networks (GANs)"
      ],
      "metadata": {
        "id": "757pZXH2hPtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple GAN (Discriminator and Generator)"
      ],
      "metadata": {
        "id": "FR6qXFTjhQwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator (a binary classifier)\n",
        "discriminator = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(150, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\") # Output 0 (fake) or 1 (real)\n",
        "])\n",
        "\n",
        "# Generator (takes random noise and generates an image)\n",
        "codings_size_gan = 100 # Size of the random noise vector\n",
        "generator = keras.models.Sequential([\n",
        "    keras.layers.Dense(100, activation=\"relu\", input_shape=[codings_size_gan]),\n",
        "    keras.layers.Dense(150, activation=\"relu\"),\n",
        "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"), # Output image pixels\n",
        "    keras.layers.Reshape([28, 28])\n",
        "])\n",
        "\n",
        "# GAN (Generator + Discriminator)\n",
        "gan = keras.models.Sequential([generator, discriminator])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1nQccOkhRlf",
        "outputId": "baa764cf-7097-4067-f065-78c0015790b4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the GAN\n",
        "# Freeze discriminator when training the generator\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "discriminator.trainable = False # Important for GAN training loop\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
      ],
      "metadata": {
        "id": "t23h3FLthSkn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom training loop for GANs (as described in the book)\n",
        "# This is a conceptual implementation of the training loop for clarity.\n",
        "\n",
        "def train_gan(gan, dataset, codings_size, n_epochs, batch_size):\n",
        "    generator, discriminator = gan.layers\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
        "        for X_batch in dataset: # Assuming dataset yields only X (images)\n",
        "            # Get the actual batch size for the current batch\n",
        "            current_batch_size = tf.shape(X_batch)[0]\n",
        "\n",
        "            # Phase 1: Train the discriminator\n",
        "            noise = tf.random.normal(shape=[current_batch_size, codings_size])\n",
        "            generated_images = generator(noise)\n",
        "            # Cast X_batch to the same data type as generated_images (tf.float32)\n",
        "            X_batch = tf.cast(X_batch, tf.float32)\n",
        "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
        "            # Create y_discriminator with the actual size of the combined batch using tf.zeros and tf.ones\n",
        "            y_fake = tf.zeros((current_batch_size, 1), dtype=tf.float32)\n",
        "            y_real = tf.ones((current_batch_size, 1), dtype=tf.float32)\n",
        "            y_discriminator = tf.concat([y_fake, y_real], axis=0) # 0 for fake, 1 for real\n",
        "\n",
        "            discriminator.trainable = True\n",
        "            discriminator.train_on_batch(X_fake_and_real, y_discriminator)\n",
        "\n",
        "            # Phase 2: Train the generator (freeze discriminator)\n",
        "            # Use the same actual batch size for generator noise\n",
        "            noise = tf.random.normal(shape=[current_batch_size, codings_size])\n",
        "            # Create y_generator with the actual batch size using tf.ones\n",
        "            y_generator = tf.ones((current_batch_size, 1), dtype=tf.float32) # Generator tries to fool discriminator (predict real)\n",
        "            discriminator.trainable = False # Freeze discriminator\n",
        "            gan.train_on_batch(noise, y_generator) # Train the GAN (which trains the generator)\n",
        "\n",
        "# # Example dataset (Fashion MNIST X_train)\n",
        "batch_size_gan = 32\n",
        "train_dataset_gan = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000).batch(batch_size_gan)\n",
        "train_gan(gan, train_dataset_gan, codings_size_gan, n_epochs=10, batch_size=batch_size_gan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQCmq_8rhUMX",
        "outputId": "04752d4f-af31-4bd9-9ec2-9ab67da438fc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Convolutional GANs (DCGANs)"
      ],
      "metadata": {
        "id": "gPhYH9AfhV3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DCGAN Generator\n",
        "dcgan_generator = keras.models.Sequential([\n",
        "    keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size_gan]),\n",
        "    keras.layers.Reshape([7, 7, 128]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\"), # 14x14x64\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(32, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\"), # 28x28x32\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=1, padding=\"same\", activation=\"sigmoid\") # 28x28x1\n",
        "])\n",
        "\n",
        "# DCGAN Discriminator\n",
        "dcgan_discriminator = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=5, strides=2, padding=\"same\", activation=keras.layers.LeakyReLU(0.2), input_shape=[28, 28, 1]), # 14x14x32\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\", activation=keras.layers.LeakyReLU(0.2)), # 7x7x64\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Full DCGAN\n",
        "dcgan = keras.models.Sequential([dcgan_generator, dcgan_discriminator])\n",
        "\n",
        "# Compile and train (similar to simple GAN, but with Conv2D inputs/outputs)\n",
        "dcgan_discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "dcgan_discriminator.trainable = False\n",
        "dcgan.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXac_YKVhWu9",
        "outputId": "c4546e56-5115-4516-8633-c08a5728c679"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    }
  ]
}