{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4202c31",
      "metadata": {
        "id": "e4202c31"
      },
      "source": [
        "# Bab 12: Custom Models and Training with TensorFlow (Model Kustom dan Pelatihan dengan TensorFlow)\n",
        "\n",
        "### 1. Pendahuluan\n",
        "\n",
        "Bab 11 telah membahas berbagai teknik untuk melatih *Deep Neural Networks* menggunakan Keras API yang *high-level*. Bab 12 ini akan menyelam lebih dalam ke TensorFlow, mengeksplorasi cara membangun model kustom, mendefinisikan lapisan kustom, fungsi *loss* kustom, metrik kustom, dan bahkan *training loop* kustom. Ini diperlukan ketika Keras Sequential atau Functional API tidak cukup fleksibel untuk arsitektur atau alur kerja yang sangat spesifik.\n",
        "\n",
        "Bab ini berfokus pada fleksibilitas dan kontrol yang ditawarkan oleh TensorFlow yang lebih *low-level* (tetapi masih dalam konteks TensorFlow 2.x dengan *eager execution*).\n",
        "\n",
        "### 2. TensorFlow Low-Level API (API TensorFlow Tingkat Rendah)\n",
        "\n",
        "Meskipun Keras sangat berguna, memahami API TensorFlow tingkat rendah memberikan kontrol yang lebih besar dan pemahaman yang lebih dalam tentang apa yang terjadi di balik layar.\n",
        "\n",
        "#### a. Tensors and Operations (Tensor dan Operasi)\n",
        "* **Tensor:** Representasi dasar data di TensorFlow. Mirip dengan array NumPy, tetapi dapat berjalan di GPU atau TPU.\n",
        "* **Operasi (Operations):** Fungsi yang memanipulasi *tensor*. TensorFlow menyediakan berbagai operasi matematika, aljabar linier, dan lainnya.\n",
        "* TensorFlow secara otomatis membangun grafik komputasi (grafik operasi) di belakang layar, bahkan dengan *eager execution*. Grafik ini dioptimalkan untuk kinerja dan dapat dijalankan di berbagai perangkat.\n",
        "\n",
        "#### b. Graphs and Sessions (Grafik dan Sesi)\n",
        "* Pada TensorFlow 1.x, pengguna harus secara eksplisit membangun grafik komputasi dan menjalankannya dalam sebuah sesi.\n",
        "* Pada TensorFlow 2.x (dengan *eager execution* aktif secara *default*), eksekusi bersifat imperatif (operasi segera dieksekusi, mirip Python biasa). Namun, TensorFlow masih membangun grafik secara internal untuk optimisasi kinerja (misalnya, dengan menggunakan `@tf.function`).\n",
        "* **`@tf.function`:** Dekorator Python yang mengkompilasi fungsi Python menjadi grafik TensorFlow yang sangat efisien. Ini sangat penting untuk meningkatkan kinerja kode yang ditulis dalam *eager execution*.\n",
        "\n",
        "### 3. Custom Loss Functions (Fungsi Loss Kustom)\n",
        "\n",
        "Ketika fungsi *loss* standar (misalnya, `mse`, `categorical_crossentropy`) tidak memadai untuk masalah Anda, Anda dapat membuat fungsi *loss* kustom.\n",
        "\n",
        "* Fungsi *loss* kustom harus menerima `y_true` (label sebenarnya) dan `y_pred` (prediksi model) sebagai input dan mengembalikan satu *scalar* yang mewakili nilai *loss*.\n",
        "* Contoh fungsi *loss* khusus untuk regresi yang memberikan bobot lebih pada *error* besar (Huber loss) ditunjukkan.\n",
        "\n",
        "### 4. Custom Activation Functions, Initializers, Regularizers, and Constraints (Fungsi Aktivasi, Initializer, Regularizer, dan Constraint Kustom)\n",
        "\n",
        "Anda dapat membuat fungsi atau objek kustom untuk aspek-aspek lain dari model Anda:\n",
        "\n",
        "* **Fungsi Aktivasi Kustom:** Fungsi Python yang menerima *tensor* sebagai input dan mengembalikan *tensor* yang diaktifkan.\n",
        "* **Initializer Kustom:** Fungsi atau kelas yang menginisialisasi bobot lapisan.\n",
        "* **Regularizer Kustom:** Fungsi atau kelas yang menambahkan penalti ke *loss*.\n",
        "* **Constraint Kustom:** Fungsi atau kelas yang menerapkan kendala pada bobot lapisan (misalnya, `tf.keras.constraints.max_norm`).\n",
        "\n",
        "Anda dapat menyertakan fungsi kustom ini dalam model Keras Anda dengan melewatkannya sebagai argumen ke lapisan yang relevan.\n",
        "\n",
        "### 5. Custom Models (Model Kustom)\n",
        "\n",
        "Keras Subclassing API (yang diperkenalkan di Bab 10) adalah cara yang lebih fleksibel untuk membangun model yang sepenuhnya kustom.\n",
        "\n",
        "* Anda mewarisi dari `tf.keras.Model`.\n",
        "* Inisialisasi lapisan Anda di metode `__init__()`.\n",
        "* Definisikan *forward pass* di metode `call()`.\n",
        "* Ini memungkinkan model dengan *loops*, logika kondisional, atau berbagai arsitektur dinamis.\n",
        "* Contoh model dengan *skip connection* atau *residual blocks* dapat diimplementasikan dengan mudah menggunakan pendekatan ini.\n",
        "\n",
        "### 6. Custom Layers (Lapisan Kustom)\n",
        "\n",
        "Ketika Anda membutuhkan lapisan yang melakukan sesuatu yang tidak standar (misalnya, lapisan yang tidak memiliki bobot, atau memiliki beberapa input/output yang aneh), Anda dapat membuat lapisan kustom.\n",
        "\n",
        "* Anda mewarisi dari `tf.keras.layers.Layer`.\n",
        "* Metode `__init__()`: Buat sub-lapisan dan variabel (weights) yang akan digunakan oleh lapisan ini. Gunakan `self.add_weight()`.\n",
        "* Metode `build(input_shape)`: Ini dipanggil pertama kali lapisan digunakan. Di sinilah Anda dapat membuat variabel-variabel yang bergantung pada bentuk input.\n",
        "* Metode `call(inputs)`: Definisikan *forward pass* lapisan.\n",
        "* Metode `compute_output_shape(input_shape)`: Definisikan bentuk output lapisan (opsional, TensorFlow dapat menyimpulkan ini secara otomatis di banyak kasus).\n",
        "\n",
        "### 7. Custom Training Loops (Loop Pelatihan Kustom)\n",
        "\n",
        "Meskipun `model.fit()` sangat nyaman, terkadang Anda membutuhkan kontrol penuh atas proses pelatihan. Ini mungkin diperlukan untuk algoritma pelatihan yang sangat spesifik (misalnya, *GANs*, *Reinforcement Learning*), atau jika Anda ingin mengimplementasikan teknik pelatihan tingkat rendah yang tidak didukung langsung oleh Keras.\n",
        "\n",
        "Langkah-langkah umum dalam *training loop* kustom:\n",
        "1.  **Mendefinisikan *Optimizer* dan *Loss Function*.**\n",
        "2.  **Mengiterasi *Epochs*:** Loop untuk setiap *epoch*.\n",
        "3.  **Mengiterasi *Batches*:** Loop untuk setiap *mini-batch* dari data.\n",
        "4.  **Menghitung Gradien (Forward Pass + Backward Pass):**\n",
        "    * Menggunakan `tf.GradientTape()` untuk merekam operasi dan menghitung gradien.\n",
        "    * `with tf.GradientTape() as tape:`\n",
        "    * `loss = loss_fn(y_true, y_pred)`\n",
        "    * `gradients = tape.gradient(loss, model.trainable_variables)`\n",
        "5.  **Memperbarui Parameter:**\n",
        "    * `optimizer.apply_gradients(zip(gradients, model.trainable_variables))`\n",
        "6.  **Memperbarui Metrik:** Menggunakan `tf.keras.metrics` untuk melacak kinerja.\n",
        "7.  **Menampilkan Kemajuan:** Mencetak metrik di setiap *epoch*.\n",
        "\n",
        "### 8. TensorFlow Functions (Fungsi TensorFlow)\n",
        "\n",
        "Menggunakan `@tf.function` adalah kunci untuk mendapatkan kinerja yang baik dari kode TensorFlow yang ditulis dalam *eager execution*. Ini mengkompilasi fungsi Python menjadi grafik TensorFlow yang dapat dioptimalkan dan dijalankan dengan sangat efisien.\n",
        "\n",
        "* Ketika fungsi dengan `@tf.function` pertama kali dipanggil, TensorFlow akan menelusuri kodenya untuk membuat grafik.\n",
        "* Panggilan selanjutnya akan langsung menjalankan grafik yang sudah dikompilasi, yang jauh lebih cepat daripada eksekusi *eager* biasa.\n",
        "* **Aturan untuk `@tf.function`:** Hindari penggunaan *Python side effects* (misalnya, mencetak, memodifikasi daftar Python) di dalam fungsi yang dihiasi, karena efek samping ini hanya terjadi pada jejak pertama.\n",
        "\n",
        "### 9. Kesimpulan\n",
        "\n",
        "Bab 12 adalah panduan esensial untuk para ahli dan mereka yang membutuhkan fleksibilitas maksimum di TensorFlow. Ini menunjukkan bagaimana membuat fungsi *loss*, aktivasi, *initializer*, *regularizer*, dan *constraint* kustom. Lebih penting lagi, ia mendemonstrasikan bagaimana membangun model dan lapisan kustom menggunakan Subclassing API, serta mengimplementasikan *training loop* kustom yang memberikan kontrol penuh atas proses pembelajaran. Penggunaan `@tf.function` adalah kunci untuk mengkompilasi dan mengoptimalkan kode yang kompleks menjadi grafik TensorFlow yang berkinerja tinggi."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9527f36",
      "metadata": {
        "id": "f9527f36"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70ad4f80",
      "metadata": {
        "id": "70ad4f80"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "619fc4e3",
      "metadata": {
        "id": "619fc4e3"
      },
      "source": [
        "### Loading Fashion MNIST (as used in previous chapters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "451f2429",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "451f2429",
        "outputId": "e3f1343e-899d-4827-d554-68d2adfd9f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3546ab6",
      "metadata": {
        "id": "e3546ab6"
      },
      "source": [
        "## 2. TensorFlow Low-Level API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f83af2af",
      "metadata": {
        "id": "f83af2af"
      },
      "source": [
        "### Tensors and Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "89f0cb19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89f0cb19",
        "outputId": "227598d0-dcb3-4dcd-c0e1-c2714934f5bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.constant([[1., 2., 3.], [4., 5., 6.]]) # a matrix\n",
        "tf.constant(42) # a scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9bf6ad4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bf6ad4c",
        "outputId": "7565bb71-5943-4d37-8795-d6075afa81a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 5.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "t[:, 1:]\n",
        "t[..., 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b6fde6b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6fde6b4",
        "outputId": "0813b2b2-7f7e-4c72-cf67-20a8dae9be0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[14., 32.],\n",
              "       [32., 77.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "t + 10\n",
        "tf.square(t)\n",
        "t @ tf.transpose(t) # Matrix multiplication"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfee8818",
      "metadata": {
        "id": "cfee8818"
      },
      "source": [
        "### Graphs and Sessions (TensorFlow 2.x with @tf.function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "59a3cb37",
      "metadata": {
        "id": "59a3cb37"
      },
      "outputs": [],
      "source": [
        "def huber_fn(y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < 1\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss = tf.abs(error) - 0.5\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1f44a835",
      "metadata": {
        "id": "1f44a835"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def compiled_huber_fn(y_true, y_pred):\n",
        "    return huber_fn(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c02b7de",
      "metadata": {
        "id": "7c02b7de"
      },
      "source": [
        "## 3. Custom Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "283ef9d8",
      "metadata": {
        "id": "283ef9d8"
      },
      "outputs": [],
      "source": [
        "def create_huber_loss(threshold=1.0):\n",
        "    def huber_loss(y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = threshold * tf.abs(error) - 0.5 * threshold**2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "    return huber_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0beb1caf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0beb1caf",
        "outputId": "f97a2c67-cbcd-4f62-8006-39d175b586ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - loss: 0.7612\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.5685\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4371 \n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3132 \n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2080 \n"
          ]
        }
      ],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Use the custom loss function\n",
        "model.compile(loss=create_huber_loss(threshold=2.0), optimizer=\"nadam\")\n",
        "# Dummy data for regression example (Fashion MNIST is classification)\n",
        "X_dummy = tf.constant(np.random.rand(100, 28, 28).astype(np.float32))\n",
        "y_dummy = tf.constant(np.random.rand(100, 1).astype(np.float32))\n",
        "history = model.fit(X_dummy, y_dummy, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85a53835",
      "metadata": {
        "id": "85a53835"
      },
      "source": [
        "## 4. Custom Activation Functions, Initializers, Regularizers, and Constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d023a412",
      "metadata": {
        "id": "d023a412"
      },
      "source": [
        "### Custom Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bae4d97c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bae4d97c",
        "outputId": "f4730207-2dcb-477e-c8cb-2b89ecacdc4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Dense name=dense_2, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def my_softplus(z): # custom activation function\n",
        "    return tf.math.log(tf.exp(z) + 1.0)\n",
        "\n",
        "# Example:\n",
        "keras.layers.Dense(30, activation=my_softplus, input_shape=X_train.shape[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ca31b8",
      "metadata": {
        "id": "e8ca31b8"
      },
      "source": [
        "### Custom Glorot Initializer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d1b7560d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1b7560d",
        "outputId": "d3e1c865-7751-4b8a-c875-a8ae65e09452"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Dense name=dense_3, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Default in Keras Dense layer is Glorot uniform.\n",
        "# You can customize it like this:\n",
        "# keras.layers.Dense(30, kernel_initializer=\"glorot_normal\", activation=\"relu\")\n",
        "\n",
        "# Custom Initializer example:\n",
        "def my_glorot_initializer(shape, dtype=tf.float32):\n",
        "    fan_in, fan_out = shape\n",
        "    limit = tf.math.sqrt(2.0 / (fan_in + fan_out))\n",
        "    return tf.random.uniform(shape, -limit, limit, dtype)\n",
        "\n",
        "# Example usage:\n",
        "keras.layers.Dense(30, kernel_initializer=my_glorot_initializer, activation=\"relu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e7f851",
      "metadata": {
        "id": "40e7f851"
      },
      "source": [
        "### Custom Regularizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f5d4a375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5d4a375",
        "outputId": "70280fa7-335f-4374-b423-df45b00fc3f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Dense name=dense_4, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "def l1_regularizer(weight_matrix):\n",
        "    return tf.reduce_sum(tf.abs(0.01 * weight_matrix))\n",
        "\n",
        "# Example:\n",
        "keras.layers.Dense(30, activation=\"relu\", kernel_regularizer=l1_regularizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ab4920",
      "metadata": {
        "id": "e4ab4920"
      },
      "source": [
        "### Custom Constraint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e729ffe5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e729ffe5",
        "outputId": "dadfa7b8-8ed7-4604-b8e7-fa7b4a60ce85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Dense name=dense_5, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "def custom_max_norm(weights):\n",
        "    return tf.clip_by_norm(weights, clip_norm=3.0, axes=0)\n",
        "\n",
        "# Example:\n",
        "keras.layers.Dense(30, activation=\"relu\", kernel_constraint=custom_max_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c203002",
      "metadata": {
        "id": "3c203002"
      },
      "source": [
        "## 5. Custom Models (Subclassing API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bdff4720",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdff4720",
        "outputId": "0d77136f-1471-42a8-fa84-994f3f678cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.3662 - mse_loss: 0.3903 \n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2823 - mse_loss: 0.3666 \n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2080 - mse_loss: 0.3491\n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1935 - mse_loss: 0.3659\n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1561 - mse_loss: 0.3469\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1458 - mse_loss: 0.3565\n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1285 - mse_loss: 0.3096\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1329 - mse_loss: 0.3070\n",
            "Epoch 9/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1255 - mse_loss: 0.3127\n",
            "Epoch 10/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1242 - mse_loss: 0.3090\n"
          ]
        }
      ],
      "source": [
        "class WideAndDeepModel(keras.Model):\n",
        "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
        "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
        "        self.main_output = keras.layers.Dense(1)\n",
        "        self.aux_output = keras.layers.Dense(1) # Example for multiple outputs\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_A, input_B = inputs # Assuming two inputs\n",
        "        hidden1 = self.hidden1(input_B)\n",
        "        hidden2 = self.hidden2(hidden1)\n",
        "        concat = keras.layers.concatenate([input_A, hidden2])\n",
        "        main_output = self.main_output(concat)\n",
        "        aux_output = self.aux_output(hidden2)\n",
        "        return main_output, aux_output # Return tuple for multiple outputs\n",
        "\n",
        "# Example usage (requires dummy inputs)\n",
        "X_train_A = np.random.rand(100, 5) # dummy wide input\n",
        "X_train_B = np.random.rand(100, 6) # dummy deep input\n",
        "y_train_main = np.random.rand(100, 1) # dummy main output\n",
        "y_train_aux = np.random.rand(100, 1) # dummy auxiliary output\n",
        "\n",
        "model_custom = WideAndDeepModel()\n",
        "model_custom.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\n",
        "history = model_custom.fit((X_train_A, X_train_B), (y_train_main, y_train_aux), epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8e693f",
      "metadata": {
        "id": "5b8e693f"
      },
      "source": [
        "## 6. Custom Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c0de748d",
      "metadata": {
        "id": "c0de748d"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
        "                       for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        return inputs + Z # Residual connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0718aac9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0718aac9",
        "outputId": "c2a680d6-ffc7-4a4d-abad-8d64530e6972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.6145\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4329\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2678\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2532\n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1772\n"
          ]
        }
      ],
      "source": [
        "class ResidualRegressor(keras.Model):\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden1 = keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
        "        self.block1 = ResidualBlock(2, 30)\n",
        "        self.block2 = ResidualBlock(2, 30)\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = self.hidden1(inputs)\n",
        "        Z = self.block1(Z)\n",
        "        Z = self.block2(Z)\n",
        "        return self.out(Z)\n",
        "\n",
        "# Example usage (needs data and compilation)\n",
        "model_res = ResidualRegressor(1)\n",
        "model_res.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = model_res.fit(np.random.rand(100, 28, 28), np.random.rand(100, 1), epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8027fa98",
      "metadata": {
        "id": "8027fa98"
      },
      "source": [
        "## 7. Custom Training Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "412f58b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "412f58b5",
        "outputId": "0873e557-4021-4e3e-9707-5acfd92f994c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "  Loss: 110105051791360.0\n",
            "Epoch 2/5\n",
            "  Loss: 4.075533025238498e+27\n",
            "Epoch 3/5\n",
            "  Loss: inf\n",
            "Epoch 4/5\n",
            "  Loss: inf\n",
            "Epoch 5/5\n",
            "  Loss: inf\n"
          ]
        }
      ],
      "source": [
        "# Create a simple model for custom training loop\n",
        "model = keras.models.Sequential([keras.layers.Dense(1, input_shape=[1])])\n",
        "\n",
        "# Create dummy data\n",
        "X_train_custom = tf.constant(np.arange(100).reshape(100, 1), dtype=tf.float32)\n",
        "y_train_custom = tf.constant(np.arange(100).reshape(100, 1) * 2 + 1, dtype=tf.float32) # y = 2x + 1\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = keras.losses.MeanSquaredError()\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 5\n",
        "batch_size = 32\n",
        "n_batches = int(np.ceil(len(X_train_custom) / batch_size))\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
        "    for i in range(n_batches):\n",
        "        X_batch = X_train_custom[i * batch_size:(i + 1) * batch_size]\n",
        "        y_batch = y_train_custom[i * batch_size:(i + 1) * batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True)\n",
        "            loss = loss_fn(y_batch, y_pred)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f\"  Loss: {loss.numpy()}\") # Print final batch loss for epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be65fa47",
      "metadata": {
        "id": "be65fa47"
      },
      "source": [
        "### Custom Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3bf80b5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bf80b5e",
        "outputId": "e6e50adf-8e81-4a45-a17c-f65edf43ac4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - huber_metric: 0.2713 - loss: 0.5426\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - huber_metric: 0.1647 - loss: 0.3295\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - huber_metric: 0.0709 - loss: 0.1418\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - huber_metric: 0.0854 - loss: 0.1707 \n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - huber_metric: 0.0524 - loss: 0.1048\n"
          ]
        }
      ],
      "source": [
        "# Assuming create_huber_loss function is defined earlier in the notebook\n",
        "# as provided in the initial context.\n",
        "\n",
        "class HuberMetric(keras.metrics.Metric):\n",
        "    def __init__(self, threshold=1.0, name=None, dtype=None, **kwargs):\n",
        "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
        "        self.threshold = threshold\n",
        "        # Reusing the custom loss function defined earlier\n",
        "        self.huber_fn = create_huber_loss(threshold)\n",
        "        # Explicitly specify shape=() for scalar variables\n",
        "        self.total = self.add_weight(name=\"total\", shape=(), initializer=\"zeros\", dtype=tf.float32)\n",
        "        self.count = self.add_weight(name=\"count\", shape=(), initializer=\"zeros\", dtype=tf.float32)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Ensure y_true and y_pred have compatible shapes and types\n",
        "        # Reshape y_pred to match y_true if necessary and possible\n",
        "        if y_true.shape.rank != y_pred.shape.rank:\n",
        "            # Attempt to reshape y_pred to match y_true's rank if y_true is (batch_size, 1)\n",
        "            if y_true.shape.rank == 2 and y_pred.shape.rank == 1:\n",
        "                 y_pred = tf.expand_dims(y_pred, axis=-1)\n",
        "            # Handle other rank mismatches if necessary, or raise an error\n",
        "            else:\n",
        "                 # This case is less likely with a simple sequential model\n",
        "                 # but good practice to acknowledge.\n",
        "                 tf.print(\"Shape rank mismatch in HuberMetric.update_state:\",\n",
        "                          \"y_true shape:\", tf.shape(y_true),\n",
        "                          \"y_pred shape:\", tf.shape(y_pred))\n",
        "                 # You might want to raise an error here if unexpected\n",
        "                 # raise ValueError(\"Shape rank mismatch between y_true and y_pred\")\n",
        "\n",
        "        # Ensure y_true and y_pred have the same float type for calculations\n",
        "        y_pred = tf.cast(y_pred, self.dtype)\n",
        "        y_true = tf.cast(y_true, self.dtype)\n",
        "\n",
        "        # Calculate Huber loss for the current batch\n",
        "        # Ensure shapes are compatible for subtraction after casting/reshaping\n",
        "        if y_true.shape != y_pred.shape:\n",
        "             tf.print(\"Shape mismatch after casting/reshaping in HuberMetric.update_state:\",\n",
        "                      \"y_true shape:\", tf.shape(y_true),\n",
        "                      \"y_pred shape:\", tf.shape(y_pred))\n",
        "             # You might want to raise an error here if unexpected\n",
        "             # raise ValueError(\"Shape mismatch between y_true and y_pred after processing\")\n",
        "\n",
        "\n",
        "        metric_values = self.huber_fn(y_true, y_pred)\n",
        "\n",
        "        # Sum the loss values and add to the total\n",
        "        self.total.assign_add(tf.reduce_sum(tf.cast(metric_values, self.dtype)))\n",
        "        # Count the number of samples in the batch and add to the count\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), self.dtype))\n",
        "\n",
        "    def result(self):\n",
        "        # Compute the average Huber loss, ensuring division is float\n",
        "        # Add epsilon for stability in case count is zero\n",
        "        return self.total / (self.count + tf.keras.backend.epsilon())\n",
        "\n",
        "    def reset_state(self):\n",
        "        # Reset the state variables for a new epoch or evaluation\n",
        "        self.total.assign(0.0)\n",
        "        self.count.assign(0.0)\n",
        "\n",
        "# Define a simple sequential model suitable for regression (single output neuron)\n",
        "# Add a Flatten layer to handle image input\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=X_train.shape[1:]), # Explicitly flatten the 28x28 images\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1) # Single output for regression\n",
        "])\n",
        "\n",
        "# Compile the model, specifying the loss, optimizer, and the custom metric\n",
        "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[HuberMetric(threshold=2.0, name=\"huber_metric\")])\n",
        "\n",
        "# Create dummy data suitable for a regression problem\n",
        "# Input shape should match the Flatten layer's expected input (28, 28)\n",
        "X_dummy = tf.constant(np.random.rand(100, 28, 28).astype(np.float32))\n",
        "# Target data shape should match the model's output shape (1,)\n",
        "y_dummy = tf.constant(np.random.rand(100, 1).astype(np.float32))\n",
        "\n",
        "# Train the model using the dummy data.\n",
        "history = model.fit(X_dummy, y_dummy, epochs=5)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}