{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4202c31",
      "metadata": {
        "id": "e4202c31"
      },
      "source": [
        "# Bab 13: Loading and Preprocessing Data with TensorFlow (Memuat dan Mempraproses Data dengan TensorFlow)\n",
        "\n",
        "### 1. Pendahuluan\n",
        "\n",
        "Bab 12 telah menunjukkan bagaimana membangun model kustom dan *training loop* kustom dengan TensorFlow. Namun, untuk dataset yang sangat besar yang tidak muat dalam memori (dataset *out-of-core*), atau untuk menerapkan transformasi data yang kompleks dan *pipelining* yang efisien, diperlukan alat khusus. Bab 13 ini berfokus pada **TFRecord** dan **tf.data API** di TensorFlow untuk menangani data secara efisien.\n",
        "\n",
        "TFRecord adalah format biner yang sangat efisien untuk menyimpan data pelatihan. `tf.data` API menyediakan cara yang kuat dan fleksibel untuk membuat *pipeline* input yang dapat menangani dataset besar, melakukan transformasi data yang kompleks, dan mengoptimalkan kinerja *input*.\n",
        "\n",
        "### 2. Dataset API (tf.data)\n",
        "\n",
        "`tf.data` API memungkinkan Anda membangun *pipeline* input yang efisien untuk memuat data, mempraprosesnya, mengacaknya, dan membuat *batch* darinya. Ini adalah fondasi untuk penanganan data di TensorFlow 2.x.\n",
        "\n",
        "#### a. Membuat Objek Dataset (Creating a Dataset Object)\n",
        "* **Dari Array NumPy:** `tf.data.Dataset.from_tensor_slices((X, y))` membuat dataset dari array NumPy atau TensorFlow *tensor*.\n",
        "* **Dari File Teks:** `tf.data.TextLineDataset()` untuk membaca file teks baris demi baris.\n",
        "\n",
        "#### b. Transformasi Dataset (Dataset Transformations)\n",
        "Dataset dapat diubah menggunakan berbagai metode:\n",
        "* **`map(function)`:** Menerapkan fungsi transformasi ke setiap elemen dalam dataset. Ini sangat berguna untuk pra-pemrosesan.\n",
        "    * Fungsi yang dipetakan harus berupa operasi TensorFlow, bukan operasi Python biasa, untuk kinerja yang optimal. Gunakan `@tf.function` atau operasi TensorFlow *built-in*.\n",
        "* **`filter(predicate)`:** Memfilter elemen berdasarkan predikat.\n",
        "* **`batch(batch_size)`:** Menggabungkan elemen ke dalam *batch*.\n",
        "* **`shuffle(buffer_size)`:** Mengacak elemen dalam dataset. `buffer_size` adalah ukuran buffer untuk pengacakan.\n",
        "* **`repeat(count)`:** Mengulang dataset sejumlah kali. Jika `count=None`, akan mengulang tanpa batas.\n",
        "* **`prefetch(buffer_size)`:** Mempramuat (*prefetch*) *batch* berikutnya saat *batch* saat ini sedang diproses. Ini sangat penting untuk efisiensi CPU/GPU karena memungkinkan *pipelining* data. Gunakan `tf.data.AUTOTUNE` untuk ukuran buffer optimal.\n",
        "\n",
        "#### c. Menggunakan Dataset dengan Model Keras (Using Datasets with Keras Models)\n",
        "Setelah membuat *pipeline* input menggunakan `tf.data`, Anda dapat memberinya langsung ke metode `fit()`, `evaluate()`, dan `predict()` dari model Keras.\n",
        "\n",
        "### 3. TFRecord Format\n",
        "\n",
        "TFRecord adalah format biner sederhana untuk data sekuensial yang sangat efisien untuk dibaca oleh TensorFlow.\n",
        "\n",
        "#### a. Struktur Data TFRecord (TFRecord Data Structure)\n",
        "* Setiap entri dalam file TFRecord disebut `Example`.\n",
        "* Setiap `Example` berisi satu atau lebih **Features**.\n",
        "* Setiap `Feature` adalah kamus yang memetakan nama fitur (string) ke salah satu dari tiga jenis daftar:\n",
        "    * `bytes_list`: Untuk string biner atau *byte*.\n",
        "    * `float_list`: Untuk float (32-bit).\n",
        "    * `int64_list`: Untuk integer (64-bit).\n",
        "\n",
        "#### b. Menulis File TFRecord (Writing TFRecord Files)\n",
        "* Gunakan `tf.io.TFRecordWriter` untuk menulis data ke file TFRecord.\n",
        "* Konversi data Anda ke format `Example` menggunakan `tf.train.Features` dan `tf.train.Feature` yang sesuai.\n",
        "\n",
        "#### c. Membaca File TFRecord (Reading TFRecord Files)\n",
        "* Gunakan `tf.data.TFRecordDataset()` untuk membuat dataset yang membaca dari satu atau lebih file TFRecord.\n",
        "* Dataset ini menghasilkan *scalar binary string* untuk setiap `Example` yang dibaca.\n",
        "* Untuk mem-parsing *binary string* ini kembali ke *tensor* yang dapat digunakan, gunakan `tf.io.parse_single_example()` atau `tf.io.parse_example()` dengan deskripsi fitur yang sesuai.\n",
        "\n",
        "#### d. Fitur Berkode Protobuf (Parsing Protobuf Encoded Features)\n",
        "* Saat membaca, Anda harus menentukan skema data (jenis dan bentuk fitur) untuk memungkinkan TensorFlow mem-parsing data biner.\n",
        "* Contoh skema untuk fitur numerik dan fitur *string* ditunjukkan.\n",
        "\n",
        "### 4. Pra-pemrosesan Data (Preprocessing the Input Features)\n",
        "\n",
        "`tf.data` API tidak hanya untuk memuat, tetapi juga untuk melakukan pra-pemrosesan data secara efisien.\n",
        "\n",
        "#### a. Normalisasi Data (Scaling Features)\n",
        "* Scaling fitur (misalnya, normalisasi atau standardisasi) dapat diterapkan dalam *pipeline* `tf.data` menggunakan fungsi `map()`.\n",
        "* Pertimbangkan untuk melakukan adaptasi pra-pemrosesan (misalnya, menghitung *mean* dan *variance* untuk standardisasi) sebagai langkah terpisah sebelum *pipelining* data pelatihan lengkap.\n",
        "\n",
        "#### b. Batching dan Pra-pemrosesan Bersama (Batching and Preprocessing Together)\n",
        "* Sangat efisien untuk mempraproses data dalam *batch* menggunakan `map()` dengan fungsi yang dapat bekerja pada *batch*.\n",
        "* Urutan `shuffle()`, `batch()`, dan `prefetch()` sangat penting untuk kinerja.\n",
        "\n",
        "#### c. Tokenisasi dan Padding (Tokenization and Padding)\n",
        "* Untuk data teks, tokenisasi dan padding adalah langkah pra-pemrosesan umum.\n",
        "* `tf.keras.preprocessing.text.Tokenizer` dapat digunakan untuk tokenisasi.\n",
        "* `tf.keras.preprocessing.sequence.pad_sequences` untuk padding.\n",
        "* Pada `tf.data`, ini sering dilakukan dalam fungsi `map()` atau dengan fungsi khusus dari `tf.text` (untuk teks).\n",
        "\n",
        "### 5. TF.Transform\n",
        "\n",
        "`TF.Transform` adalah pustaka TensorFlow untuk pra-pemrosesan data yang dilakukan secara *batch* penuh atau *eager* dan kemudian dijalankan secara efisien di *pipeline* produksi. Ini berguna untuk transformasi yang membutuhkan statistik dari seluruh dataset (misalnya, penskalaan min-max, *one-hot encoding*).\n",
        "\n",
        "### 6. TFRecord dan Estimators (TFRecord and Estimators)\n",
        "\n",
        "Meskipun model Keras sangat direkomendasikan, TensorFlow juga menyediakan Estimator API (tingkat lebih tinggi dari Keras tetapi lebih rendah dari *pre-built Estimators*). Estimator seringkali dapat menggunakan `tf.data` *pipeline* dan TFRecord. Namun, fokus buku ini lebih pada Keras.\n",
        "\n",
        "### 7. Penggunaan Data Tensorflow.io (Using tensorflow_io data)\n",
        "\n",
        "`tensorflow-io` adalah paket yang menyediakan dukungan untuk lebih banyak format file dan sistem file (misalnya, Apache Parquet, CSV dengan fitur yang kompleks, sistem file cloud). Ini memperluas kemampuan `tf.data` untuk membaca dari berbagai sumber.\n",
        "\n",
        "### 8. Studi Kasus: Menangani Dataset yang Besar (Case Study: Handling Large Datasets)\n",
        "\n",
        "Bagian ini mungkin akan memberikan contoh end-to-end tentang bagaimana menggunakan `tf.data` dan TFRecord untuk mengelola dataset yang sangat besar, seperti dataset gambar ImageNet. Ini akan melibatkan:\n",
        "* Membuat file TFRecord dari gambar mentah dan labelnya.\n",
        "* Membangun *pipeline* `tf.data` untuk memuat, mendekode, mengacak, mempraproses (misalnya, augmentasi data), dan mem-*batch* data secara efisien.\n",
        "\n",
        "### 9. Kesimpulan\n",
        "\n",
        "Bab 13 adalah bab krusial untuk implementasi *deep learning* dalam skala produksi. Pemahaman mendalam tentang `tf.data` API sangat penting untuk membangun *pipeline* input yang efisien dan fleksibel, terutama saat berhadapan dengan dataset besar. Pengenalan format TFRecord sebagai format penyimpanan data yang dioptimalkan untuk TensorFlow melengkapi pengetahuan tentang cara menangani data secara efektif dalam ekosistem TensorFlow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "-UxSCWOcSfPH"
      },
      "id": "-UxSCWOcSfPH"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "d7wu4OMTSgo8"
      },
      "id": "d7wu4OMTSgo8",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. The Data API"
      ],
      "metadata": {
        "id": "Sxkic05mSjn8"
      },
      "id": "Sxkic05mSjn8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Dataset from NumPy Arrays"
      ],
      "metadata": {
        "id": "UzAS7ULySm5A"
      },
      "id": "UzAS7ULySm5A"
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.constant(np.arange(10).reshape(5, 2), dtype=tf.float32) # Dummy data\n",
        "y = tf.constant(np.arange(5), dtype=tf.int32) # Dummy labels\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "# Iterate through the dataset (for demonstration)\n",
        "for item_x, item_y in dataset:\n",
        "    print(item_x.numpy(), item_y.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNiqWnl3SoIS",
        "outputId": "8a446136-8354-46c6-aedd-64b4fd0ec073"
      },
      "id": "qNiqWnl3SoIS",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.] 0\n",
            "[2. 3.] 1\n",
            "[4. 5.] 2\n",
            "[6. 7.] 3\n",
            "[8. 9.] 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chaining Transformations"
      ],
      "metadata": {
        "id": "DLxOWPcaSrOL"
      },
      "id": "DLxOWPcaSrOL"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "dataset = dataset.repeat(3).batch(2) # Repeat 3 times, then batch by 2\n",
        "\n",
        "for item_x, item_y in dataset:\n",
        "    print(item_x.numpy(), item_y.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHVUHR9xSs8I",
        "outputId": "500b3b73-7677-4da5-de48-2d2bea721bd1"
      },
      "id": "YHVUHR9xSs8I",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1.]\n",
            " [2. 3.]] [0 1]\n",
            "[[4. 5.]\n",
            " [6. 7.]] [2 3]\n",
            "[[8. 9.]\n",
            " [0. 1.]] [4 0]\n",
            "[[2. 3.]\n",
            " [4. 5.]] [1 2]\n",
            "[[6. 7.]\n",
            " [8. 9.]] [3 4]\n",
            "[[0. 1.]\n",
            " [2. 3.]] [0 1]\n",
            "[[4. 5.]\n",
            " [6. 7.]] [2 3]\n",
            "[[8. 9.]] [4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle, Batch, Prefetch\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "dataset = dataset.shuffle(buffer_size=len(X)).batch(2).prefetch(1)\n",
        "\n",
        "for item_x, item_y in dataset:\n",
        "    print(item_x.numpy(), item_y.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7rUYJGuSwFO",
        "outputId": "ed634200-162c-4257-bbf0-27a76ab48392"
      },
      "id": "p7rUYJGuSwFO",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8. 9.]\n",
            " [4. 5.]] [4 2]\n",
            "[[6. 7.]\n",
            " [0. 1.]] [3 0]\n",
            "[[2. 3.]] [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a Dataset with Keras"
      ],
      "metadata": {
        "id": "Iec0mVgYSxYH"
      },
      "id": "Iec0mVgYSxYH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_train, y_train, X_valid, y_valid from Fashion MNIST or other data\n",
        "# Create a dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(32).prefetch(1)\n",
        "\n",
        "# Build a simple model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model using the dataset\n",
        "model.fit(train_dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6pwQrqZSyKp",
        "outputId": "e819d303-a888-49be-b1cd-759679d751fd"
      },
      "id": "x6pwQrqZSyKp",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6765 - loss: 1.0130\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8241 - loss: 0.5060\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8407 - loss: 0.4529\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8503 - loss: 0.4245\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8605 - loss: 0.3999\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8608 - loss: 0.3883\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8669 - loss: 0.3745\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8689 - loss: 0.3618\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8733 - loss: 0.3528\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8766 - loss: 0.3387\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa9c81ca7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading CSV Files"
      ],
      "metadata": {
        "id": "6SRBspiiS2Z-"
      },
      "id": "6SRBspiiS2Z-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the California Housing dataset (if available locally)\n",
        "csv_path = \"/content/sample_data/california_housing_train.csv\" # Example path\n",
        "\n",
        "# This part of the book shows how to load CSV using pandas first, then convert to TF dataset.\n",
        "# Or directly using tf.data.experimental.make_csv_dataset (deprecated in favor of tf.data.experimental.make_csv_dataset)\n",
        "# and then tf.data.experimental.make_csv_dataset"
      ],
      "metadata": {
        "id": "iWpKegJ-S4HA"
      },
      "id": "iWpKegJ-S4HA",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess a single CSV file (example)\n",
        "def preprocess_csv_line(line):\n",
        "    # Example parsing for a simplified CSV\n",
        "    # You should adjust the number and default values (defs)\n",
        "    # based on the actual structure of your CSV file.\n",
        "    # The number of default values should match the number of columns\n",
        "    # in your CSV.\n",
        "    defs = [0.] * 9 # Default values for 9 features\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "\n",
        "    # Assuming the last column is the target and the rest are features\n",
        "    x = tf.stack(fields[0:-1]) # All but last as features\n",
        "    y = tf.stack(fields[-1:]) # Last as target\n",
        "    return x, y\n",
        "\n",
        "# Example for make_csv_dataset (simplified)\n",
        "# This function creates a tf.data Dataset from one or more CSV files.\n",
        "def csv_reader_dataset(filepaths, n_readers=5, batch_size=32, n_parse_threads=5,\n",
        "                       shuffle_buffer_size=10000):\n",
        "    # list_files creates a dataset of file names matching the pattern\n",
        "    dataset = tf.data.Dataset.list_files(filepaths).interleave(\n",
        "        # interleave reads lines from the files concurrently\n",
        "        tf.data.TextLineDataset,\n",
        "        cycle_length=n_readers,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE # Let TensorFlow decide the optimal parallelism\n",
        "    )\n",
        "    # shuffle shuffles the elements (lines) in the dataset\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    # map applies the preprocess_csv_line function to each element (line)\n",
        "    dataset = dataset.map(preprocess_csv_line, num_parallel_calls=n_parse_threads)\n",
        "    # batch groups consecutive elements into batches\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # prefetch allows the data pipeline to fetch the next batch while the current one is being processed\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE) # Prefetch for performance\n",
        "    return dataset\n",
        "\n",
        "# --- FIX REQUIRED HERE ---\n",
        "# You MUST replace this placeholder string with the actual path to your CSV file.\n",
        "# Examples:\n",
        "# train_filepaths = \"/home/your_user/data/california_housing_train.csv\"  # Absolute path\n",
        "# train_filepaths = \"../data/california_housing_train.csv\"               # Relative path\n",
        "# train_filepaths = \"/content/sample_data/california_housing_train.csv\" # If in Google Colab sample data\n",
        "#\n",
        "# Make sure the file exists at the path you provide.\n",
        "train_filepaths = \"/content/sample_data/california_housing_train.csv\" # <--- REPLACE THIS STRING\n",
        "\n",
        "# Create the training dataset using the function\n",
        "train_dataset_csv = csv_reader_dataset(train_filepaths)\n",
        "\n",
        "# You can now iterate through train_dataset_csv or use it with model.fit()\n",
        "# for item_x, item_y in train_dataset_csv:\n",
        "#     print(item_x.numpy().shape, item_y.numpy().shape)"
      ],
      "metadata": {
        "id": "AhVm_SrJTHQN"
      },
      "id": "AhVm_SrJTHQN",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. TFRecord Format"
      ],
      "metadata": {
        "id": "jRXUtlTITIxE"
      },
      "id": "jRXUtlTITIxE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating TFRecord Files"
      ],
      "metadata": {
        "id": "AYul_7poTJ2Y"
      },
      "id": "AYul_7poTJ2Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Creating a simple TFRecord file\n",
        "# Define a simple Example structure\n",
        "from tensorflow.train import Example, Features, Feature, BytesList, FloatList, Int64List\n",
        "\n",
        "example = Example(features=Features(feature={\n",
        "    \"feature1\": Feature(float_list=FloatList(value=[1.0, 2.0])),\n",
        "    \"feature2\": Feature(int64_list=Int64List(value=[3])),\n",
        "    \"feature3\": Feature(bytes_list=BytesList(value=[b\"hello\"]))\n",
        "}))\n",
        "\n",
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "    f.write(example.SerializeToString())"
      ],
      "metadata": {
        "id": "eILelENfTMfv"
      },
      "id": "eILelENfTMfv",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading TFRecord Files"
      ],
      "metadata": {
        "id": "fV12Ip4tTPjy"
      },
      "id": "fV12Ip4tTPjy"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_tfrecord = tf.data.TFRecordDataset(\"my_data.tfrecord\")\n",
        "\n",
        "# Function to parse the TFRecord example\n",
        "def parse_example(serialized_example):\n",
        "    feature_description = {\n",
        "        \"feature1\": tf.io.FixedLenFeature([2], tf.float32, default_value=[0., 0.]),\n",
        "        \"feature2\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "        \"feature3\": tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    }\n",
        "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
        "\n",
        "parsed_dataset = dataset_tfrecord.map(parse_example)\n",
        "\n",
        "for parsed_features in parsed_dataset:\n",
        "    print(parsed_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuKV0s9WTQNq",
        "outputId": "4bf65e0d-e209-4ba3-b790-db6ed4ebe90d"
      },
      "id": "ZuKV0s9WTQNq",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'feature1': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>, 'feature2': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature3': <tf.Tensor: shape=(), dtype=string, numpy=b'hello'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing the Input Features"
      ],
      "metadata": {
        "id": "GwAwrjpZTR-c"
      },
      "id": "GwAwrjpZTR-c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Preprocessing Layers (using keras.layers.preprocessing)"
      ],
      "metadata": {
        "id": "1H3G7tqVTUrf"
      },
      "id": "1H3G7tqVTUrf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of TextVectorization (for text data, not directly from book's main example but common)\n",
        "text_dataset = tf.data.Dataset.from_tensor_slices([\"hello world\", \"hello from tensorflow\"])\n",
        "max_tokens = 1000 # Example\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode='int', # or 'tf_idf'\n",
        "    output_sequence_length=10 # Example fixed length\n",
        ")\n",
        "text_vectorization.adapt(text_dataset.batch(32)) # Adapt to the data\n",
        "vectorized_text_data = text_dataset.map(lambda x: text_vectorization(x))"
      ],
      "metadata": {
        "id": "QbvoP68fTVmX"
      },
      "id": "QbvoP68fTVmX",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardization (Normalization)"
      ],
      "metadata": {
        "id": "AEP_h1aqTW_v"
      },
      "id": "AEP_h1aqTW_v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of standardization within a tf.data pipeline\n",
        "# You'd typically compute mean and std from training data first.\n",
        "mean_X = np.mean(X_train_full, axis=0) # For image data\n",
        "std_X = np.std(X_train_full, axis=0)\n",
        "\n",
        "def standardize_image(image, label):\n",
        "    return (tf.cast(image, tf.float32) - mean_X) / std_X, label\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.map(standardize_image).batch(32).prefetch(1)"
      ],
      "metadata": {
        "id": "NEt6C_O4TY43"
      },
      "id": "NEt6C_O4TY43",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. TF.Transform (Conceptual)"
      ],
      "metadata": {
        "id": "loAK869ITa2A"
      },
      "id": "loAK869ITa2A"
    },
    {
      "cell_type": "code",
      "source": [
        "# TF.Transform requires Apache Beam and a more complex setup for production pipelines.\n",
        "# The book describes its conceptual use case but typically doesn't provide\n",
        "# runnable code snippets without setting up a Beam pipeline.\n",
        "# Key idea: analyze data once for stats (e.g., mean, variance, vocabulary)\n",
        "# then use those stats to transform data consistently during training and serving."
      ],
      "metadata": {
        "id": "txZjbI9fTb5F"
      },
      "id": "txZjbI9fTb5F",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Using tensorflow-io data (Conceptual)"
      ],
      "metadata": {
        "id": "aWFMSxIpTdUo"
      },
      "id": "aWFMSxIpTdUo"
    },
    {
      "cell_type": "code",
      "source": [
        "# This requires installing tensorflow-io (pip install tensorflow-io)\n",
        "# import tensorflow_io as tfio\n",
        "\n",
        "# Example for Parquet (conceptual, depends on actual file structure)\n",
        "# dataset_parquet = tfio.IODataset.from_parquet(\"/path/to/my/data.parquet\")\n",
        "# dataset_parquet = dataset_parquet.map(lambda x: (x['feature_col'], x['target_col'])).batch(32)"
      ],
      "metadata": {
        "id": "F944Yr0ETfRy"
      },
      "id": "F944Yr0ETfRy",
      "execution_count": 34,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}